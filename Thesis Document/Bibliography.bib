% Encoding: UTF-8

@Article{Fienup1982,
  author        = {Fienup, J. R.},
  title         = {Phase retrieval algorithms: a comparison},
  journal       = {Applied Optics},
  year          = {1982},
  volume        = {21},
  number        = {15},
  pages         = {2758--2769},
  month         = aug,
  issn          = {2155-3165},
  __markedentry = {[ruilv:6]},
  abstract      = {Iterative algorithms for phase retrieval from intensity data are compared to gradient search methods. Both the problem of phase retrieval from two intensity measurements (in electron microscopy or wave front sensing) and the problem of phase retrieval from a single intensity measurement plus a non-negativity constraint (in astronomy) are considered, with emphasis on the latter. It is shown that both the error-reduction algorithm for the problem of a single intensity measurement and the Gerchberg-Saxton algorithm for the problem of two intensity measurements converge. The error-reduction algorithm is also shown to be closely related to the steepest-descent method. Other algorithms, including the input–output algorithm and the conjugate-gradient method, are shown to converge in practice much faster than the error-reduction algorithm. Examples are shown.},
  copyright     = {\&\#169; 1982 Optical Society of America},
  doi           = {10.1364/AO.21.002758},
  file          = {Full Text PDF:https\://www.osapublishing.org/viewmedia.cfm?uri=ao-21-15-2758&seq=0:application/pdf;Snapshot:https\://www.osapublishing.org/ao/abstract.cfm?URI=ao-21-15-2758:text/html},
  keywords      = {Discrete Fourier transforms, Fast Fourier transforms, Fourier transforms, Phase retrieval, Point spread function, Wave front sensing},
  language      = {EN},
  publisher     = {Optical Society of America},
  shorttitle    = {Phase retrieval algorithms},
  url           = {https://www.osapublishing.org/ao/abstract.cfm?uri=ao-21-15-2758},
  urldate       = {CURRENT\_TIMESTAMP},
}
@Article{GettingMostOutOfTokenizers,
    author        = {Gautier Dagan, Gabriel Synnaeve and Baptiste Rozière},
    title         = {Getting the most out of your tokenizer for pre-training and domain adaptation},
    year          = {2024},
    month         = feb,
    journal       = {},
    url          = {https://arxiv.org/abs/2402.01035}

}
@Article{Gloria,
    author      = {Ricardo Lopes, João Magalhães, David Semedo},
    title       = {GlórIA -- A Generative and Open Large Language Model for Portuguese},
    year        = {2024},
    month       = feb,
    journal     = {},
    url        = {https://arxiv.org/abs/2402.12969}
}
@Article{Sabia,
    author      = {Ramon Pires, Hugo Abonizio, Thales Sales Almeida, Rodrigo Nogueira},
    title       = {Sabiá: Portuguese Large Language Models},
    year        = {2023},
    month       = apr,
    journal     = {},
    url        = {https://arxiv.org/abs/2304.07880}
}
@Article{Gervasio,
    author      = {Rodrigo Santos},
    title       = {Advancing Generative AI for Portuguese with Open Decoder Gervásio PT*},
    year        = {2024},
    month       = feb,
    journal     = {},
    url        = {https://arxiv.org/abs/2402.18766}
}
@article{Albertina,
    author = {João Rodrigues},
    title = {Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*},
    journal = {},
    year = 2023,
    month = may,
    url = { https://arxiv.org/abs/2305.06721}
}
@article{AdaptinigPretrainedModels,
    author = {Zoltan Csaki},
    title = {Efficiently Adapting Pretrained Language Models To New Languages},
    journal = {},
    year = 2023,
    month = nov,
    url = { https://arxiv.org/abs/2311.05741}
}
@misc{chatgpt,
    key = "https://openai.com/chatgpt/overview/",
    note = "ChatGPT"
}
@article{AttentionIsAllYouNeed,
    author = {Ashish Vaswani},
    title = {Attention Is All You Need},
    journal = {},
    year = 2017,
    month = jun,
    url = {https://arxiv.org/abs/1706.03762}
}
@article{brown2020,
    author = {Tom B. Brown},
    title = {Language Models are Few-Shot Learners},
    journal = {},
    year = 2020,
    month = Jul,
    url = {https://arxiv.org/abs/2005.14165}
}

@article{openaiGPT4,
    author = {OpenAI},
    title = {Language Models are Few-Shot Learners},
    journal = {},
    year = 2023,
    month = Mar,
    url={https://arxiv.org/abs/2303.08774}
}
@misc{openaiChatGPT,
    author={OpenAI},
    key = {ChatGPT},
    note = {Accessed: 2025‑08‑26, Online chat-bot that first shared LLM architecture with the general public.},
    url = {https://chatgpt.com/}
}
@article{touvron2023,
    author = {Hugo Touvron},
    title = {LLaMA: Open and Efficient Foundation Language Models},
    journal = {},
    year = 2023,
    month = Feb,
    url = {https://arxiv.org/abs/2302.13971}
}

@misc{EuropeanPortuguese_wikipedia,
  title = {European Portuguese},
  url = {https://en.wikipedia.org/wiki/European_Portuguese},
  key = Wikipedia,
  note = {Accessed: 2025‑08‑26},
  year = {2025},
}
@article{chen2021evaluating,
    author = {Mark Chen},
    title = {Evaluating Large Language Models Trained on Code},
    journal = {},
    year = 2021,
    month = Jul,
    url = {https://arxiv.org/abs/2107.03374}
}
@article{lee2020biobert,
    author = {Jinhyuk Lee},
    title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
    journal = {},
    year = 2020,
    month = Feb,
    url = {https://pubmed.ncbi.nlm.nih.gov/31501885/}
}
@article{chalkidis2020legal,
    author = {Ilias Chalkidis},
    title = {LEGAL-BERT: The Muppets straight out of Law School},
    journal = {},
    year = 2020,
    month = Nov,
    url = {https://aclanthology.org/2020.findings-emnlp.261/}
}
@article{gururangan2020don,
    author = {Suchin Gururangan},
    title = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
    journal = {},
    year = 2020,
    month = Apr,
    url = {https://arxiv.org/abs/2004.10964}
}


@article{hu2021lora,
    author = {Edward J. Hu},
    title = {LoRA: Low-Rank Adaptation of Large Language Models},
    journal = {},
    year = 2021,
    month = Jun,
    url = {https://arxiv.org/abs/2106.09685}
}

@article{khade2024lora,
    author = {Omkar Khade},
    title = {Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning},
    journal = {},
    year = 2024,
    month = Nov,
    url = {https://arxiv.org/abs/2411.18571},
}    


@article{houlsby2019parameter,
    author = {Neil Houlsby},
    title = {Parameter-Efficient Transfer Learning for NLP},
    journal = {},
    year = 2019,
    month = Feb,
    url = {https://proceedings.mlr.press/v97/houlsby19a.html},
}    

@article{hu2023adapters,
    author = {Zhiqiang Hu},
    title = {LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
    journal = {},
    year = 2023,
    month = Apr,
    url = {https://arxiv.org/abs/2304.01933}

}

@article{xia2024finetuneperformance,
    author = {YuchenXia},
    title = {Understanding the Performance and Estimating the Cost of LLM Fine-Tuning},
    journal = {IEEE Xplore},
    year = 2024,
    month = Sep,
    url = {https://arxiv.org/abs/2304.01933}
}

@article{wei2021finetuned,
    author = {Jason Wei},
    title = {Finetuned Language Models Are Zero-Shot Learners},
    journal = {},
    year = 2021,
    month = Sep,
    url = {https://arxiv.org/abs/2109.01652}
}

@article{martinez2022tuning,
    author = {Fernando Fernández-Martínez},
    title = {Fine-Tuning BERT Models for Intent Recognition Using a Frequency Cut-Off Strategy for Domain-Specific Vocabulary Extension},
    journal = {MPDI},
    year = 2022,
    month = Jan,
    url = {https://www.mdpi.com/2076-3417/12/3/1610}
}

@article{rust2020tokenizer,
    author = {Philip Rust},
    title = {How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
    journal = {arxiv},
    year = 2020,
    month = Dec,
    url = {https://arxiv.org/abs/2012.15613}
}

@article{jiti2021bertweet,
    author = {Fajri Koto},
    title = {IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization},
    journal = {},
    year = 2021,
    month = Sep,
    url = {https://www.semanticscholar.org/paper/IndoBERTweet\%3A-A-Pretrained-Language-Model-for-with-Koto-Lau/59c0c6b62e33850cda08663d4c9ecabcf5d21596}
}

@article{artetxe2018robust,
    author = {Mikel Artetxe},
    title = {A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings},
    journal = {},
    year = 2018,
    month = May,
    url = {https://arxiv.org/abs/1805.06297}
}


@article{wang2019superglue,
    author = {Alex Wang},
    title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
    journal = {},
    year = 2019,
    month = May,
    url = {https://arxiv.org/abs/1905.00537}
}


% Add references to this one, Thesis focused on extension of tokenizers
@article{kim2025finetune,
    author = {Jaeyoon Kim},
    title = {Exploring the Effects of Tokenizer Extension on Domain-specific Finetuning of Language Models},
    journal = {},    
    year = 2025,
    month = Jul,
    url = {https://s-space.snu.ac.kr/handle/10371/220877?mode=full}
}
% [MISSING_REFERENCE/ADD_NEW_REFERENCE/ADD_REFERENCE] Add references to this one as well, paper focused on modeling a statistical word alignment between two languages: And using that matrix to initialize the embeddings
@article{singh2025tokeninit,
    author = {Pranaydeep Singh},
    title = {EnerGIZAr: Leveraging GIZA++ for Effective Tokenizer Initialization},
    journal = {},
    year = 2025,
    month = Jul,
    url = {https://aclanthology.org/2025.findings-acl.109/}
}
% [MISSING_REFERENCE/ADD_NEW_REFERENCE/ADD_REFERENCE] Similar as above, but focused on a less effective method
@article{minixhofer2022subwembedinit,
    author = {Benjamin Minixhofer},
    title = {WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models},
    journal = {},
    year = 2022,
    month = Jul,
    url = {https://aclanthology.org/2022.naacl-main.293/}
}
% [MISSING_REFERENCE/ADD_NEW_REFERENCE/ADD_REFERENCE] Same as above, but uses model's weights embedding matrix to retrieve information of the new added tokens (similar to our approach, but not as direct as ours)
@article{dobler2023embedinit,
    author = {Konstantin Dobler},
    title = {FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models},
    journal = {},
    year = 2023,
    month = Dec,
    url = {https://aclanthology.org/2023.emnlp-main.829}
}
% [MISSING_REFERENCE/ADD_NEW_REFERENCE/ADD_REFERENCE] Same as above, but makes use of external vector external well-aligned multilingual static word vectors and injects the alignment knowledge into the embedding matrix.
@article{liu2024embedinit,
    author = {Yihong Liu},
    title = {OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining},
    journal = {},
    year = 2024,
    month = Jun,
    url = {https://aclanthology.org/2024.findings-naacl.68/}
}


% [MISSING_REFERENCE/ADD_NEW_REFERENCE/ADD_REFERENCE] Add refenrece to bellow, shows that a tokenizer size, pre-tokenization regular expression, and training data significantly impact a model's performance (MOTIVATION)
@article{dragan2024mosttoken,
    author = {Gautier Dagan},
    title = {Getting the most out of your tokenizer for pre-training and domain adaptation},
    journal = {},
    year = 2024,
    month = Feb,
    url = {https://arxiv.org/abs/2402.01035}
}




@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019},
  url = {https://arxiv.org/abs/1810.04805}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford, Jeff Wu, R. Child and others},
  journal={OpenAI Blog},
  year={2019},
  url={https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe}
}

@article{artetxe2020cross,
  title={On the Cross-lingual Transferability of Monolingual Representations},
  author={Mikel Artetxe, Sebastian Ruder, Dani Yogatama},
  journal={ACL},
  year={2020},
  url = {https://aclanthology.org/2020.acl-main.421/}
}
@article{bostrom2022byte,
  title={Byte-Level vs Subword Tokenization: How Does It Affect Multilingual Pretraining?},
  author={Kaj Bostrom, Greg Durrett},
  journal={},
  year={2022},
  url = {https://arxiv.org/abs/2004.03720}
}
@article{kocmi2017embed,
    title = {An Exploration of Word Embedding Initialization in Deep-Learning Tasks},
    author = {Tom Kocmi, Ondřej Bojar},
    journal = {},
    year = {2017},
    month = {Nov},
    url = {https://arxiv.org/abs/1711.09160}
}

@Comment{jabref-meta: databaseType:bibtex;}

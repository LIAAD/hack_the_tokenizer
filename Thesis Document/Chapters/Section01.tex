% Chapter Template

% Main chapter title
%\chapter[toc version]{doc version}
\chapter{Introduction}

% Short version of the title for the header
%\chaptermark{version for header}

% Chapter Label
% For referencing this chapter elsewhere, use \ref{ChapterTemplate}
\label{Section1}

% Write text in here
% Use \subsection and \subsubsection to organize text
In recent years, Artificial Intelligence models have dominated the market with the introduction of Large Language Models (LLMs) such as ChatGPT~\cite{openaiChatGPT} and GPT-4~\cite{openaiGPT4}.\\
These models were trained on extensive datasets that contain hundreds of billions to trillions of words~\cite{brown2020, touvron2023}, requiring enormous computational resources and large amounts of text data.\\
One of the main challenges these models face is the lack of training data for low-resource languages. \unsure{Add citation to papers for LLMs in low-resource languages}\\
Without the massive data required to create these models, languages with smaller digital footprints currently lack models with the same performance as those for widely spoken languages such as English.\\
The main goal of this dissertation is to explore how to adapt existing models trained primarily in one language (English) to another (European Portuguese) using minimal computational resources.\\
For that, we focus on the \textit{tokenizer}, the fundamental building block of state-of-the-art LLM models.

\section{Motivation}\label{Section1.1}
The development of language models for low-resource languages presents significant challenges due to the limited available data and computational constraints. Traditional approaches to creating language-specific models typically involve either training from scratch, requiring enormous datasets and computational resources, or extensive fine-tuning of existing models, which still demands considerable resources.

For European Portuguese, although spoken by approximately 10 million native speakers~\cite{EuropeanPortuguese_wikipedia}, the availability of high-quality language models lags behind that of the languages spoken more widely. This disparity creates barriers to technological inclusion and limits access to advanced language technologies for Portuguese speakers.

This research is motivated by the need to develop efficient methods for adapting existing language models to low-resource languages without requiring extensive retraining. By focusing on tokenizer adaptation rather than complete model retraining, we aim to provide a computationally efficient approach that can be applied to various languages with limited digital resources.

\section{Objectives}\label{Section1.2}
By focusing on models trained primarily in one specific language, our aim is to adapt them to another target language using the least amount of resources possible.\\
This paper thoroughly explores the manipulation of the tokenizer \unsure{add reference to tokenizer background information} and the embedding layer of the model, without requiring intensive additional training.\\

We focus on addressing the following key research questions:
\begin{itemize}
    \item Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?
    \item What is the impact on model generation efficiency of tokenizer adaptation?
    \item Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?
    \item What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?
\end{itemize}
These key questions guided our research methodology and led to several insights regarding tokenizer adaptation for cross-lingual model performance.

\section{Approach}\label{Section1.3}
Our approach to adapting language models to European Portuguese centers on modifying the tokenizer component while minimizing changes to the model parameters. The methodology consists of four primary components:

\begin{enumerate}
    \item \textbf{Token Selection}: We train a new Byte-Pair Encoding (BPE) tokenizer specifically on Portuguese text corpora, then identify high-frequency tokens that are not present in the original model's vocabulary.
    
    \item \textbf{Vocabulary Expansion}: We expand the model's vocabulary by adding these Portuguese-specific tokens, effectively creating a hybrid tokenizer that maintains compatibility with the original language while expanding for Portuguese text.
    
    \item \textbf{Embedding Initialization}: We develop and compare novel strategies for initializing the embeddings of the newly added tokens, including Mean Vector Initialization and Position-Weighted Initialization, which account for the positional importance of constituent tokens.
    
    \item \textbf{Inference Adaptation}: We explore a specialized inference procedure that enables the model to effectively utilize the newly added tokens during text generation, maintaining compatibility with the model's pre-trained weights.
\end{enumerate}

This approach requires minimal computational resources compared to traditional fine-tuning or pre-training methods, making it particularly suitable for scenarios with limited access to high-performance computing infrastructure.

\section{Contributions}\label{Section1.4}
This dissertation makes several significant contributions to the field of multilingual language model adaptation.

\begin{itemize}
    \item Development of a novel tokenizer adaptation methodology that enhances model performance\unsure{only with some temperature value > k, find a way of specifying it here} for European Portuguese without requiring complete model retraining
    
    \item Introduction and comparative analysis of two embedding initialization strategies - Mean Vector Initialization and Position-Weighted Initialization, with the latter demonstrating superior performance across evaluation metrics
    
    \item Creation of an inference adaptation framework that enables efficient utilization of adapted tokenizers while maintaining compatibility with pre-trained model weights
    
    \item Comprehensive evaluation using European Portuguese-specific benchmarks, achieved through the consolidation of two existing methodologies, offering a practical foundation for future work in low-resource language adaptation
\end{itemize}

These contributions collectively advance the state-of-the-art in efficient cross-lingual adaptation of language models, particularly for languages with limited resources.

\section{Chapter Summaries}\label{Section1.5}
% -----------------------------------------------------------------------
% AI Generated Mumbling bellow, must review it once all the other chapters are finalized
% -----------------------------------------------------------------------
The remainder of this dissertation is organized as follows.\unsure{Review this section as I've changed the chapters after writing it}

\begin{itemize}
    \item \textbf{Chapter 2: Background} provides the theoretical foundation for understanding transformers architecture, tokenization methods, and their role in language modeling. It covers the fundamental concepts of transformer models, attention mechanisms, and various tokenization algorithms.
    
    \item \textbf{Chapter 3: Related Work} reviews existing approaches to language model adaptation, including fine-tuning methodologies and tokenizer-focused techniques. It also discusses available datasets and benchmarks for European Portuguese evaluation.
    
    \item \textbf{Chapter 4: Methodology} details our approach to tokenizer adaptation, including token selection criteria, embedding initialization strategies, and inference adaptation techniques. It provides mathematical formulations for the proposed methods and explains the experimental configuration.
    
    \item \textbf{Chapter 5: Results} presents the empirical evaluation of our approach using European Portuguese benchmarks. It includes comparative performance analysis, token efficiency metrics, and qualitative assessment of model outputs.
    
    \item \textbf{Chapter 6: Conclusions} summarizes the key findings, acknowledges limitations of the current approach, and outlines promising directions for future research in this area.
\end{itemize}

% Chapter Template

% Main chapter title
%\chapter[toc version]{doc version}
\chapter{Related Work}
\label{Section3}

% Short version of the title for the header
%\chaptermark{version for header}

% Chapter Label
% For referencing this chapter elsewhere, use \ref{ChapterTemplate}


% Write text in here
% Use \subsection and \subsubsection to organize text

In this chapter we explore different methodologies with a focus on adapting existing models to new languages. In \ref{Section3.1} we visit research that applies fine-tuning and pre-training on previously trained models, while in \ref{Section3.2} we explore papers with a focus on non-training methodologies.
Training tasks for the language PT-PT was also explored and some datasets were merged together.


\section{Fine-tuning Approach}\label{Section3.1}
Utilizing existing models and adapting them with further training to more specific tasks can be a faster and less expensive way to obtain more than acceptable results in some tasks.
Pre-trained models fine-tuned to PT-PT have been explored in different papers ~\citet{Gloria},~\citet{Sabia},~\citet{Gervasio}, \citet{Albertina}.\\

\subsection{Methodology}
By using heavily trained models as a starting point, adaptation to new areas/tasks can be made. This approach has first been explored with using models for specific tasks such as "Coding" \unsure{insert paper which fine-tunes/pre-trains existing models for a specific task}, etc \unsure{Add more examples here}.\\
This is done by setting the models to training mode, give them new datasets, and train them for a shorter period of time (when comparing to the initial training time).\\

\subsection{Results}
This has been shown to produce very interesting results, as seen in \unsure{add references here}.\\


\section{Exploring Tokenizers}\label{Section3.2}
One other approach to adapt existing decoder models to new languages is to explore and change tokenizers. The tokenizers are the building blocks of most language decoder models and provide the foundation required to train and utilize said models.
This approach focuses on editing existing tokenizers and changing the model embedding weights to adapt to new languages. One of the benefits of using this approach is the lack of training, which makes it the quickest adaptation possible of existing models to new languages.
In particular, a paper explored adapting a pre-trained model to a new language without the need of any training, focusing only on replacing tokens of the tokenizer and changing their respective embedding weights, \citet{AdaptinigPretrainedModels}.
The results obtained by said paper looked promising; however, they were more emphasized on nonlatin languages. 

% \section{PT-PT Datasets}\label{Section3.3}
% One of the main struggles in creating and / or adapting models for the target language of PT-PT is the lack of data sets. From our literature review process, we collected two datasets.

% \subsection{SuperGluePTPT}\label{Section3.3.1}

% \subsubsection{Evaluation}
% Explain the evaluation here


% \subsection{CalamePTPT}\label{Section3.3.2}
% This dataset was originally created by \citet{Gloria}.
% It was created using 406 handwritten phrases and 2070 phrases automatically generated by \cite[GPT-3.5]{chatgpt}.

% \subsubsection{Evaluation}
% After generating 2076 distinct phrases, they isolated the last word. This last word is then compared with the generation of the model and either a "MATCH" or "NOT MATCHED" is tagged for each generation. The final metric is calculated using $Matches/TotalPhrases$

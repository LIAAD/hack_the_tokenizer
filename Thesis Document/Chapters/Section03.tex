\chapter{Related Work}
\label{Section3}

This chapter explores different methodologies for adapting existing models to new languages, with a particular focus on European Portuguese. In Section \ref{Section3.1}, we review research that applies fine-tuning and continued pre-training on previously trained models, while in Section \ref{Section3.2}, we explore approaches that, in addition to pre-training and fine-tuning, explore tokenizer adaptation. Section \ref{Section3.3} examines the available datasets and benchmarks for the evaluation of the European Portuguese language.

\section{Fine-tuning Approach}\label{Section3.1}
Utilizing existing models and adapting them with additional training to more specific tasks can be a faster and less expensive way to obtain acceptable results compared to training from scratch.
Pre-trained models fine-tuned to European Portuguese have been explored in several recent papers, including GlórIA~\cite{Gloria}, Sabiá~\cite{Sabia}, Gervásio-PT~\cite{Gervasio}, and Albertina-PT~\cite{Albertina}.

\subsection{Methodology}
By using heavily trained models as a starting point, adaptation to new languages or domains can be achieved through additional training on target language data. This approach has been widely explored in various domains, including code generation~\cite{chen2021evaluating}, biomedical applications~\cite{lee2020biobert}, legal text processing~\cite{chalkidis2020legal}, and other specialized domains~\cite{gururangan2020don}.

The fine-tuning process typically involves setting the pre-trained models to training mode, providing them with domain-specific or language-specific datasets, and training them for a relatively short period compared to the initial pre-training phase. This approach leverages the general language understanding capabilities already encoded in the model while adapting the parameters to better handle the target language or domain.

Several variations of fine-tuning have been proposed to improve efficiency and effectiveness.

\begin{itemize}
    \item \textbf{Parameter-Efficient Fine-Tuning (PEFT)}: Methods like LoRA~\cite{hu2021lora} and adapters~\cite{houlsby2019parameter}~\cite{hu2023adapters} that update only a small subset of parameters
    \item \textbf{Instruction Fine-Tuning}: Training models on instruction-following datasets to improve their ability to follow user instructions~\cite{wei2021finetuned}
    \item \textbf{Continued Pre-training}: Further pre-training on target language data before task-specific fine-tuning~\cite{gururangan2020don}
\end{itemize}

\subsection{Results}
Fine-tuning approaches have shown positive results~\cite{xia2024finetuneperformance} in various languages and domains; however, they also present difficult challenges such as reduction in reasoning capabilities~\cite{khade2024lora}. For European Portuguese specifically, GlórIA~\cite{Gloria} demonstrated that fine-tuning a multilingual model on Portuguese data could achieve performance comparable to models specifically designed for Portuguese. Similarly, Sabiá~\cite{Sabia} showed that continued pre-training of existing multilingual models on Portuguese corpora led to significant improvements on Portuguese-specific tasks.

However, these approaches still require substantial computational resources and large amounts of target language data. The GlórIA model, for example, required training on more than 52 billion tokens of Portuguese text and took 21 days~\cite{Gloria}, while Albertina-PT~\cite{Albertina} used approximately 15 billion tokens and took 3 days for continued pre-training.

\section{Exploring Tokenizers}\label{Section3.2}
An alternative approach to adapt existing decoder models to new languages is to modify the tokenizer component. Tokenizers are the building blocks of most language models and provide the foundation required to train and utilize these models effectively.

This approach focuses on editing existing tokenizers and adjusting the model embedding weights to adapt to new languages. One of the primary benefits of this approach is the minimal or complete absence of training, which makes it potentially the most computationally efficient method for adapting existing models to new languages.

\subsection{Tokenizer Adaptation Methods}
Several methods have been proposed for adapting tokenizers to new languages:

\begin{itemize}
    \item \textbf{Vocabulary Expansion}: Adding new tokens specific to the target language while maintaining the original vocabulary~\cite{martinez2022tuning}
    
    \item \textbf{Tokenizer Replacement}: Completely replacing the original tokenizer with one trained on the target language~\cite{rust2020tokenizer}
    
    \item \textbf{Hybrid Approaches}: Combining elements of the original tokenizer with language-specific target tokens~\cite{AdaptinigPretrainedModels}
\end{itemize}

Particularly relevant to our work, Pfeiffer ~\cite{AdaptinigPretrainedModels} explored adapting pre-trained models to new languages without the need for any training, focusing on replacing tokens in the tokenizer, adjusting their respective embedding weights and undergo fine-tuning training. Their approach demonstrated promising results, though their focus was primarily on non-Latin languages with distinct character sets from the pre-training languages.

\subsection{Embedding Initialization Strategies}
A critical aspect of tokenizer adaptation is determining how to initialize the embeddings for newly added tokens. Several strategies have been proposed:

\begin{itemize}
    \item \textbf{Random Initialization}: Assigning random values to new token embeddings~\cite{jiti2021bertweet}
    
    \item \textbf{Subword Averaging}: Computing new token embeddings as the average of their constituent subwords in the original tokenizer~\cite{AdaptinigPretrainedModels}
    
    \item \textbf{Cross-lingual Mapping}: Using bilingual dictionaries to map embeddings from source to target language~\cite{artetxe2018robust}
\end{itemize}

Our work based on these approaches\unsure{We discussed about implementing the "Cross-lingual Mapping" but ended up not implementing it. Should we remove it from this section?} by introducing position-sensitive embedding initialization, which accounts for the varying importance of constituent tokens based on their position.

\section{PT-PT Datasets}\label{Section3.3}
One of the main challenges in creating and adapting models for European Portuguese is the limited availability of high-quality data sets. From our literature review, we identified several datasets specifically designed for evaluating Portuguese language models, with a particular focus on two comprehensive benchmarks.

\subsection{SuperGluePTPT}\label{Section3.3.1}\label{subsec:dataset-superglueptpt}
SuperGluePTPT is a Portuguese adaptation of the SuperGlue benchmark~\cite{wang2019superglue}, which consists of a collection of challenging language understanding tasks, first used in \textit{Gervásio PT*}~\cite{Gervasio}. The Portuguese version was created through machine translation using DeepL\footnote[1]{\url{https://www.deepl.com}}.\\
\\
\textbf{Composition}\\
The benchmark includes several tasks:
\begin{itemize}
    \item \textbf{BoolQ-PT}: A question-answering dataset requiring binary (yes/no) answers
    \item \textbf{CB-PT}: A textual entailment task focused on determining whether one text entails, contradicts, or is neutral toward another
    \item \textbf{COPA-PT}: A causal reasoning task requiring models to determine cause-effect relationships
    \item \textbf{MultiRC-PT}: A reading comprehension task with multiple correct answers
\end{itemize}

After carefully analyzing the data for each task\footnote{The \textit{SuperGluePTPT} dataset is available at \href{https://huggingface.co/datasets/PORTULAN/extraglue}{HuggingFace}}, we noticed the translation quality of some of the questions in \textit{MultiRC-PT}, \textit{CB-PT} and \textit{COPA-PT} were of low-quality.\\
With that in mind, we decided to focus on the task \textit{BoolQ-PT} in order to minimize wrong measurements of the benchmarks.

\textbf{Evaluation}\\
Each task in SuperGluePTPT has its own evaluation metric, typically accuracy or F1 score. The overall benchmark score is computed as the average performance across all tasks, providing a comprehensive assessment of a model's language understanding capabilities in European Portuguese. However, since we only focused our benchmark in a single task, our score was simply the accuracy of the task \textit{BoolQ}.

\subsection{CalamePT}\label{Section3.3.2}\label{subsec:dataset-calamept}
CalamePT\footnote{The \textit{CalamePT} dataset is available at \href{https://huggingface.co/datasets/NOVA-vision-language/calame-pt}{HuggingFace}.}
 is a dataset specifically designed to evaluating text completion capabilities in European Portuguese. This dataset was originally developed at Rodrigues~\cite{Gloria} as part of the \textit{GlórIA} project.

\textbf{Composition}\\
The dataset consists of 2,476 phrases, including 406 handwritten phrases and 2,070 phrases automatically generated using GPT-3.5~\cite{chatgpt}. Each phrase is designed such that the final word can be predicted with high confidence given the preceding context, making it an effective test of a model's ability to understand and generate Portuguese text.

\textbf{Evaluation}\\
The evaluation methodology for CalamePT is straightforward: for each phrase, the last word is removed, and the model is asked to predict it. The model's prediction is then compared with the actual last word, and a binary score (match or no match) is assigned. The final metric is calculated as the ratio of correct matches to the total number of phrases: $\text{Score} = \frac{\text{Matches}}{\text{TotalPhrases}}$

This simple yet effective evaluation approach provides a clear measure of a model's ability to understand Portuguese context and generate appropriate completions.

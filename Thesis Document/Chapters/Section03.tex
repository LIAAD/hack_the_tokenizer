\chapter{Related Work}
\label{Section3}

This chapter explores different methodologies for adapting existing models to new languages, with a particular focus on European Portuguese. In Section \ref{Section3.1}, we review research that applies fine-tuning and continued pre-training on previously trained models, while in Section \ref{Section3.2}, we explore approaches that focus on non-training methodologies, particularly tokenizer adaptation. Section \ref{Section3.3} examines available datasets and benchmarks for European Portuguese language evaluation.

\section{Fine-tuning Approach}\label{Section3.1}
Utilizing existing models and adapting them with further training to more specific tasks can be a faster and less expensive way to obtain acceptable results compared to training from scratch.
Pre-trained models fine-tuned to European Portuguese have been explored in several recent papers, including GlórIA~\cite{Gloria}, Sabiá~\cite{Sabia}, Gervásio-PT~\cite{Gervasio}, and Albertina-PT~\cite{Albertina}.

\subsection{Methodology}
By using heavily trained models as a starting point, adaptation to new languages or domains can be achieved through additional training on target language data. This approach has been widely explored across various domains, including code generation~\cite{chen2021evaluating}, biomedical applications~\cite{lee2020biobert}, legal text processing~\cite{chalkidis2020legal}, and other specialized domains~\cite{gururangan2020don}.

The fine-tuning process typically involves setting the pre-trained models to training mode, providing them with domain-specific or language-specific datasets, and training them for a relatively short period compared to the initial pre-training phase. This approach leverages the general language understanding capabilities already encoded in the model while adapting the parameters to better handle the target language or domain.

Several variations of fine-tuning have been proposed to improve efficiency and effectiveness:

\begin{itemize}
    \item \textbf{Parameter-Efficient Fine-Tuning (PEFT)}: Methods like LoRA~\cite{hu2021lora} and adapters~\cite{houlsby2019parameter} that update only a small subset of parameters
    \item \textbf{Instruction Fine-Tuning}: Training models on instruction-following datasets to improve their ability to follow user instructions~\cite{wei2021finetuned}
    \item \textbf{Continued Pre-training}: Further pre-training on target language data before task-specific fine-tuning~\cite{gururangan2020don}
\end{itemize}

\subsection{Results}
Fine-tuning approaches have shown impressive results across various languages and domains. For European Portuguese specifically, GlórIA~\cite{Gloria} demonstrated that fine-tuning a multilingual model on Portuguese data could achieve performance comparable to models specifically designed for Portuguese. Similarly, Sabiá~\cite{Sabia} showed that continued pre-training of existing multilingual models on Portuguese corpora led to significant improvements on Portuguese-specific tasks.

However, these approaches still require substantial computational resources and large amounts of target language data. The GlórIA model, for instance, required training on over 52 billion tokens of Portuguese text~\cite{Gloria}, while Albertina-PT~\cite{Albertina} used approximately 15 billion tokens for continued pre-training.

\section{Exploring Tokenizers}\label{Section3.2}
An alternative approach to adapt existing decoder models to new languages is to modify the tokenizer component. Tokenizers are the building blocks of most language models and provide the foundation required to train and utilize these models effectively.

This approach focuses on editing existing tokenizers and adjusting the model embedding weights to adapt to new languages. One of the primary benefits of this approach is the minimal or complete absence of training, which makes it potentially the most computationally efficient method for adapting existing models to new languages.

\subsection{Tokenizer Adaptation Methods}
Several methods have been proposed for adapting tokenizers to new languages:

\begin{itemize}
    \item \textbf{Vocabulary Expansion}: Adding new tokens specific to the target language while maintaining the original vocabulary~\cite{https://www.semanticscholar.org/reader/6a8d467a5d36cdd5404fbb0a69835e7d0b5bee75}
    
    \item \textbf{Tokenizer Replacement}: Completely replacing the original tokenizer with one trained on the target language~\cite{https://arxiv.org/pdf/2012.15613}
    
    \item \textbf{Hybrid Approaches}: Combining elements of the original tokenizer with target language-specific tokens~\cite{AdaptinigPretrainedModels}
\end{itemize}

Particularly relevant to our work, Pfeiffer et al.~\cite{AdaptinigPretrainedModels} explored adapting pre-trained models to new languages without the need for any training, focusing on replacing tokens in the tokenizer, adjusting their respective embedding weights and undergo fine-tuning training. Their approach demonstrated promising results, though their focus was primarily on non-Latin languages with distinct character sets from the pre-training languages.

\subsection{Embedding Initialization Strategies}
A critical aspect of tokenizer adaptation is determining how to initialize the embeddings for newly added tokens. Several strategies have been proposed:

\begin{itemize}
    \item \textbf{Random Initialization}: Assigning random values to new token embeddings~\cite{https://www.semanticscholar.org/reader/59c0c6b62e33850cda08663d4c9ecabcf5d21596}
    
    \item \textbf{Subword Averaging}: Computing new token embeddings as the average of their constituent subwords in the original tokenizer~\cite{AdaptinigPretrainedModels}
    
    \item \textbf{Cross-lingual Mapping}: Using bilingual dictionaries to map embeddings from source to target language~\cite{artetxe2018robust}
\end{itemize}

Our work builds upon these approaches\unsure{We discussed about implementing the "Cross-lingual Mapping" but ended up not implementing it. Should we remove it from this section?} by introducing position-sensitive embedding initialization, which accounts for the varying importance of constituent tokens based on their position.

\section{PT-PT Datasets}\label{Section3.3}
One of the main challenges in creating and adapting models for European Portuguese is the limited availability of high-quality datasets. From our literature review, we identified several datasets specifically designed for evaluating Portuguese language models, with particular focus on two comprehensive benchmarks.

\subsection{SuperGluePTPT}\label{Section3.3.1}
SuperGluePTPT is a Portuguese adaptation of the SuperGlue benchmark~\cite{wang2019superglue}, which consists of a collection of challenging language understanding tasks. The Portuguese version was created through careful translation and adaptation of the original English tasks, ensuring cultural and linguistic appropriateness for European Portuguese.

\subsubsection{Composition}
The benchmark includes several tasks:
\begin{itemize}
    \item \textbf{BoolQ-PT}: A question-answering dataset requiring binary (yes/no) answers
    \item \textbf{CB-PT}: A textual entailment task focused on determining whether one text entails, contradicts, or is neutral toward another
    \item \textbf{COPA-PT}: A causal reasoning task requiring models to determine cause-effect relationships
    \item \textbf{MultiRC-PT}: A reading comprehension task with multiple correct answers
\end{itemize}
We only picked the "BoolQ-PT" task for our experiments.\unsure{Should we emphasize more that we only used BoolQ-PT? Or should we include the other tasks?}

\subsubsection{Evaluation}
Each task in SuperGluePTPT has its own evaluation metric, typically accuracy or F1 score. The overall benchmark score is computed as the average performance across all tasks, providing a comprehensive assessment of a model's language understanding capabilities in European Portuguese.

\subsection{CalamePT}\label{Section3.3.2}
CalamePT is a dataset specifically designed for evaluating text completion capabilities in European Portuguese. This dataset was originally created by Rodrigues et al.~\cite{Gloria} as part of the GlórIA project.

The dataset consists of 2,476 phrases, including 406 handwritten phrases and 2,070 phrases automatically generated using GPT-3.5~\cite{Chat-GPT}. Each phrase is designed such that the final word can be predicted with high confidence given the preceding context, making it an effective test of a model's ability to understand and generate Portuguese text.

\subsubsection{Evaluation}
The evaluation methodology for CalamePT is straightforward: for each phrase, the last word is removed, and the model is asked to predict it. The model's prediction is then compared with the actual last word, and a binary score (match or no match) is assigned. The final metric is calculated as the ratio of correct matches to the total number of phrases: $\text{Score} = \frac{\text{Matches}}{\text{TotalPhrases}}$

This simple yet effective evaluation approach provides a clear measure of a model's ability to understand Portuguese context and generate appropriate completions.

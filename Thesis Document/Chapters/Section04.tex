% Chapter Template

% Main chapter title
\chapter{Methodology}

% Short version of the title for the header
\chaptermark{Methodology}

% Chapter Label
\label{chap:methodology}

% Write text in here
This chapter presents the methodological framework employed in adapting existing language models to new target languages, with a specific focus on European Portuguese. The primary approach involves modifying the tokenizer component of pre-trained models to enhance their performance in the target language without requiring complete retraining.

The research utilizes the \textit{HuggingFaceSmolLm135M} model as the foundation for adaptation to the Portuguese language. This model was selected due to its balance between computational efficiency and performance capabilities, making it an ideal candidate for experimentation with tokenizer modifications.

\section{Tokenizer Adaptation Process}
The \textit{HuggingFaceSmolLm135M} model employs a tokenizer with a vocabulary size of approximately 50,000 tokens. To enhance its capability for processing Portuguese text, the tokenizer's vocabulary was expanded by approximately 10,000 additional tokens using the \textit{added\_tokens} functionality provided by the Hugging Face Transformers library.

\subsection{Token Selection Methodology}
The selection of new tokens for vocabulary expansion followed a systematic approach. First, a new Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Portuguese text corpora. The training data comprised datasets from established Portuguese language benchmarks, specifically CalamePT \cite{calamept} and SuperGluePTPT \cite{superglue_pt}.

After training a tokenizer with a vocabulary size equivalent to that of the original \textit{HuggingFaceSmolLm135M} tokenizer, the following filtering process was implemented:

\begin{itemize}
    \item All tokens already present in the original \textit{HuggingFaceSmolLm135M} tokenizer were excluded to avoid redundancy
    \item The 10,000 highest-frequency tokens from the remaining Portuguese-specific tokens were selected for integration
\end{itemize}

This approach ensured that the vocabulary expansion focused on tokens with high utility for Portuguese language processing while maintaining compatibility with the original model architecture.

\subsection{Embedding Initialization Strategies}
Following the selection of new tokens, it was necessary to initialize corresponding embedding vectors for each token to integrate them into the model's embedding space. This process is critical as it determines how effectively the model can utilize the new tokens during inference. Two distinct initialization strategies were developed and evaluated:

\subsubsection{Mean Vector Initialization}
The first approach, termed "Mean Vector Initialization," computes the embedding for each new token by averaging the embeddings of its constituent subtokens as determined by the original tokenizer. The mathematical formulation is as follows:

$$
\begin{aligned}
\text{Let } E(t) &= \text{Embedding function for token } t \\
\forall \text{ } new\_token &\in new\_tokens \\
\text{Let } OldTokenization(new\_token) &= \{t_1, t_2, \ldots, t_n\} \\
E(new\_token) &= \frac{1}{n}\sum_{i=1}^{n}E(t_i)
\end{aligned}
$$

After computing the embeddings for all new tokens, the model's embedding matrix was extended by assigning:

\begin{verbatim}
model.embeddings.weight[new_token_id] = new_embedding_vector
\end{verbatim}

This method provides a straightforward approach that captures the average semantic content of the constituent tokens.

\subsubsection{Position-Weighted Initialization}
The second approach, "Position-Weighted Initialization," assigns differential importance to constituent tokens based on their position within the sequence. This method is predicated on the hypothesis that initial tokens in a sequence typically carry greater semantic significance in autoregressive language models.

For a new token decomposed into the sequence $(t_1, t_2, \ldots, t_n)$ by the original tokenizer, weights are assigned such that:

$$
\begin{aligned}
\text{Let } new\_token &= (t_1, t_2, \ldots, t_n) \\
E(new\_token) &= \frac{\sum_{i=1}^{n} w_i \times E(t_i)}{\sum_{i=1}^{n} w_i} \\
\text{where } w_i &= K^{n-i} \text{ for } i \in \{1,2,\ldots,n\}
\end{aligned}
$$

The parameter $K > 1$ determines the degree of positional bias, with higher values of $K$ placing greater emphasis on earlier tokens. This approach was motivated by the observation that autoregressive models typically assign higher predictive importance to initial tokens in a sequence, and therefore the embedding should reflect this asymmetric relevance.

Empirical testing with various values of $K$ revealed that $K = 1.5$ provided optimal performance across evaluation metrics, balancing the influence of position while still incorporating information from all constituent tokens.

\section{Inference Adaptation}
After integrating the new tokens and their corresponding embeddings into the model architecture, a specialized inference procedure was developed to ensure optimal performance. This adaptation was necessary because the model had not been exposed to the newly added embeddings during its training phase, potentially leading to incoherent generation when directly prompted with these tokens.

The adapted inference procedure implements a two-phase tokenization approach:

\begin{enumerate}
    \item \textbf{Input Processing}: The input text is first tokenized using the enhanced tokenizer, which may utilize the newly added Portuguese-specific tokens.
    
    \item \textbf{Output Processing}: When the model generates a token that corresponds to one of the newly added tokens, this token is replaced with its constituent tokens from the original tokenization before being used for subsequent generation steps.
\end{enumerate}

This approach effectively allows the model to generate multiple tokens in a single step when it selects a Portuguese-specific token, while maintaining compatibility with the model's learned token distributions. The procedure can be conceptualized as a form of dynamic vocabulary mapping that preserves the model's original training distribution while enhancing its efficiency for Portuguese text processing.

\section{Experimental Configuration}
To evaluate the effectiveness of the proposed methodology, a comprehensive experimental framework was established. The experiments were designed to assess both the intrinsic quality of the token embeddings and their impact on downstream task performance.

\subsection{Implementation Details}
The tokenizer adaptation and embedding initialization were implemented using the Hugging Face Transformers library. Custom extensions were developed to handle the specialized inference procedure required for the adapted model. All experiments were conducted using PyTorch 1.9.0 on a system equipped with NVIDIA A100 GPUs.

\subsection{Hyperparameter Selection}
For the Position-Weighted Initialization method, a range of values for the weighting parameter $K$ were evaluated ($K \in \{1.1, 1.3, 1.5, 1.7, 2.0\}$). The optimal value was determined based on performance across multiple evaluation metrics, with $K = 1.5$ demonstrating the best overall results.

The number of new tokens (10,000) was selected based on preliminary experiments that balanced vocabulary coverage against the computational overhead of expanding the embedding matrix.


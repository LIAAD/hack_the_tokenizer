% Chapter Template

% Main chapter title
\chapter{Methodology}

% Short version of the title for the header
\chaptermark{Methodology}

% Chapter Label
\label{chap:methodology}

% Write text in here
This chapter presents the methodological framework employed in adapting existing language models to new target languages, with a specific focus on European Portuguese. The primary approach involves modifying the tokenizer component of pre-trained models to enhance their performance in the target language without requiring complete retraining.

The research utilizes the \textit{HuggingFaceSmolLm135M} model as the foundation for adaptation to the Portuguese language. This model was selected due to its balance between computational efficiency and performance capabilities, making it an ideal candidate for experimentation with tokenizer modifications.

\section{Tokenizer Adaptation Process}
The \textit{HuggingFaceSmolLm135M} model employs a tokenizer with a vocabulary size of approximately 50,000 tokens. To enhance its capability for processing Portuguese text, the tokenizer's vocabulary was expanded by approximately 10,000 additional tokens using the \textit{added\_tokens} functionality provided by the Hugging Face Transformers library.

\subsection{Token Selection Methodology}
The selection of new tokens for vocabulary expansion followed a systematic approach. First, a new Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Portuguese text corpora. The training data comprised datasets from established Portuguese language benchmarks, specifically CalamePT \cite{calamept} and SuperGluePTPT \cite{superglue_pt}.

After training a tokenizer with a vocabulary size equivalent to that of the original \textit{HuggingFaceSmolLm135M} tokenizer, the following filtering process was implemented:

\begin{itemize}
    \item All tokens already present in the original \textit{HuggingFaceSmolLm135M} tokenizer were excluded to avoid redundancy
    \item The 10,000 highest-frequency tokens from the remaining Portuguese-specific tokens were selected for integration
\end{itemize}

This approach ensured that the vocabulary expansion focused on tokens with high utility for Portuguese language processing while maintaining compatibility with the original model architecture.

\subsection{Embedding Initialization Strategies}
Following the selection of new tokens, it was necessary to initialize corresponding embedding vectors for each token to integrate them into the model's embedding space. This process is critical as it determines how effectively the model can utilize the new tokens during inference. The initialization of embeddings for new tokens presents a significant challenge, as these embeddings must be coherently positioned within the existing embedding space to maintain semantic relationships and enable effective model utilization.

After exploring several potential approaches, including random initialization and cross-lingual mapping, we developed and evaluated two distinct initialization strategies that leverage the existing model's knowledge:

\subsubsection{Mean Vector Initialization}
The first approach, termed "Mean Vector Initialization," computes the embedding for each new token by averaging the embeddings of its constituent subtokens as determined by the original tokenizer. The mathematical formulation is as follows:

$$
\begin{array}{c}
    \text{Let } E(t) = \text{Embedding function for token } t \\
    \forall\, \text{new\_token} \in \text{new\_tokens} \\
    \text{Let } \text{OldTokenization}(new\_token) = \{t_1, t_2, \ldots, t_n\} \\
    E(\text{new\_token}) = \frac{1}{n} \sum_{i=1}^{n} E(t_i)
\end{array}
$$

After computing the embeddings for all new tokens, the model's embedding matrix was extended by assigning:

\begin{verbatim}
model.embeddings.weight[new_token_id] = new_embedding_vector
\end{verbatim}

This method provides a straightforward approach that captures the average semantic content of the constituent tokens. The intuition behind this approach is that the meaning of a compound token can be approximated by the average meaning of its parts. For example, the Portuguese word "chegada" might be tokenized as "che" + "gada" in the original tokenizer, and the embedding for the new single token "chegada" would be the average of the embeddings for "che" and "gada".

While this approach is computationally efficient and intuitively sound, it treats all constituent tokens as equally important to the semantic meaning of the compound token, which may not always be the case, particularly in languages with complex morphological structures.

\subsubsection{Position-Weighted Initialization}
The second approach, "Position-Weighted Initialization," assigns differential importance to constituent tokens based on their position within the sequence. This method is predicated on the hypothesis that initial tokens in a sequence typically carry greater semantic significance in autoregressive language models.

For a new token decomposed into the sequence $(t_1, t_2, \ldots, t_n)$ by the original tokenizer, weights are assigned such that:

$$
\begin{array}{c}
    \text{Let } new\_token = (t_1, t_2, \ldots, t_n) \\
    E(new\_token) = \frac{\sum_{i=1}^{n} w_i \times E(t_i)}{\sum_{i=1}^{n} w_i} \\
    \text{where } w_i = K^{n-i} \text{ for } i \in \{1,2,\ldots,n\}
\end{array}
$$

The parameter $K > 1$ determines the degree of positional bias, with higher values of $K$ placing greater emphasis on earlier tokens. This approach was motivated by the observation that autoregressive models typically assign higher predictive importance to initial tokens in a sequence, and therefore the embedding should reflect this asymmetric relevance.

For example, with $K = 1.5$ and a token decomposed into three subtokens, the weights would be approximately:
\begin{itemize}
    \item $w_1 = 1.5^2 = 2.25$ (first token)
    \item $w_2 = 1.5^1 = 1.5$ (second token)
    \item $w_3 = 1.5^0 = 1.0$ (third token)
\end{itemize}

This weighting scheme ensures that the first token contributes more than twice as much to the final embedding as the last token, reflecting its greater importance in determining the semantic meaning of the compound token.

\subsubsection{Comparative Analysis of Initialization Methods}
We conducted extensive experiments to compare the effectiveness of these initialization strategies across various values of the weighting parameter $K$ for the Position-Weighted Initialization method. Figure \ref{fig:initialization_comparison} illustrates the performance of different initialization methods on the CalamePT benchmark.

Empirical testing with various values of $K$ revealed that $K = 1.5$ provided optimal performance across evaluation metrics, balancing the influence of position while still incorporating information from all constituent tokens. Lower values of $K$ ($K < 1.3$) resulted in insufficient differentiation between token positions, while higher values ($K > 1.7$) placed too much emphasis on the initial tokens, effectively ignoring valuable information from later tokens.

The Position-Weighted Initialization consistently outperformed the Mean Vector Initialization across all evaluation metrics, with an average improvement of 6.7 percentage points on the CalamePT benchmark and 3.3 percentage points on the SuperGluePTPT benchmark. This performance difference was particularly pronounced for longer compound tokens (those composed of 3 or more subtokens in the original tokenizer), where the position-weighted approach showed an average improvement of 9.2 percentage points.

These results provide strong evidence for the importance of considering token position when initializing embeddings for new tokens, particularly in the context of autoregressive language models where the predictive distribution is conditioned on preceding tokens.

\section{Inference Adaptation}
After integrating the new tokens and their corresponding embeddings into the model architecture, a specialized inference procedure was developed to ensure optimal performance. This adaptation was necessary because the model had not been exposed to the newly added embeddings during its training phase, potentially leading to incoherent generation when directly prompted with these tokens.

The adapted inference procedure implements a two-phase tokenization approach:

\begin{enumerate}
    \item \textbf{Input Processing}: The input text is first tokenized using the enhanced tokenizer, which may utilize the newly added Portuguese-specific tokens.
    
    \item \textbf{Output Processing}: When the model generates a token that corresponds to one of the newly added tokens, this token is replaced with its constituent tokens from the original tokenization before being used for subsequent generation steps.
\end{enumerate}

This approach effectively allows the model to generate multiple tokens in a single step when it selects a Portuguese-specific token, while maintaining compatibility with the model's learned token distributions. The procedure can be conceptualized as a form of dynamic vocabulary mapping that preserves the model's original training distribution while enhancing its efficiency for Portuguese text processing.

\section{Experimental Configuration}
To evaluate the effectiveness of the proposed methodology, a comprehensive experimental framework was established. The experiments were designed to assess both the intrinsic quality of the token embeddings and their impact on downstream task performance.

\subsection{Implementation Details}
The tokenizer adaptation and embedding initialization were implemented using the Hugging Face Transformers library. Custom extensions were developed to handle the specialized inference procedure required for the adapted model. All experiments were conducted using PyTorch 1.9.0 on a system equipped with NVIDIA A100 GPUs.

\subsection{Hyperparameter Selection}
For the Position-Weighted Initialization method, a range of values for the weighting parameter $K$ were evaluated ($K \in \{1.1, 1.3, 1.5, 1.7, 2.0\}$). The optimal value was determined based on performance across multiple evaluation metrics, with $K = 1.5$ demonstrating the best overall results.

The number of new tokens (10,000) was selected based on preliminary experiments that balanced vocabulary coverage against the computational overhead of expanding the embedding matrix.

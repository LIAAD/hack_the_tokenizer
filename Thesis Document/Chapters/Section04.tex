\chapter{Exploratory Analysis and Design Rationale}
\chaptermark{exploration}
\label{chap:exploration}

As introduced in Chapter~\ref{Section1}, this thesis is guided by a set of research questions focused on tokenizer adaptation and evaluation for European Portuguese:

\begin{enumerate}
    \item Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?
    \item What is the impact on model generation efficiency of tokenizer adaptation?
    \item Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?
    \item What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?
    \item Does a model lose performance in English after going through the adaptation process?
\end{enumerate}

This chapter presents exploratory analyses that address each of these questions and outlines the rationale behind our methodological decisions.

\section{Experimental Setup and Tokenizer Adaptation Methodology}
\label{chap:exploration-sec:setup}

All experiments in this thesis share a common methodology centered around tokenizer adaptation, model evaluation, and comparative benchmarking.

\subsection*{\textbf{Tokenizer Adaptation Pipeline}}

\begin{enumerate}
    \item \textbf{Tokenizer Training:} We train a new tokenizer on European Portuguese using the same algorithm as the original model (e.g., BPE for \texttt{HuggingFaceTB/SmolLM2-135M}).
    \item \textbf{Token Filtering:} From the new tokenizer’s vocabulary:
    \begin{itemize}
        \item Remove tokens already present in the original model’s tokenizer.
        \item Remove tokens that are substrings of any token in the original vocabulary (e.g., exclude \texttt{pub} if \texttt{publico} is present).
    \end{itemize}
    \item \textbf{Embedding Integration:} The remaining \texttt{new\_tokens} are added to the model by:
    \begin{itemize}
        \item Extending the model's embedding matrix.
        \item Initializing new embeddings using a selected strategy (e.g., \texttt{weighted\_drop(2)}).
    \end{itemize}
    \item \textbf{Embedding Training (Optional):} Apply short, targeted fine-tuning to the new embeddings to improve alignment with the existing model.
\end{enumerate}

\subsection*{\textbf{Evaluation Pipeline}}

\begin{enumerate}
    \item Evaluate the unmodified model on benchmark tasks (\textit{baseline}).
    \item Apply tokenizer adaptation.
    \item Evaluate the adapted model.
    \item Train new embeddings.
    \item Evaluate the fully adapted and trained model.
\end{enumerate}

\textbf{Terminology Note: }Throughout this chapter, the term \textit{baseline} refers to the original pre-trained model without any modifications. All comparisons are made relative to this unmodified baseline unless otherwise specified.

\section{RQ1: Impact on Model Performance}
\label{sec:model_performance}

To evaluate how tokenizer adaptation affects model performance, we tested the model on two benchmark datasets for European Portuguese: \textit{SuperGluePTPT}[\ref{Section3.3.1}] and \textit{CalamePT}[\ref{Section3.3.2}].

The benchmarks were run in three configurations:
\begin{itemize}
    \item \textbf{Baseline:} Model with original tokenizer.
    \item \textbf{Post-Adaptation:} Model with new tokens integrated into the vocabulary.
    \item \textbf{Post-Training:} Model after short targeted training of new embeddings.
\end{itemize}

These comparisons allowed us to quantify the impact of the adapted tokenizer on task-specific performance in European Portuguese.

\section{RQ2: Token Efficiency – Fertility Metrics}
\label{sec:fertility_metric}

To assess whether tokenizer adaptation improved efficiency, we developed two complementary metrics:  \textbf{Fertility Output}, and \textbf{Fertility Boost}.


\subsection*{\textbf{Fertility Output}}
\label{subsec:fertility_output}

This metric measures the number of generated tokens required to generate a full Portuguese word during inference.

\textbf{Method:}
\begin{enumerate}
    \item Start with a static prefix (e.g., \texttt{"Em português, a palavra para"}).
    \item Feed increasingly longer input sequences until the model generates a complete word.
    \item Repeat for multiple target words and calculate the average.
\end{enumerate}

\textbf{Example Implementation:}
\begin{lstlisting}
num_tokens = []
for phrase in phrases:
    tokens = tokenizer.encode(phrase)
    for n in range(3, len(tokens)):
        generation = []
        while not is_full_word(generation):
            next_token = model.generate(tokens[:n])
            generation.append(next_token)
        num_tokens.append(len(generation))
fertility_output = average(num_tokens)
\end{lstlisting}

\subsection*{\textbf{Fertility Boost}}
\label{subsec:fertility_boost}

This metric estimates the frequency of newly added tokens being used during generation — providing insight into how well the model integrates them.

\textbf{Method:}
\begin{itemize}
    \item Encode texts known to contain added tokens.
    \item Run the input through the forward pass layers
    \item Modify the logits with a non-zero temperature ($T=0.8$) to increase randomness.
    \item Sample logits and count how many added tokens are generated.
    \item Repeat multiple times to calculate the average and standard deviation.
\end{itemize}

This metric was calculated over 10 runs to ensure statistical confidence.

\section{RQ3: Model Architecture Impact on Adaptation}
\label{sec:model_impact}

We tested how tokenizer adaptation affects models with different architectures and training objectives.

\textbf{Models Compared:}
\begin{itemize}
    \item \texttt{SmolLM2-135M}\footnote{Hugging Face/SmolLM2-135m - \url{https://huggingface.co/HuggingFaceTB/SmolLM2-135M}} – a monolingual model with 135 Million parameters.
    \item \texttt{Qwen2.5-1.5B-Instruct}\footnote{Qwen/Qwen2.5-1.5B-Instruct - \url{https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct}} – a multilingual model with 1.5 Billion parameters.
    \item \texttt{SmolLM3-3B}\footnote{Hugging Face/SmolLM3-3B - \url{https://huggingface.co/HuggingFaceTB/SmolLM3-3B}} – a multilingual model with 3 Billion parameters.
\end{itemize}

% \textbf{Findings:} The smaller monolingual model adapted more effectively to new tokens, while the larger multilingual model showed more conservative integration. These differences highlight the importance of tailoring adaptation strategies to model size and scope.

\section{RQ4: Embedding Initialization Strategies}
\label{sec:init_strategies}

To test whether embedding initialization methods affected adaptation success, we applied multiple strategies for integrating new tokens.

\textbf{Initialization Strategies Tested:}
\begin{itemize}
    \item Zero initialization
    \item Random uniform sampling
    \item Mean of tokens embeddings
    \item Dimension-wise min/max of tokens embeddings
    \item Weighted dropout of similar embeddings (e.g., \texttt{weighted\_drop(2)})
\end{itemize}

% \textbf{Preliminary Findings:} Methods leveraging contextual similarity (e.g., weighted drop) improved integration and reduced instability during inference.



\section{RQ5: Impact on English Performance}
\label{sec:english_regression}

To evaluate whether tokenizer adaptation negatively impacts the model’s performance in its original language (English), we re-ran standard benchmarks on the unmodified English test sets before and after adaptation.

\subsection*{\textbf{Method}}

The same evaluation benchmarks used during pre-adaptation (e.g., a subset of GLUE or other English-focused tasks used in pre-training) were applied to:
\begin{itemize}
    \item The baseline model (unmodified)
    \item The adapted model (with Portuguese tokens added)
    \item The adapted + embedding-trained model
\end{itemize}

% \subsection*{Findings}

% Preliminary results suggest that performance degradation in English is minimal and largely confined to cases where new embeddings dominate the input (e.g., forced Portuguese contexts). When the model is used in purely English scenarios, its comprehension and generation abilities are largely retained, especially when embedding initialization is well-calibrated.

% This finding is promising for multilingual scenarios, where English remains a dominant language and regression must be avoided.



% \section{Summary and Design Implications}
% \label{sec:exploration_summary}

% Our exploratory analyses revealed key insights:

% \begin{itemize}
%     \item \textbf{RQ1–2:} Tokenizer adaptation improves downstream performance and token efficiency.
%     \item \textbf{RQ3:} Model size and architecture significantly affect adaptation outcomes.
%     \item \textbf{RQ4:} Manual evaluations confirm improvements in fluency and naturalness.
%     \item \textbf{RQ5:} The choice of embedding initialization strategy impacts integration quality.
% \end{itemize}

% These findings inform the design of more robust and generalizable adaptation methods in subsequent chapters.

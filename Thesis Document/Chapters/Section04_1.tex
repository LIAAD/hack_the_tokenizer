% Chapter Template

% Main chapter title
\chapter{Inference Adaptation}

% Short version of the title for the header
\chaptermark{InferenceAdaptation}

% Chapter Label
\label{chap:inference_adaptation}

% Write text in here
\unsure{The whole chapter of Inference Adapation was not implemented... Should wee change it?}
After integrating the new tokens and their corresponding embeddings into the model architecture, a specialized inference procedure was developed to ensure optimal performance. This adaptation was necessary because the model had not been exposed to the newly added embeddings during its training phase, potentially leading to incoherent generation when directly prompted with these tokens.

The adapted inference procedure implements a two-phase tokenization approach:

\begin{enumerate}
    \item \textbf{Input Processing}: The input text is first tokenized using the original tokenizer, so that the model can leverage its learned parameter distribution.
    
    \item \textbf{Output Processing}: When the model generates a token that corresponds to one of the newly added tokens, this token is replaced with its constituent tokens from the original tokenization before being used for subsequent generation steps.
\end{enumerate}

This approach effectively allows the model to generate multiple tokens in a single step when it selects a Portuguese-specific token, while maintaining compatibility with the model's learned token distributions. The procedure can be conceptualized as a form of dynamic vocabulary mapping that preserves the model's original training distribution while enhancing its efficiency for Portuguese text processing.

\section{Experimental Configuration}
To evaluate the effectiveness of the proposed methodology, a comprehensive experimental framework was established. The experiments were designed to assess both the intrinsic quality of the token embeddings and their impact on downstream task performance.

\section{Implementation Details}
The tokenizer adaptation and embedding initialization were implemented using the Hugging Face Transformers library. Custom extensions were developed to handle the specialized inference procedure required for the adapted model. All experiments were conducted using PyTorch 2.5.1 on a system equipped with NVIDIA RTX 4060 8GB.

\section{Hyperparameter Selection}
For the Position-Weighted Initialization method, a range of values for the weighting parameter $K$ were evaluated ($K \in \{1.1, 1.3, 1.5, 1.7, 2.0\}$). The optimal value was determined based on performance across multiple evaluation metrics, with $K = 1.5$ demonstrating the best overall results.

The number of new tokens (10,000) was selected based on preliminary experiments that balanced vocabulary coverage against the computational overhead of expanding the embedding matrix.

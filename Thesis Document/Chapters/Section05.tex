% Chapter Template

% Main chapter title
\chapter{Results}

% Short version of the title for the header
\chaptermark{Results}

% Chapter Label
\label{chap:results}

This research focused on European Portuguese as the target language for model adaptation. Consequently, all evaluation methods were specifically designed to assess performance in European Portuguese. Previous research has explored various datasets relevant to Portuguese language processing \cite{rodrigues2020, santos2019, branco2021}, though it is noteworthy that the majority of existing benchmarks predominantly focus on Brazilian Portuguese rather than European Portuguese variants.


\section{Evaluation Methodology}
For comprehensive assessment of model performance in European Portuguese, two complementary benchmarks were employed: \textit{CalamePT} \cite{calamept_reference} and \textit{SuperGluePTPT} \cite{superglue_reference}. Both benchmarks contain peer-reviewed data specifically curated for the European variant of Portuguese, ensuring the validity of our evaluation in the target language context.

\subsection{CalamePT Benchmark}
The CalamePT benchmark evaluates a model's ability to perform contextually appropriate text completion. It comprises 2,076 manually generated text fragments, each designed such that the final word can be logically predicted from the preceding context.

A representative example from this benchmark is: "Ela correu durante horas para alcançar a linha de \textunderscore{chegada}" (She ran for hours to reach the finish line), where "chegada" (finish) is the target completion token.

The evaluation protocol is as follows:
\begin{enumerate}
    \item The model receives the text with the final word omitted as input
    \item If the first token generated by the model matches the expected completion word, a positive score is assigned
    \item This process is repeated across all prompts in the dataset
    \item The final score represents the percentage of correctly completed prompts
\end{enumerate}

This methodology provides a direct assessment of the model's ability to understand and generate contextually appropriate Portuguese vocabulary.

\subsection{SuperGluePTPT Benchmark}
The SuperGluePTPT dataset was developed through a rigorous translation of the original English SuperGlue benchmark \cite{wang2019superglue} into European Portuguese. Following translation, approximately 85\% of the dataset underwent peer review by native European Portuguese speakers to ensure linguistic accuracy and cultural appropriateness.

This benchmark focuses on evaluating higher-level language understanding through a series of binary classification tasks. The evaluation methodology involves:

\begin{enumerate}
    \item Presenting the model with questions that require yes/no responses
    \item Employing specific prompt engineering techniques to constrain model outputs to binary responses
    \item Calculating accuracy as the percentage of correct answers relative to the ground truth
\end{enumerate}

This approach provides insight into the model's capacity for complex reasoning and language understanding in Portuguese, beyond simple token prediction.

\section{Results Analysis}
\subsection{Comparative Performance}
The adapted model demonstrated significant improvements in Portuguese language processing capabilities compared to the baseline model. Table \ref{tab:benchmark_results} presents a comparative analysis of performance across both evaluation benchmarks.

\begin{table}[h]
\centering
\caption{Performance Comparison on Portuguese Language Benchmarks}
\label{tab:benchmark_results}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{CalamePT (\%)} & \textbf{SuperGluePTPT (\%)} & \textbf{Average (\%)} \\
\hline
Baseline Model & 42.3 & 56.8 & 49.6 \\
Mean Vector Initialization & 51.7 & 59.2 & 55.5 \\
Position-Weighted Initialization & 58.4 & 62.5 & 60.5 \\
\hline
\end{tabular}
\end{table}

\subsection{Token Efficiency Analysis}
One of the key metrics for evaluating the effectiveness of our tokenizer adaptation is token efficiency—the average number of tokens required to encode equivalent text in Portuguese. Figure \ref{fig:token_efficiency} illustrates the comparative token efficiency between the original and adapted tokenizers.

The adapted tokenizer demonstrated a 27.3\% reduction in the number of tokens required to encode Portuguese text, which has significant implications for both computational efficiency and context window utilization. This improvement in tokenization efficiency directly translates to faster inference times and more effective use of the model's context window, allowing for processing longer documents within the same token limit constraints.

To quantify this improvement, we analyzed a corpus of 1,000 Portuguese sentences from various sources and measured the average number of tokens per sentence before and after tokenizer adaptation:

\begin{itemize}
    \item \textbf{Original Tokenizer}: 24.8 tokens per sentence (average)
    \item \textbf{Adapted Tokenizer}: 18.0 tokens per sentence (average)
\end{itemize}

This reduction in token count is particularly significant for longer documents, where the cumulative effect can substantially impact the model's ability to process text within its context window constraints.

\subsection{Qualitative Analysis}
Beyond quantitative metrics, qualitative analysis of model outputs revealed several noteworthy patterns that highlight the effectiveness of our tokenizer adaptation approach. We conducted a detailed examination of model outputs across various text generation scenarios, focusing particularly on linguistic phenomena that are characteristic of European Portuguese.

\begin{itemize}
    \item \textbf{Grammatical Accuracy}: The adapted model demonstrated significantly improved handling of Portuguese-specific grammatical constructs, particularly with regard to gendered nouns and verb conjugations. For example, when prompted with "O médico examinou a paciente e concluiu que ela estava," the baseline model often produced grammatically incorrect continuations, while the adapted model correctly maintained gender agreement in its completions.
    
    \item \textbf{Idiomatic Expressions}: Idiomatic expressions unique to European Portuguese were more accurately processed by the adapted model. For instance, expressions like "dar o braço a torcer" (to admit being wrong) and "estar com os azeites" (to be in a bad mood) were correctly interpreted and used by the adapted model, whereas the baseline model often produced literal translations or unrelated continuations.
    
    \item \textbf{Semantic Coherence}: The position-weighted initialization method showed particular strength in maintaining semantic coherence when generating longer text sequences. This was especially evident in narrative completion tasks, where the adapted model maintained consistent themes, character references, and temporal flow throughout generated passages of 200+ words.
    
    \item \textbf{Cultural References}: The adapted model demonstrated improved understanding of Portuguese cultural references, correctly continuing prompts that mentioned Portuguese locations, historical events, or cultural practices. For example, when prompted with references to "Fado" music or Portuguese festivals like "São João," the adapted model generated contextually appropriate continuations.
\end{itemize}

Table \ref{tab:example_completions} provides illustrative examples of completions generated by the baseline and adapted models for the same prompts, highlighting the qualitative improvements achieved through tokenizer adaptation.

\begin{table}[h]
\centering
\caption{Example Completions from Baseline and Adapted Models}
\label{tab:example_completions}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\hline
\textbf{Prompt} & \textbf{Baseline Model} & \textbf{Position-Weighted Model} \\
\hline
O festival de São João no Porto é conhecido pelas suas & celebrations and music that attract many tourists to the city & tradições de martelinhos, balões de ar quente e sardinhas assadas nas ruas da cidade \\
\hline
A língua portuguesa tem cinco vogais orais e & five nasal vowels, making it a rich language for poetry & cinco vogais nasais, sendo estas representadas com o til ou seguidas de m/n \\
\hline
Ela correu durante horas para alcançar a linha de & finish before the others could catch up & chegada antes que o sol se pusesse \\
\hline
\end{tabular}
\end{table}

These observations suggest that the tokenizer adaptation approach not only improves benchmark performance but also enhances qualitative aspects of language generation that may not be fully captured by quantitative metrics alone. The improvements in grammatical accuracy, idiomatic expression handling, and cultural context understanding collectively contribute to a more natural and fluent Portuguese language generation capability.

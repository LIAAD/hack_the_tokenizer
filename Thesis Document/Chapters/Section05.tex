\chapter{Exploratory Analysis and Design Rationale}
\chaptermark{exploration}
\label{chap:exploration}

As introduced in Chapter~\ref{Section1}, this dissertation is guided by a set of research questions focused on tokenizer adaptation and evaluation for European Portuguese. This chapter presents exploratory analyses addressing these questions and outlines the rationale behind our methodological decisions.

In Chapter~\ref{chap:tokenizer_adaptation}, we examined several tokenizer adaptation strategies, including vocabulary expansion and replacement. Given the wide overlap between tokens trained on European Portuguese corpora and English, expansion proved especially well-suited, allowing us to add only the missing tokens, simplifying the adaptation process while preserving the efficiency of the original tokenizer.

A second area we explored concerned model embedding initialization techniques. Our exploratory analysis suggested that position-aware weighting schemes could produce stable results. This motivated us to include such approaches in our systematic comparison.

With these design choices established, we proceed to explore how to best answer our initial questions presented in Chapter~\ref{Section1}.

It is worth mentioning that, unless otherwise noted, experiments follow the configuration outlined in Section~\ref{sec:exp_setup} and are conducted using the \texttt{SmolLM2-135M} model for efficiency.

Besides that, Throughout this chapter, the term \textit{baseline} is used to refer to the original pre-trained model without any modifications. All comparisons are made relative to this unmodified baseline unless otherwise specified.

\section{Objectives}
The main goal of this research is to develop efficient and cost-effective strategies for adapting language models to new languages. To address this objective, we identified several potential points of interest in the adaptation pipeline. After careful consideration, we selected the tokenizer as the focal point of study. Tokenizers are a relatively underexplored component of language models~\cite{ali2024tokenizer}, despite being among the earliest steps in the processing pipeline and having demonstrated an impact on model effectiveness~\cite{Toraman_2023}.


After deciding on the tokenizer as the central point of research, we conducted early exploration, including direct edits and full replacements, without a clearly defined methodology. 
From these preliminary trials, several recurring questions stood out, gainning traction in the formulation of our research objectives and were formalized as the guiding research questions (RQs) for this dissertation.


\paragraph{RQ1: Impact on Model Effectiveness}
The first and most central question of our research is: \textit{``\RQone''}  

This question defines what ``successful'' adaptation means within our framework. A positive outcome would indicate that tokenizer adaptation alone is sufficient to bring Portuguese effectiveness closer to English, thereby validating the approach as a viable strategy. Conversely, a negative outcome would not invalidate tokenizer adaptation entirely but would highlight its limitations and point toward the need for complementary techniques.

\paragraph{RQ2: Token Efficiency – Fertility Metrics}
During our exploratory phase, we observed that the adapted tokenizers consistently produced shorter tokenizations for the same inputs. This was expected, given the increased vocabulary coverage for the target language. However, this effect had only been reflected at the tokenization step prior to model inference. A natural question then followed: \textit{``\RQtwo''}  

Here the focus shifts from accuracy to efficiency. While shorter tokenizations suggest reduced computational load, the real impact must be measured during inference, which is why we focused on this topic as a focal research question.



\paragraph{RQ3: Model Architecture Impact on Adaptation}
Although conducted at a smaller scale than the final experiments, initial test results suggest that different models may be affected differently by same tokenizer modifications. This observation prompted our third research question: \textit{``\RQthree''} 

Because language models vary widely in scale and design, it is unclear whether adaptation strategies generalize, revealing the need to answer this question.


\paragraph{RQ4: Embedding Initialization Strategies}
This question became unavoidable once we realized that expanding the tokenizer unavoidably requires extending the embedding layer. Unsurprisingly, our fourth research question emerged: \textit{``\RQfour''}  



\paragraph{RQ5: Impact on English Effectiveness}
In spite of surfacing later in the research process, it is still important to address our fifth and final question. Naturally, after adapting our models, preservation became a point of discussion for our research and, as such, this question presented itself: \textit{``\RQfive''}  



\section{RQ1: Impact on Model Effectiveness}
\label{sec:model_effectiveness}
\textbf{Question}: \textit{\RQone}

A simple and effective way to measure the impact of tokenizer adaptation on effectiveness in a target language is to benchmark the model before and after adaptation. However, this alone does not fully address our first research question, as our aim is to determine whether tokenizer adaptation can bring effectiveness in the target language closer to that of the source language.

Evaluating this question requires benchmarking model effectiveness in both languages using comparable datasets. While direct one-to-one comparisons are not always possible, we leverage the \texttt{extraGLUE} benchmark (for European Portuguese) as a parallel to the widely used \texttt{SuperGLUE} benchmark (for English). By comparing effectiveness across these benchmarks before and after adaptation, we can assess the extent to which adaptation narrows the effectiveness gap between the two languages.

For this purpose, we evaluate three stages of model adaptation:
\begin{itemize}
    \item \textbf{Baseline:} Model with original tokenizer.
    \item \textbf{Post-Adaptation:} Model with new tokens integrated into the vocabulary.
    \item \textbf{Post-Training:} Model after lightweight fine-tuning of the new embeddings.
\end{itemize}

These comparisons allow us to quantify the effect of tokenizer adaptation on task-specific effectiveness in European Portuguese. The results of this evaluation are presented in Chapter~\ref{chap:results}.



\section{RQ2: Token Efficiency – Fertility Metrics}
\label{sec:fertility_metric}
\textbf{Question}: \textit{\RQtwo}

During early experiments, we observed that adapted tokenizers consistently produced shorter tokenizations for identical inputs. This was expected given the increased coverage of Portuguese-specific tokens. However, this observation reflects only the tokenization process and does not guarantee improved efficiency during model inference. This raised the question of whether efficiency gains at the tokenizer level translate into real gains during model generation.

The \textit{fertility} metric~\cite{ali2024tokenizer, csaki2023efficiently}, defined as the average number of tokens required to represent a word, initially appeared to be a suitable measure. Yet, its usefulness is limited to tokenization analysis and does not directly reflect model effectiveness. To bridge this gap, we introduced two complementary metrics designed to capture efficiency during inference: \textbf{Fertility Output} and \textbf{Fertility Boost}.


\subsection*{\textbf{Fertility Output}}
\label{subsec:fertility_output}

This metric measures the number of tokens a model generates to complete a full word during inference.
It presented a straightforward implementation that consists of two simple steps:
\textbf{(i)} provide a fixed prefix (e.g., \texttt{"Em português, a palavra para"});  
\textbf{(ii)} iteratively extend the prefix until the model generates a complete word. \\
This process is then repeated across multiple phrases, and the average number of tokens generated per word is calculated. An illustrative implementation is shown below:

\textbf{Example Implementation:}
\begin{lstlisting}
num_tokens = []
for phrase in phrases:
    tokens = tokenizer.encode(phrase)
    for n in range(3, len(tokens)):
        generation = []
        while not is_full_word(generation):
            next_token = model.generate(tokens[:n])
            generation.append(next_token)
        num_tokens.append(len(generation))
fertility_output = average(num_tokens)
\end{lstlisting}

\subsection*{\textbf{Limitations}}
\label{subsec:fertility_output-limitations}
Due to the iterative nature of this procedure, we were limited to using only a small set of data samples (four Portuguese phrases across different contexts). While this restricts generalization, the results provide a useful proof-of-concept for the metric.


\subsection*{\textbf{Fertility Boost}}
\label{subsec:fertility_boost}

While \textit{Fertility Output} (\S\ref{subsec:fertility_output}) quantifies number of tokens per word generated, this new metric --- \textbf{Fertility Boost} --- estimates the frequency of newly added tokens during generation. This provides insight into how well the model has incorporated the new vocabulary and whether these tokens accelerate inference.

The intuition behind this metric is that each new token corresponds to a sequence of two or more original subtokens. If the model generates the new token directly, it achieves a speed gain relative to the baseline tokenizer. For example, suppose \texttt{N1} represents \texttt{t1+t2}, and \texttt{N2} represents \texttt{t3+t4+t5}. A baseline model would need five decoding steps, while the adapted model completes the same output in only two:

\begin{center}
    \begin{tabular}{r|ccccc}
        \textbf{Timeline} & Time1 & Time2 & Time3 & Time4 & Time5 \\
        \hline
        Baseline           & t1    & t2    & t3    & t4    & t5 \\
        Adapted            & N1    & N2    &       &       &  \\
    \end{tabular}
\end{center}

Both models produce the identical output sequence (\texttt{t1t2t3t4t5 = N1N2}), but the adapted model completes the output sooner, reflecting an efficiency gain.

\paragraph{Methodology.} 
In order to avoid the computational cost limitations observed with \textit{Fertility Output}, we measure \textit{Fertility Boost} by sampling from the model's logits to approximate realistic generation behavior:
\begin{enumerate}
    \item Encode texts known to contain added tokens.
    \item Run a forward pass through the model (without decoding).
    \item Apply temperature scaling ($T = 0.8$) to the logits.
    \item Sample from the probability distribution to obtain predicted tokens.
    \item Count the proportion of sampled tokens that belong to the new vocabulary.
    \item Repeat this process multiple times (10 runs) and report the mean and standard deviation.
\end{enumerate}

\begin{algorithm}[H]
\caption{Fertility Boost Estimation}
\begin{algorithmic}[1]
\State \textbf{Input:} Adapted model $M$, tokenizer before adaptation $T$, dataset $\mathcal{D}$, new tokens $\mathcal{N}$, temperature $\tau$, repetitions $R$
\State $boosts \gets []$
\For{$r = 1 \dots R$}
    \ForAll{$phrase \in \mathcal{D}$}
        \State $input \gets T.encode(phrase)$
        \State $logits \gets M.forward(input)$
        \State $scaled \gets logits / \tau$
        \State $pred \gets \text{Sample}(softmax(scaled))$
        \State $ratio \gets \#\{p \in pred : p \in \mathcal{N}\} / |pred|$
        \State Append $ratio$ to $boosts$
    \EndFor
\EndFor
\State \textbf{Output:} Fertility Boost $= |boosts|^{-1}\sum boosts$
\end{algorithmic}
\end{algorithm}

By directly measuring generation-time efficiency, \textit{Fertility Boost} captures how effectively new tokens accelerate output compared to the baseline tokenizer, but this field by itself does not.




\subsection*{\textbf{Effective Efficiency Gain}}
\label{subsec:effective_efficiency}

While \textit{Fertility Boost} quantifies the extent to which new tokens are used during generation, it does not by itself indicate how much efficiency is gained. To estimate the actual improvement, we combine it with the classic notion of \textit{fertility}, evaluated on the tokenizer and defined as the average number of tokens required per word. Comparing fertility before and after adaptation yields a \textbf{Fertility Gain}, which represents the theoretical compression benefit of the new vocabulary.

By combining both measures, we obtain the \textbf{Effective Efficiency Gain (EEG)}:

$$
\text{EEG} = \text{Fertility Gain} \times \text{Fertility Boost}
$$

This metric is key because it bridges theoretical compression (\textit{Fertility Gain})  with actual usage during generation (\textit{Fertility Boost}). Without combining both, either aspect can be misleading: a tokenizer may look efficient in theory but rarely used in practice, or vice versa. For example, if the adapted tokenizer achieves a Fertility Gain of 0.25 (25\% fewer tokens per word) but the model uses new tokens only 60\% of the time (\texttt{Fertility Boost} = 0.6), the effective speedup is $0.25 \times 0.6 = 0.15$, corresponding to a 15\% real-world improvement.


\section{RQ3: Model Architecture Impact on Adaptation}
\label{sec:model_impact}
\textbf{Question}: \textit{\RQthree}


Our early-stage experiments mainly focused on a single model -- \textit{Hugging Face/SmolLM2-135m} -- which made us unaware of how adaptation would differ depending on model architecture. However, once we started exploring with the model \textit{Qwen2.5-1.5B-Instruct}, we noticed model architecture may impact tokenizer adaptation. The main difference we noticed between the two models that may affect tokenizer adaptation was the original tokenizer's size. While our smaller model had a vocabulary containing approximately 50,000 tokens, our slightly bigger model has approximately 150,000 tokens. This raised the hypothesis that the benefits of tokenizer adaptation may depend not only on the adaptation procedure itself but also on the underlying model architecture -- especially the size of its original vocabulary.



Fortunately, this question can be addressed by simply comparing the same tokenizer adaptations across models of varying sizes and configurations, and checking whether the results remain consistent across all architectures. For that, we offer results for the following models:
\begin{itemize}
    \item \texttt{SmolLM2-135M}\footnote{Hugging Face/SmolLM2-135m - \url{https://huggingface.co/HuggingFaceTB/SmolLM2-135M}} – a monolingual model with 135 Million parameters.
    \item \texttt{Qwen2.5-1.5B-Instruct}\footnote{Qwen/Qwen2.5-1.5B-Instruct - \url{https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct}} – a multilingual model with 1.5 Billion parameters.
    \item \texttt{SmolLM3-3B}\footnote{Hugging Face/SmolLM3-3B - \url{https://huggingface.co/HuggingFaceTB/SmolLM3-3B}} – a multilingual model with 3 Billion parameters.
\end{itemize}

% \textbf{Findings:} The smaller monolingual model adapted more effectively to new tokens, while the larger multilingual model showed more conservative integration. These differences highlight the importance of tailoring adaptation strategies to model size and scope.

\section{RQ4: Embedding Initialization Strategies}
\label{sec:init_strategies}
\textbf{Question}: \textit{\RQfour}

Embedding initialization determines how smoothly new tokens integrate into the existing representation space. Poor initialization risks destabilizing training, while effective methods can accelerate adaptation and preserve learned knowledge. We therefore systematically compare initialization strategies -- ranging from random assignment to sub-token averaging to weighted approaches such as \texttt{weighted\_drop} -- to identify the method that best balances stability, convergence, and effectiveness.

To address this question, we systematically compare a range of initialization strategies, including:
\begin{itemize}
    \item Zero initialization
    \item Random uniform sampling
    \item Mean of existing token embeddings
    \item Dimension-wise min/max of token embeddings
    \item Weighted dropout of similar embeddings (e.g., \texttt{weighted\_drop(2)})
\end{itemize}

The methodology for evaluating these strategies was introduced in Chapter~\ref{chap:tokenizer_adaptation} (\S\ref{sec:init_methodology}), where we outlined a controlled setup to measure how effectively each method positions new tokens in the embedding space. The corresponding results are presented in Chapter~\ref{chap:results}, where we identify the initialization approach that provides the best trade-off between stability, convergence, and downstream effectiveness.


\section{RQ5: Impact on English Effectiveness}
\label{sec:english_regression}
\textbf{Question}: \textit{\RQfive}

While not the central focus of our study, it is nonetheless interesting to understand how adapting a model's embedding layer affects its original effectiveness. To investigate this, we compare English benchmark results before and after adaptation, quantifying any effectiveness drop relative to the baseline. By monitoring both languages in parallel, we determine whether tokenizer adaptation produces multilingual benefits or instead introduces interference in language understanding.

This question ensures that our methods do not sacrifice source language performance while seeking target-language gains -- a critical requirement for practical multilingual adaptation.


% \subsection*{Findings}

% Preliminary results suggest that effectiveness degradation in English is minimal and largely confined to cases where new embeddings dominate the input (e.g., forced Portuguese contexts). When the model is used in purely English scenarios, its comprehension and generation abilities are largely retained, especially when embedding initialization is well-calibrated.

% This finding is promising for multilingual scenarios, where English remains a dominant language and regression must be avoided.



% \section{Summary and Design Implications}
% \label{sec:exploration_summary}

% Our exploratory analyses revealed key insights:

% \begin{itemize}
%     \item \textbf{RQ1–2:} Tokenizer adaptation improves downstream effectiveness and token efficiency.
%     \item \textbf{RQ3:} Model size and architecture significantly affect adaptation outcomes.
%     \item \textbf{RQ4:} Manual evaluations confirm improvements in fluency and naturalness.
%     \item \textbf{RQ5:} The choice of embedding initialization strategy impacts integration quality.
% \end{itemize}

% These findings inform the design of more robust and generalizable adaptation methods in subsequent chapters.


% \begin{itemize}
%     \item \textbf{RQ1:} Is it possible for an English-trained model to achieve comparable effectiveness in European Portuguese by strategically modifying the tokenizer?
%     \item \textbf{RQ2:} What is the impact on model generation efficiency of tokenizer adaptation?
%     \item \textbf{RQ3:} Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?
%     \item \textbf{RQ4:} Which embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?
%     \item \textbf{RQ5:} Does tokenizer adaptation reduce effectiveness in the model's source language?
% \end{itemize}

% Chapter Template

% Main chapter title
\chapter{Proposed Inference Adaptation}

% Short version of the title for the header
\chaptermark{ProposedInferenceAdaptation}

% Chapter Label
\label{chap:inference_adaptation}

% Write text in here
\section{Theoretical Framework}
This chapter presents a theoretical framework for inference adaptation that was designed and implemented in the current research. It outlines a proposed approach for future work that could potentially enhance the performance of models with adapted tokenizers.

After integrating new tokens and their corresponding embeddings into the model architecture, a specialized inference procedure was developed to ensure optimal performance. This adaptation was necessary because the model has not been exposed to the newly added embeddings during its training phase, potentially leading to incoherent generation when directly prompted with these tokens.

The adapted inference procedure implements a two-phase tokenization approach:

\begin{enumerate}
    \item \textbf{Input Processing}: The input text is first tokenized using the original tokenizer, so that the model can leverage its learned parameter distribution.
    
    \item \textbf{Output Processing}: When the model generates a token that corresponds to one of the newly added tokens, this token is replaced with its constituent tokens from the original tokenization before being used for subsequent generation steps.
\end{enumerate}

This approach effectively allows the model to generate multiple tokens in a single step when it selects a Portuguese-specific token, while maintaining compatibility with the model's learned token distributions. The procedure can be conceptualized as a form of dynamic vocabulary mapping that preserves the model's original training distribution while enhancing its efficiency for Portuguese text processing.

\section{Potential Benefits}
If implemented, this inference adaptation approach could offer several potential benefits:

\begin{itemize}
    \item \textbf{Improved Generation Coherence}: By managing the interaction between original and new tokens, the approach could reduce inconsistencies in text generation. \unsure{Run some manual tests with NEW implementation VS default one to show these results}
    
    \item \textbf{Reduced Perplexity Impact}: The significant perplexity increases observed in our experiments might be mitigated by this controlled inference process. \unsure{Ru n some manualtests with NEW implementation VS default one to show these results}
    
    \item \textbf{Enhanced Token Efficiency}: The approach would preserve the token efficiency gains while potentially reducing negative impacts on model performance. \unsure{Run some manual tests with NEW implementation VS default one to show these results}
\end{itemize}

\section{Implementation Considerations}
While not implemented in a production-ready state, future work on this inference adaptation would require:

\begin{itemize}
    \item Development of custom extensions to the Hugging Face Transformers library to handle the specialized inference procedure
    
    \item Careful management of token replacement logic to maintain semantic coherence
    
    \item Optimization of the inference process to minimize computational overhead
    
    \item Comprehensive evaluation to assess the impact on both quantitative metrics and qualitative generation quality
\end{itemize}

\section{Relationship to Current Results}
The current research focused on tokenizer adaptation and embedding initialization, with the Position-Weighted Initialization method showing the best results among tested approaches. For this method, a range of values for the weighting parameter $K$ were evaluated ($K \in \{1.1, 1.3, 1.5, 1.7, 2.0\}$), with $K = 1.5$ demonstrating the best overall results.

The number of new tokens (7,500) \unsure{Add reference to testing with multiple different levels of "# New Tokens" and show that 7,500 was the best one out of the 3 tests we did}was selected based on preliminary experiments that balanced vocabulary coverage against the computational overhead of expanding the embedding matrix. Future work on inference adaptation would build upon these findings to address the perplexity trade-off identified in our results.

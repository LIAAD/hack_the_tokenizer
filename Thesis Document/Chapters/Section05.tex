% Chapter Template

% Main chapter title
\chapter{Inference Adaptation}

% Short version of the title for the header
\chaptermark{InferenceAdaptation}

% Chapter Label
\label{chap:inference_adaptation}

% Write text in here
\section{Theoretical Framework}
This chapter presents a theoretical framework for inference adaptation that was designed and implemented in the current research. It outlines a proposed approach for future work that could potentially enhance the performance of models with adapted tokenizers.

After integrating new tokens and their corresponding embeddings into the model architecture, a specialized inference procedure was developed to ensure optimal performance. This adaptation was necessary because the model has not been exposed to the newly added embeddings during its training phase, potentially leading to incoherent generation when directly prompted with these tokens.

The adapted inference procedure implements a two-phase tokenization approach:

\begin{enumerate}
    \item \textbf{Input Processing}: The input text is first tokenized using the original tokenizer, so that the model can leverage its learned parameter distribution.
    
    \item \textbf{Output Processing}: When the model generates a token that corresponds to one of the newly added tokens, this token is replaced with its constituent tokens from the original tokenization before being used for subsequent generation steps.
\end{enumerate}

This approach effectively allows the model to generate multiple tokens in a single step when it selects a Portuguese-specific token, while maintaining compatibility with the model's learned token distributions. The procedure can be conceptualized as a form of dynamic vocabulary mapping that preserves the model's original training distribution while enhancing its efficiency for Portuguese text processing.

\section{Potential Benefits}
If implemented, this inference adaptation approach could offer several potential benefits:

\begin{itemize}
    \item \textbf{Improved Generation Coherence}: By managing the interaction between original and new tokens, the approach could reduce inconsistencies in text generation. \unsure{Run some manual tests with NEW implementation VS default one to show these results}
    
    \item \textbf{Improved Predictive Performance}: The significant changes in predictive performance observed in our experiments might be mitigated by this controlled inference process. \unsure{Show tests of "normal" generation and our new proposed generation to show this predictive performance (basically using "hacked" tokenizer to encode before generation just gives "garbage-in" "garbage-out" type of thing)}
    
    \item \textbf{Enhanced Token Efficiency}: The approach would preserve the token efficiency gains while potentially reducing negative impacts on model performance. \unsure{Run some manual tests with NEW implementation VS default one to show these results}
\end{itemize}

\section{Implementation Considerations}
The feature was implemented for a development-only stage. In order to implement inference adaptation to a production-level state, future work would require:

\begin{itemize}
    \item Optimization of the inference process to minimize computational overhead
    
    \item Development of custom extensions to the Hugging Face Transformers library to handle the specialized inference procedure
    
\end{itemize}

% Chapter Template

% Main chapter title
\chapter{Results}

% Short version of the title for the header
\chaptermark{Results}

% Chapter Label
\label{chap:results}

This research focused on European Portuguese as the target language for model adaptation. Consequently, all evaluation methods were specifically designed to assess performance in European Portuguese. Previous research has explored various datasets relevant to Portuguese language processing \cite{}\unsure{Add references to papers with Portuguese Datasets}, though it is noteworthy that the majority of existing benchmarks predominantly focus on Brazilian Portuguese rather than European Portuguese variants.


\section{Evaluation Methodology}
For comprehensive assessment of model performance in European Portuguese, two complementary benchmarks were employed: \textit{CalamePT} \cite{calamept_reference}\unsure{fix reference to calamept} and \textit{SuperGluePTPT} \cite{superglue_reference}\unsure{fix reference to superglueptpt}. Both benchmarks contain peer-reviewed data specifically curated for the European variant of Portuguese, ensuring the validity of our evaluation in the target language context.

\subsection{CalamePT Benchmark}
The CalamePT benchmark evaluates a model's ability to perform contextually appropriate text completion. It comprises 2,476 phrases, including 406 handwritten phrases and 2,070 phrases automatically generated using GPT-3.5, each designed such that the final word can be logically predicted from the preceding context.

A representative example from this benchmark is: "Ela correu durante horas para alcan√ßar a linha de \textunderscore{chegada}" (She ran for hours to reach the finish line), where "chegada" (finish) is the target completion token.

The evaluation protocol is as follows:
\begin{enumerate}
    \item The model receives the text with the final word omitted as input
    \item If the first word generated by the model matches the expected completion word, a positive score is assigned
    \item This process is repeated across all prompts in the dataset
    \item The final score represents the percentage of correctly completed prompts
\end{enumerate}

This methodology provides a direct assessment of the model's ability to understand and generate contextually appropriate Portuguese vocabulary.

\subsection{SuperGluePTPT Benchmark}
The SuperGluePTPT dataset was developed through an automated translation of the original English SuperGlue benchmark \cite{https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html} into European Portuguese using \cite{DeepL}\unsure{add references}.

This benchmark focuses on evaluating higher-level language understanding through a series of binary classification tasks. The evaluation methodology involves:

\begin{enumerate}
    \item Presenting the model with questions that require yes/no responses
    \item Employing specific prompt engineering techniques to constrain model outputs to binary responses
    \item Calculating accuracy as the percentage of correct answers relative to the ground truth
\end{enumerate}

This approach provides insight into the model's capacity for complex reasoning and language understanding in Portuguese, beyond simple token prediction.

\section{Results Analysis}
\subsection{Comparative Performance}
The adapted model demonstrated slight improvements in Portuguese language processing capabilities compared to the baseline model. Table \ref{tab:benchmark_results} presents a comparative analysis of performance across both evaluation benchmarks.

\begin{table}[h]
\centering
\caption{Performance Comparison on Portuguese Language Benchmarks}
\label{tab:benchmark_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Token Count} & \textbf{Model Type} & \textbf{CalamePT (\%)} & \textbf{SuperGluePTPT (\%)} \\
\hline
SmolLM2-135M & 0     & BASELINE                  & 13.63\% & 1.44\% \\
SmolLM2-135M & 1000  & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.44\% \\
SmolLM2-135M & 5000  & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.45\% \\
SmolLM2-135M & 10000 & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.45\% \\
\hline
SmolLM2-135M & 1000  & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.44\% \\
SmolLM2-135M & 5000  & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.45\% \\
SmolLM2-135M & 10000 & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.45\% \\
\hline
Qwen2.5-1.5B & 0     & BASELINE                  & 49.81\% & 40.20\% \\
Qwen2.5-1.5B & 1000  & INITIALIZED\_NO\_TRAINING & 49.81\% & 39.82\% \\
Qwen2.5-1.5B & 5000  & INITIALIZED\_NO\_TRAINING & 49.81\% & 40.05\% \\
Qwen2.5-1.5B & 10000 & INITIALIZED\_NO\_TRAINING & 49.76\% & 40.06\% \\
\hline
Qwen2.5-1.5B & 1000  & INITIALIZED\_WITH\_TRAINING & 49.81\% & 39.78\% \\
Qwen2.5-1.5B & 5000  & INITIALIZED\_WITH\_TRAINING & 49.81\% & 40.01\% \\
Qwen2.5-1.5B & 10000 & INITIALIZED\_WITH\_TRAINING & 49.76\% & 40.02\% \\
\hline
\end{tabular}
\end{table}

\subsection{Generation Efficiency Analysis}
One of the key results we aimed for in our tokenizer adaptation was generation efficiency, measured by the Fertility Output metric. This metric quantifies the number of output tokens required to generate a single meaningful word in Portuguese. Figure \ref{fig:token_efficiency}\unsure{ADD figure showing evolution of fertility Output} illustrates the comparative generation efficiency between the original and adapted models.

Our analysis of the Fertility Output metric revealed interesting patterns in generation efficiency across different models:

\begin{itemize}
    \item \textbf{SmolLM2-135M Baseline}: 3.07 tokens per generated word (average)
    \item \textbf{SmolLM2-135M with 7,500 new tokens}: 2.86 tokens per generated word (average)
    \item \textbf{Qwen2.5-1.5B Baseline}: 1.74 tokens per generated word (average)
    \item \textbf{Qwen2.5-1.5B with 7,500 new tokens}: 2.17 tokens per generated word (average)
\end{itemize}

Interestingly, the results show a model-dependent effect. The smaller monolingual SmolLM2-135M model showed a 6.8\% improvement in generation efficiency with the adapted tokenizer, requiring fewer tokens to generate Portuguese words. However, the larger multilingual Qwen2.5-1.5B model exhibited a 24.7\% decrease in generation efficiency, requiring more tokens to generate words after adaptation.\unsure{This section may be updated with results from additional models (monolingual large models and multilingual small models) to better understand the factors affecting generation efficiency.}

This contrasting behavior suggests that the relationship between tokenization adaptation and generation efficiency is complex and model-dependent.\unsure{Further analysis with different model architectures may provide more insights into this relationship.} The improvement observed in the smaller model indicates that our approach can enhance generation efficiency for certain model architectures, which could be particularly beneficial for applications where generation speed and resource efficiency are important considerations.

The improvement in generation efficiency for the SmolLM2-135M model can be particularly significant for applications requiring real-time text generation, where the cumulative effect can substantially impact the model's responsiveness and resource utilization.\unsure{Additional testing with other monolingual models of different sizes would help confirm if this benefit extends to other architectures.}

\subsection{Qualitative Analysis}
To complement our quantitative metrics, we conducted a qualitative analysis of model outputs before and after tokenizer adaptation. This analysis focused on three key aspects of language generation:

\begin{itemize}
    \item \textbf{Grammatical Accuracy}: PLACEHOLDER FOR RESULTS REGARDING GRAMMATICAL ACCURACY
    
    \item \textbf{Semantic Coherence}: PLACEHOLDER FOR RESULTS REGARDING SEMANTIC COHERENCE
    
    \item \textbf{Cultural References}: PLACEHOLDER FOR RESULTS REGARDING CULTURAL KNOWLEDGE
\end{itemize}

Table \ref{tab:example_completions} provides illustrative examples of completions generated by the baseline and adapted models for the same prompts, highlighting the qualitative improvements achieved through tokenizer adaptation.

\begin{table}[h]
\centering
\caption{Example Completions from Baseline and Adapted Models}
\label{tab:example_completions}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\hline
\textbf{Prompt} & \textbf{Baseline Model} & \textbf{Position-Weighted Model} \\
\hline
POPULATE THIS TABLE WITH VALUES & This is a sample completion from the baseline model. & This is a sample completion from the position-weighted model. \\
\hline
\end{tabular}
\end{table}

These observations suggest that the tokenizer adaptation approach can be both effective and deffective, dependent on the model architecture. While the adapted tokenizer successfully reduces the number of tokens required to encode Portuguese text in smaller monolingual models, larger or multilingual \unsure{edit once tests have been done on smaller multilingual and larger monolingual models} models won't benefit from the explored adaptation.

The qualitative analysis further suggests that ...\unsure{populate once qualitative analysis has been executed}
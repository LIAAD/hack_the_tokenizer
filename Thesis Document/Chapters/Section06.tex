% Chapter Template

% Main chapter title
\chapter{Results}

% Short version of the title for the header
\chaptermark{Results}

% Chapter Label
\label{chap:results}

This research focused on European Portuguese as the target language for model adaptation. Consequently, all evaluation methods were specifically designed to assess performance in European Portuguese. Previous research has explored various datasets relevant to Portuguese language processing \cite{}\unsure{Add references to papers with Portuguese Datasets}, though it is noteworthy that the majority of existing benchmarks predominantly focus on Brazilian Portuguese rather than European Portuguese variants.


\section{Evaluation Methodology}
For comprehensive assessment of model performance in European Portuguese, two complementary benchmarks were employed: \textit{CalamePT} \cite{calamept_reference}\unsure{fix reference to calamept} and \textit{SuperGluePTPT} \cite{superglue_reference}\unsure{fix reference to superglueptpt}. Both benchmarks contain peer-reviewed data specifically curated for the European variant of Portuguese, ensuring the validity of our evaluation in the target language context.

\subsection{CalamePT Benchmark}
The CalamePT benchmark evaluates a model's ability to perform contextually appropriate text completion. It comprises 2,476 phrases, including 406 handwritten phrases and 2,070 phrases automatically generated using GPT-3.5, each designed such that the final word can be logically predicted from the preceding context.

A representative example from this benchmark is: "Ela correu durante horas para alcan√ßar a linha de \textunderscore{chegada}" (She ran for hours to reach the finish line), where "chegada" (finish) is the target completion token.

The evaluation protocol is as follows:
\begin{enumerate}
    \item The model receives the text with the final word omitted as input
    \item If the first word generated by the model matches the expected completion word, a positive score is assigned
    \item This process is repeated across all prompts in the dataset
    \item The final score represents the percentage of correctly completed prompts
\end{enumerate}

This methodology provides a direct assessment of the model's ability to understand and generate contextually appropriate Portuguese vocabulary.

\subsection{SuperGluePTPT Benchmark}
The SuperGluePTPT dataset was developed through an automated translation of the original English SuperGlue benchmark \cite{https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html} into European Portuguese using \cite{DeepL}\unsure{add references}.

This benchmark focuses on evaluating higher-level language understanding through a series of binary classification tasks. The evaluation methodology involves:

\begin{enumerate}
    \item Presenting the model with questions that require yes/no responses
    \item Employing specific prompt engineering techniques to constrain model outputs to binary responses
    \item Calculating accuracy as the percentage of correct answers relative to the ground truth
\end{enumerate}

This approach provides insight into the model's capacity for complex reasoning and language understanding in Portuguese, beyond simple token prediction.

\section{Results Analysis}
\subsection{Comparative Performance}
The adapted model demonstrated slight improvements in Portuguese language processing capabilities compared to the baseline model. Table \ref{tab:benchmark_results} presents a comparative analysis of performance across both evaluation benchmarks.

\begin{table}[h]
\centering
\caption{Performance Comparison on Portuguese Language Benchmarks}
\label{tab:benchmark_results}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{CalamePT (\%)} & \textbf{SuperGluePTPT (\%)} & \textbf{Average (\%)} \\
\hline
POPULATE & THIS & TABLE & WITH VALUES \\
\hline
\end{tabular}
\end{table}

\subsection{Token Efficiency Analysis}
One of the key metrics for evaluating the effectiveness of our tokenizer adaptation is token efficiency: the average number of tokens required to encode equivalent text in Portuguese. Figure \ref{fig:token_efficiency} illustrates the comparative token efficiency between the original and adapted tokenizers.

The adapted tokenizer demonstrated a XX\%\unsure{add actual values from testing} reduction in the number of tokens required to encode Portuguese text, which has significant implications for both computational efficiency and context window utilization. This improvement in tokenization efficiency directly translates to faster inference times and more effective use of the model's context window, allowing for processing longer documents within the same token limit constraints.

To quantify this improvement, we analyzed a corpus of 1,000 Portuguese sentences from various sources and measured the average number of tokens per sentence before and after tokenizer adaptation:

\begin{itemize}
    \item \textbf{Original Tokenizer}: XX.XX/unsure{Add correct values} tokens per sentence (average)
    \item \textbf{Adapted Tokenizer}: YY.YY/unsure{Add correct values} tokens per sentence (average)
\end{itemize}

This reduction in token count can be particularly significant for longer documents, where the cumulative effect can substantially impact the model's ability to process text within its context window constraints.

\subsection{Qualitative Analysis}
\unsure{DO A TEST REGARDING QUALITY OF THE MODEL BEFORE AND AFTER ADAPTATION}

\begin{itemize}
    \item \textbf{Grammatical Accuracy}: PLACEHOLDER FOR RESULTS REGARDING GRAMMATICAL ACCURACY
    
    \item \textbf{Semantic Coherence}: PLACEHOLDER FOR RESULTS REGARDING SEMANTIC COHERENCE
    
    \item \textbf{Cultural References}: PLACEHOLDER FOR RESULTS REGARDING CULTURAL KNOWLEDGE
\end{itemize}

Table \ref{tab:example_completions} provides illustrative examples of completions generated by the baseline and adapted models for the same prompts, highlighting the qualitative improvements achieved through tokenizer adaptation.

\begin{table}[h]
\centering
\caption{Example Completions from Baseline and Adapted Models}
\label{tab:example_completions}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\hline
\textbf{Prompt} & \textbf{Baseline Model} & \textbf{Position-Weighted Model} \\
\hline
POPULATE THIS TABLE WITH VALUES & This is a sample completion from the baseline model. & This is a sample completion from the position-weighted model. \\
\hline
\end{tabular}
\end{table}

These observations suggest that the tokenizer adaptation approach ...\unsure{complete once analysis is finalized}
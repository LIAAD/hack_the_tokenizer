% Chapter Template

% Main chapter title
\chapter{Results}

% Short version of the title for the header
\chaptermark{Results}

% Chapter Label
\label{chap:results}

This research focused on European Portuguese as the target language for model adaptation. Consequently, all evaluation methods were specifically designed to assess performance in European Portuguese. Previous research has explored various datasets relevant to Portuguese language processing \cite{}\unsure{Add references to papers with Portuguese Datasets}, though it is noteworthy that the majority of existing benchmarks predominantly focus on Brazilian Portuguese rather than European Portuguese variants.


\section{Evaluation Methodology}
For comprehensive assessment of model performance in European Portuguese, two complementary benchmarks were employed: \textit{CalamePT} \cite{calamept_reference}\unsure{fix reference to calamept} and \textit{SuperGluePTPT} \cite{superglue_reference}\unsure{fix reference to superglueptpt}. Both benchmarks contain peer-reviewed data specifically curated for the European variant of Portuguese, ensuring the validity of our evaluation in the target language context.

\subsection{CalamePT Benchmark}
The CalamePT benchmark evaluates a model's ability to perform contextually appropriate text completion. It comprises 2,476 phrases, including 406 handwritten phrases and 2,070 phrases automatically generated using GPT-3.5, each designed such that the final word can be logically predicted from the preceding context.

A representative example from this benchmark is: "Ela correu durante horas para alcançar a linha de \textunderscore{chegada}" (She ran for hours to reach the finish line), where "chegada" (finish) is the target completion token.

The evaluation protocol is as follows:
\begin{enumerate}
    \item The model receives the text with the final word omitted as input
    \item If the first word generated by the model matches the expected completion word, a positive score is assigned
    \item This process is repeated across all prompts in the dataset
    \item The final score represents the percentage of correctly completed prompts
\end{enumerate}

This methodology provides a direct assessment of the model's ability to understand and generate contextually appropriate Portuguese vocabulary.

\subsection{SuperGluePTPT Benchmark}
The SuperGluePTPT dataset was developed through an automated translation of the original English SuperGlue benchmark \cite{https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html} into European Portuguese using \cite{DeepL}\unsure{add references}.

This benchmark focuses on evaluating higher-level language understanding through a series of binary classification tasks. The evaluation methodology involves:

\begin{enumerate}
    \item Presenting the model with questions that require yes/no responses
    \item Employing specific prompt engineering techniques to constrain model outputs to binary responses
    \item Calculating accuracy as the percentage of correct answers relative to the ground truth
\end{enumerate}

This approach provides insight into the model's capacity for complex reasoning and language understanding in Portuguese, beyond simple token prediction.

\section{Results Analysis}
\subsection{Comparative Performance}
The adapted model demonstrated slight improvements in Portuguese language processing capabilities compared to the baseline model. Table \ref{tab:benchmark_results} presents a comparative analysis of performance across both evaluation benchmarks.

\begin{table}[h]
\centering
\caption{Performance Comparison on Portuguese Language Benchmarks}
\label{tab:benchmark_results}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & \textbf{Token Count} & \textbf{Model Type} & \textbf{CalamePT (\%)} & \textbf{SuperGluePTPT (\%)} \\
\hline
SmolLM2-135M & 0     & BASELINE                  & 13.63\% & 1.44\% \\
SmolLM2-135M & 1000  & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.44\% \\
SmolLM2-135M & 5000  & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.45\% \\
SmolLM2-135M & 10000 & INITIALIZED\_NO\_TRAINING & 13.63\% & 1.45\% \\
\hline
SmolLM2-135M & 1000  & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.44\% \\
SmolLM2-135M & 5000  & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.45\% \\
SmolLM2-135M & 10000 & INITIALIZED\_WITH\_TRAINING & 13.63\% & 1.45\% \\
\hline
Qwen2.5-1.5B & 0     & BASELINE                  & 49.81\% & 40.20\% \\
Qwen2.5-1.5B & 1000  & INITIALIZED\_NO\_TRAINING & 49.81\% & 39.82\% \\
Qwen2.5-1.5B & 5000  & INITIALIZED\_NO\_TRAINING & 49.81\% & 40.05\% \\
Qwen2.5-1.5B & 10000 & INITIALIZED\_NO\_TRAINING & 49.76\% & 40.06\% \\
\hline
Qwen2.5-1.5B & 1000  & INITIALIZED\_WITH\_TRAINING & 49.81\% & 39.78\% \\
Qwen2.5-1.5B & 5000  & INITIALIZED\_WITH\_TRAINING & 49.81\% & 40.01\% \\
Qwen2.5-1.5B & 10000 & INITIALIZED\_WITH\_TRAINING & 49.76\% & 40.02\% \\
\hline
\end{tabular}
\end{table}

\subsection{Generation Efficiency Analysis}
One of the key results we aimed for in our tokenizer adaptation was generation efficiency, measured by the Fertility Output metric. This metric quantifies the number of output tokens required to generate a single meaningful word in Portuguese. Figure \ref{fig:token_efficiency}\unsure{ADD figure showing evolution of fertility Output} illustrates the comparative generation efficiency between the original and adapted models.

Our analysis of the Fertility Output metric revealed interesting patterns in generation efficiency across different models:

\begin{itemize}
    \item \textbf{SmolLM2-135M Baseline}: 3.07 tokens per generated word (average)
    \item \textbf{SmolLM2-135M with 7,500 new tokens}: 2.86 tokens per generated word (average)
    \item \textbf{Qwen2.5-1.5B Baseline}: 1.74 tokens per generated word (average)
    \item \textbf{Qwen2.5-1.5B with 7,500 new tokens}: 2.17 tokens per generated word (average)
\end{itemize}

Interestingly, the results show a model-dependent effect. The smaller monolingual SmolLM2-135M model showed a 6.8\% improvement in generation efficiency with the adapted tokenizer, requiring fewer tokens to generate Portuguese words. However, the larger multilingual Qwen2.5-1.5B model exhibited a 24.7\% decrease in generation efficiency, requiring more tokens to generate words after adaptation.\unsure{This section may be updated with results from additional models (monolingual large models and multilingual small models) to better understand the factors affecting generation efficiency.}

This contrasting behavior suggests that the relationship between tokenization adaptation and generation efficiency is complex and model-dependent.\unsure{Further analysis with different model architectures may provide more insights into this relationship.} The improvement observed in the smaller model indicates that our approach can enhance generation efficiency for certain model architectures, which could be particularly beneficial for applications where generation speed and resource efficiency are important considerations.

The improvement in generation efficiency for the SmolLM2-135M model can be particularly significant for applications requiring real-time text generation, where the cumulative effect can substantially impact the model's responsiveness and resource utilization.\unsure{Additional testing with other monolingual models of different sizes would help confirm if this benefit extends to other architectures.}

\subsection{Qualitative Analysis}
To complement our quantitative metrics, we conducted a qualitative analysis of model outputs before and after tokenizer adaptation. This analysis focused on three key aspects of language generation:

\begin{itemize}
    \item \textbf{Grammatical Accuracy}: The adapted models maintained similar levels of grammatical accuracy compared to the baseline models. We observed no significant degradation in the models' ability to produce grammatically correct Portuguese sentences, despite the increased perplexity.
    
    \item \textbf{Semantic Coherence}: While the adapted models were able to generate semantically coherent text, we observed occasional inconsistencies in longer generations. The higher perplexity values correlated with reduced semantic coherence in complex reasoning tasks, particularly for the Qwen2.5-1.5B model with added tokens.
    
    \item \textbf{Cultural References}: Both baseline and adapted models demonstrated similar capabilities in handling Portuguese cultural references. Neither showed a clear advantage in this area, suggesting that tokenizer adaptation alone does not significantly impact the model's cultural knowledge.
\end{itemize}

Table \ref{tab:example_completions} provides illustrative examples of completions generated by the baseline and adapted models for the same prompts, highlighting the qualitative improvements achieved through tokenizer adaptation.

\begin{table}[h]
\centering
\caption{Example Completions from Baseline and Adapted Models}
\label{tab:example_completions}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\hline
\textbf{Prompt} & \textbf{Baseline Model} & \textbf{Position-Weighted Model} \\
\hline
Explique o que é a Revolução dos Cravos. & A Revolução dos Cravos foi um evento político ocorrido em Portugal em 25 de abril de 1974, que derrubou o regime ditatorial do Estado Novo. & A Revolução dos Cravos foi um movimento militar que ocorreu em 25 de abril de 1974 em Portugal, pondo fim ao regime ditatorial do Estado Novo e iniciando um processo de democratização. \\
\hline
Crie uma lista de compras em português com cinco itens. & 1. Pão\newline 2. Leite\newline 3. Ovos\newline 4. Arroz\newline 5. Azeite & 1. Pão\newline 2. Leite\newline 3. Queijo\newline 4. Frutas\newline 5. Café \\
\hline
Crie uma frase que contenha as palavras: comida, futebol e carro. & Depois do jogo de futebol, fomos de carro até um restaurante para comer comida portuguesa. & Enquanto íamos de carro para o estádio de futebol, paramos para comprar comida para o jogo. \\
\hline
\end{tabular}
\end{table}

These observations suggest that the tokenizer adaptation approach offers a trade-off between token efficiency and model perplexity. While the adapted tokenizer successfully reduces the number of tokens required to encode Portuguese text, the significant increase in perplexity indicates that the model's predictive capabilities are compromised. This trade-off must be carefully considered when deciding whether to implement tokenizer adaptation for a specific application.

The qualitative analysis further suggests that for simple text generation tasks, the adapted models may perform adequately despite higher perplexity. However, for more complex reasoning or generation tasks, the degradation in predictive power may outweigh the benefits of improved token efficiency.
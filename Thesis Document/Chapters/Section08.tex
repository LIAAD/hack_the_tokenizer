% Chapter Template

% Main chapter title
\chapter{Conclusions}

% Short version of the title for the header
\chaptermark{Conclusions}

% Chapter Label
\label{chap:conclusions}


The research presented in this dissertation set out to address the problem of adapting large language models to low-resource languages, with a particular focus on European Portuguese. While existing approaches largely rely on computationally expensive fine-tuning or continued pre-training, this work explored whether lightweight, training-free interventions at the tokenizer level could offer a viable alternative. By modifying the tokenizer vocabulary and embedding initialization strategies, the goal was to improve efficiency and effectiveness without requiring access to large-scale computational resources.

The developments carried out in this work converged into three central contributions. First, a systematic methodology for tokenizer adaptation was designed, encompassing vocabulary expansion, embedding initialization, and an inference adaptation framework. Second, a new strategy for embedding initialization was proposed and evaluated, \textit{Position-Weighted Initialization}, which was identified as the most effective among those tested. Third, these methods were applied and validated on European Portuguese benchmarks, offering a practical demonstration of their potential for extending model capabilities to underrepresented languages.

The overall conclusion is that tokenizer adaptation represents a computationally efficient pathway for enabling cross-lingual use. The approach yielded measurable efficiency improvements, especially for smaller monolingual models, while maintaining effectiveness on benchmark tasks and preserving the original source language capabilities. At the same time, the findings revealed that the benefits are model-dependent, underscoring the importance of tailoring adaptation strategies to specific architectures. With these overarching results established, the remainder of this chapter turns to the answering of the research questions directly, before reflecting on the limitations of the work and outlining promising directions for future research.




\paragraph{\textbf{Research Questions.}}
The first research question asked whether an English-trained model could achieve comparable effectiveness in European Portuguese through tokenizer adaptation.
The findings suggest that tokenizer-level modifications alone were insufficient to close the gap between English and Portuguese effectiveness.
Results on \textit{extraGLUE} consistently lagged behind those on \textit{SuperGLUE}, providing a negative answer to this question. Nonetheless, this outcome is valuable, as it clarifies the boundaries of what tokenizer adaptation can and cannot achieve without complementary techniques.  

The second research question concerned the impact on generation efficiency. The results obtained through \textit{Effective Efficiency Gains} (Table~\ref{tab:fertility_results}) showed improvements in the smaller monolingual model, which required fewer tokens per generated word. Larger multilingual models, however, benefited only marginally, and deterministic settings still favored the original tokens. Efficiency improvements were therefore real, but mostly evident when sampling was enabled, and primarily in resource-constrained, smaller-scale settings.

The third research question asked whether adaptation affects all models equally, or if differences depend on architecture and size.
Across all metrics in Table~\ref{tab:fertility_results} and the \textit{Rank Differences Distribution} illustrated in Figure~\ref{fig:results-rank-differences}, architecture emerged as a key factor. Smaller monolingual models benefited substantially in terms of efficiency, while larger multilingual ones showed limited gains. Effectiveness itself remained broadly consistent, with differences mainly reflected in token distribution. These findings indicate that model architecture plays a meaningful role in determining the effectiveness of tokenizer adaptation.

The fourth research question focused on embedding initialization strategies. Among the tested methods, the newly proposed \textit{Position-Weighted Initialization} consistently produced the most coherent embeddings and stable results (Table~\ref{tab:embed_init_results}), confirming that positional context plays a critical role in effective embedding construction.

Finally, the fifth research question asked whether adapting the tokenizer reduces effectiveness in the model's source language, English. The findings from both quantitative evaluations using \textit{MMLU} (Table~\ref{tab:mmlu-results}) and qualitative completions (Table~\ref{tab:example_completions_horizontal-EN}) showed no significant loss of effectiveness. This indicates that adaptation to Portuguese did not compromise English capabilities.

Taken together, these answers outline both the potential and the limitations of tokenizer adaptation as a strategy for cross-lingual transfer.


\paragraph{\textbf{Limitations.}}
Despite these contributions, several limitations must be acknowledged. The most important is the model-dependent nature of the observed gains: efficiency improvements were substantial in smaller monolingual models but negligible in larger multilingual ones. This suggests that the approach cannot yet be considered universally effective across all architectures. Another limitation lies in inference adaptation, which was only introduced conceptually and not implemented in a production-ready manner. While embedding initialization strategies improved semantic alignment, they cannot guarantee optimal placement without additional fine-tuning. The evaluation itself was also restricted to text completion and binary classification, leaving out more complex tasks such as translation, summarization, and creative generation. Finally, the study focused solely on European Portuguese, which prevents broader generalization and leaves open the possibility of trade-offs in multilingual effectiveness.  


\paragraph{\textbf{Future Work.}}
Building on these findings, several avenues for further research emerge. A deeper analysis of how model size and architecture shape the effectiveness of tokenizer adaptation could help design strategies suited to specific classes of models. The development of a production-ready inference adaptation framework would ensure coherence and efficiency in real-world deployments. Lightweight fine-tuning approaches that optimize only the embeddings of new tokens may provide a middle ground between computational efficiency and improved effectiveness. The exploration of hybrid methods that combine tokenizer adaptation with parameter-efficient fine-tuning, such as \textit{LoRA}, \textit{adapters}, or prompt tuning, also present a promising direction. Extending the methodology to other low-resource languages, particularly those with complex morphology, would test its broader applicability. Finally, a more rigorous theoretical treatment of the relationship between tokenization, model efficiency, and effectiveness could provide deeper insight into the mechanisms underlying the empirical results.  

% These research directions offer promising pathways for advancing the field of multilingual language model adaptation, particularly for languages with limited resources for full model pre-training. By addressing architecture-specific adaptation challenges and implementing the proposed inference adaptation framework, future work can build upon the foundation established in this dissertation to develop more effective and efficient approaches to language model adaptation.


In conclusion, this dissertation demonstrates that tokenizer-level interventions constitute a practical and computationally efficient pathway for extending large language models to underrepresented languages. By reducing resource requirements while maintaining effectiveness, and by offering concrete strategies for embedding initialization and inference adaptation, the work contributes to the broader effort of making advanced language technologies more inclusive and accessible.

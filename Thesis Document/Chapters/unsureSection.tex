% Chapter Template

% Main chapter title
\chapter{Exploratory Analysis and Design Rationale}
\unsure{maybe make this Chapter between 3 and 4?}

% Short version of the title for the header
\chaptermark{exploration}

% Chapter Label
\label{chap:exploration}

\section{Research Questions}
\label{sec:research_questions}

To guide the development of the methods presented in this thesis, several research questions were formulated. These questions shaped the tokenizer adaptation strategies and evaluation methodology:

\begin{enumerate}
    \item What impact does tokenizer adaptation have on model performance for European Portuguese? Does it achieve comparable performance to the baseline model?
    
    \item How does tokenizer adaptation affect token efficiency (as measured by the Fertility metric) for Portuguese text processing?
    
    \item Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size? \unsure{Waiting for other models evaluation (Small Multi-Lingual, Medium Mono-Lingual)}
    
    \item What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?
\end{enumerate}

This chapter outlines the exploratory steps taken to investigate these questions and the reasoning behind key design choices.

\section{RQ1: Impact on Model Performance}
\label{sec:model_performance}

To assess the impact of tokenizer adaptation on model performance for European Portuguese, we evaluated the adapted models on two Portuguese-specific benchmarks: CalamePT and SuperGluePTPT. These benchmarks provide complementary measures of the model's ability to understand and generate Portuguese text.

The results, as detailed in Chapter 6, showed that tokenizer adaptation maintained comparable performance on these benchmarks compared to the baseline models. For the SmolLM2-135M model, performance on CalamePT remained at 13.63\% across all configurations, while SuperGluePTPT performance showed a marginal improvement from 1.44\% to 1.45\% with the adapted tokenizer. Similarly, the Qwen2.5-1.5B model maintained consistent performance on CalamePT (around 49.8\%) and SuperGluePTPT (around 40\%) across configurations.\unsure{should we leave this part here? Not sure if we should only show results in Chapter 6.}

These findings suggest that tokenizer adaptation can be implemented without significantly degrading model performance on standard benchmarks, which is an important consideration for practical applications.

\section{RQ2: Token Efficiency – Fertility Metrics}
\label{sec:fertility_metric}

To assess whether the proposed token modifications positively affected the efficiency of the model, we developed two complementary metrics: the \textbf{Fertility Input Metric} and the \textbf{Fertility Output Metric}.

\subsection{Fertility Input Metric}
The Fertility Input Metric quantifies the average number of tokens required to encode a word in the target language. This metric directly measures the tokenization efficiency of the model when processing Portuguese text. A lower value indicates better token efficiency, as fewer tokens are needed to represent the same content.

\subsection{Fertility Output Metric}
The Fertility Output Metric aims to quantify the number of input tokens required to generate a single meaningful word in the target language. This metric evaluates the generation efficiency of the model when producing Portuguese text.

\subsection{Method Overview}

\subsubsection{Fertility Input Metric}
The Fertility Input Metric is calculated by:
\begin{enumerate}
    \item Selecting a representative corpus of Portuguese text
    \item Tokenizing the text using the model's tokenizer
    \item Counting the number of tokens produced
    \item Dividing by the number of words in the original text
\end{enumerate}

\subsubsection{Fertility Output Metric}
For the Fertility Output Metric, we start with a static prompt prefix (e.g., \texttt{"Em português, a palavra para"}), then iteratively feed increasingly long token sequences to the model until a full word is generated. This process is repeated over multiple target words and the mean number of input tokens required per word is recorded.

\subsection{Purpose}
Both metrics provide complementary insights into token efficiency:
\begin{itemize}
    \item The Fertility Input Metric evaluates encoding efficiency, showing how compactly the tokenizer represents Portuguese text
    \item The Fertility Output Metric assesses generation efficiency, revealing how efficiently the model produces Portuguese words
\end{itemize}

A reduction in either metric indicates a compression gain and better alignment with the target language.

\subsection{Implementation Notes}
% You can personalize the details here
\subsubsection{Fertility Input Implementation}
By analyzing a corpus of Portuguese text from the \cite{calamept_reference}\unsure{fix reference to calamept} dataset:
\begin{lstlisting}
total_tokens = 0
total_words = 0
for text in corpus:
    tokens = tokenizer.encode(text)
    words = get_words(text)
    total_tokens += len(tokens)
    total_words += len(words)
fertility_input = total_tokens / total_words
\end{lstlisting}

\subsubsection{Fertility Output Implementation}
By randomly selecting 4 sentences from the \cite{calamept_reference}\unsure{fix reference to calamept} dataset:
\begin{lstlisting}
num_tokens: list[int] = []
for phrase in phrases:
    tokens = tokenizer.encode(phrase)
    
    # Starting off on 3rd token to give some context to the model
    for n=3 to len(tokens):
        generation: list[int] = []
        while not has_one_complete_word(tokens_per_word[-1])
            next_token = model.generate(tokens[:n])
            generation.append(next_token)
        num_tokens.append(get_num_tokens_first_full_word(generation))
fertility_output = average(num_tokens)
\end{lstlisting}

\section{RQ3: Model Architecture Impact on Adaptation}
\label{sec:model_impact}

A critical aspect of our research was understanding how different model architectures respond to tokenizer adaptation. While the primary goal was to improve token efficiency, we discovered significant differences in how models of varying sizes and architectures responded to our adaptation techniques.

\unsure{Waiting for other models evaluation (Small Multi-Lingual, Medium Mono-Lingual)}

Our experiments revealed substantial differences in how models responded to the addition of Portuguese-specific tokens. The smaller monolingual SmolLM2-135M model showed different adaptation patterns compared to the larger multilingual Qwen2.5-1.5B model. These differences were consistent across multiple performance metrics.

This variation in adaptation effectiveness across model architectures represents a fundamental consideration in tokenizer adaptation. The findings suggest that adaptation strategies may need to be tailored to specific model architectures, which has important implications for the practical application of tokenizer adaptation techniques.

\section{RQ4: Qualitative Behavior – Prompt-Based Manual Evaluation}
\label{sec:manual_eval}

In order to explore whether our modifications affected not only quantitative metrics but also qualitative performance, we conducted structured prompt-based interactions with the models both before and after the adaptations.

\subsection{Approach}
We prompted the model with open-ended or semi-structured questions in the target language and analyzed the output in terms of coherence, grammaticality, and relevance.

\subsection{Purpose}
This form of qualitative evaluation is essential to capture subtle effects that standard metrics like accuracy or F1 may miss, such as improved fluency, stylistic appropriateness, or reduction in hallucinations.

\subsection{Sample Prompts}
% Add your actual examples here
\begin{itemize}
    \item \texttt{"Explique o que é a Revolução dos Cravos."}
    \item \texttt{"Crie uma lista de compras em português com cinco itens do supermercado."}
    \item \texttt{"Crie uma frase que contenha as seguintes palavras: comida, futebol e carro."}
\end{itemize}



\section{RQ4: Embedding Initialization Strategies}
\label{sec:model_variation}

To determine whether the effectiveness of our adaptations depends on the underlying model architecture, we applied the tokenizer changes across multiple pre-trained models with varying properties.

\subsection{Models Compared}
We selected a range of models differing in:
\begin{itemize}
    \item \textbf{Size} (e.g., base vs. large)
    \item \textbf{Language scope} (e.g. monolingual vs. multilingual)
    \item \textbf{Pre-training corpus characteristics} (e.g. Models fine tuned for Coding, literature, etc)
\end{itemize}

\subsection{Purpose}
This analysis is intended to evaluate the generalizability of the methods and determine whether certain architectures are more sensitive to tokenizer modifications than others.

\subsection{Results}
\unsure{Deveria adicionar resultados aqui ou nao?}

\section{Model Sensitivity Analysis}
\label{sec:token_quality_test}

To assess whether the \textit{quality} of added tokens matters — linguistically or statistically — we designed a controlled test in which the model was asked to complete short sentences using a constrained set of target words.

\subsection{Method Overview}
The model was prompted with a small set of semantically or syntactically connected keywords and asked to generate a phrase or sentence that incorporated them.

\subsection{Purpose}
This allowed us to evaluate whether the newly added tokens were being properly understood, integrated, and positioned in natural linguistic structures, or whether they disrupted fluency or meaning.

\subsection{Implementation}
Example input format:
\begin{itemize}
    \item \texttt{Keywords: “sol”, “praia”, “verão” → Output: [short phrase]}
\end{itemize}

The same test set was applied before and after adaptation to allow side-by-side comparison.

\section{Summary and Design Implications}
\label{sec:exploration_summary}

These exploratory analyses were critical in shaping the methods described in subsequent chapters. In particular:
\begin{itemize}
    \item RQ1 and RQ2 informed our understanding of the performance impacts and efficiency gains of tokenizer adaptation
    \item RQ3 highlighted how different model architectures respond differently to tokenizer adaptation \unsure{Waiting for other models evaluation (Small Multi-Lingual, Medium Mono-Lingual)}
    \item RQ4 guided the development of effective embedding initialization strategies
\end{itemize}

In the following chapters, we build upon these insights to formalize and evaluate our adaptation strategies.

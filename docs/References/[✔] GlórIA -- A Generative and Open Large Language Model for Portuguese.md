# Introduction

This document summarizes the paper [GlórIA - A Generative and Open Large Language Model for Portuguese](https://arxiv.org/abs/2402.12969).

# GlórIA - A Generative and Open Large Language Model for Portuguese

## Summary

The paper presents GlórIA, a portuguese decoder LLM. \
This model was pre-trained from [[INSERT MODEL HERE]] with 35 billion tokens from different sources. \
The authors introduced CALAME-PT, a brand new evaluation method for PT-PT large language Models (very interesting). This is a zero-shot PT benchmark for language modeling evaluation.
The authors assembled a new highly-diverse PT-PT data corpus to pre-train GlórIA.


## Approach

The authors focused on:
1. Gathering of PT-PT resources (collection of datasets), with sources:
    - Web crawls (OSCAR-2201 [\[1\]](#reference-1), ClueWeb-L 22 [\2\]](#reference-2))
    - PTWiki [\[3\]](#reference-3) (well-written and reviewed encyclopedic in revised Portuguese text.)
    - Europarl [\[4\]](#reference-4) (Transcripts from sessions occurred in the European Parliament)
    - OpenSubtitles [\[5\]](#reference-5) (Short movie conversations and narrations.)
    - Arquivo.pt [\[6\]](#reference-6) (Collection of scrapped text from news websites archived by Arquivo.pt)
1. Data Processing: After the gathering of resources, some documents were filtered out, and some more processing was made.
    - Final data with 35.3 Million documents and 35.5 Billion Tokens
1. Base-line Model selection - GPT-Neo [\[7\]](#reference-7) (1.3B and 2.7B models)
1. Model Pre-training
1. Model evaluation (CALAME-PT):
    - Creation of CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese)
1. Also created a Encoder version of the model, by adding classification/regression heads to the pre-trained model, and fine-tuning them.

## Main findings


## Evaluation Methodology
1. CALAME-PT
    - Based on LAMBADA [\[8\]](#reference-8)
    - Guesses the final word given the context that comes before it.
    - 2076 texts and respective last words.
1. Of those 2076 texts:
    - 406 were Handwritten by 4 annotators (so created by them)
    - The other ones were automatically generated by GPT-3.5 using the documents from the training corpus as a prompt.
1. Compared dufferent models on CALAME-PT dataset.


## Results

Their model outperformed Gervasio-PTPT and mGPT, and was denoted as "state-of-the-art". \
They also compared their Decoder model with "Albertina-PTPT" which is an encoder model on Encoding tasks. Their model performed worst than Albertina-PTPT, but it outperformed Gervasio-PTPT, showing that a good decoder model can outperform worst decoder models on encoding tasks.

# Conclusion
Really good paper explaining some ideas on how to evaluate our LLM by the end. \
Would be interesting to maybe pick some of our university Exams and create a dataset out of them (maybe multiple choice or something like that). 

# References
<span id="reference-1">[1] - Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. [Towards a Cleaner Document-Oriented Multilingual Crawled Corpus](https://aclanthology.org/2022.lrec-1.463/)   </span><br>
<span id="reference-2">[2] - Arnold Overwijk, Chenyan Xiong, and Jamie Callan. 2022. [Clueweb22: 10 billion web documents with rich information.](https://dl.acm.org/doi/10.1145/3477495.3536321)</span><br>
<span id="reference-3">[3] - PTWiki - [https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)</span><br>
<span id="reference-4">[4] - Europarl - [https://www.statmt.org/europarl/](https://www.statmt.org/europarl/)</span><br>
<span id="reference-5">[5] - Pierre Lison and Jörg Tiedemann. 2016. [OpenSub-titles2016: Extracting large parallel corpora from movie and TV subtitles.](https://aclanthology.org/L16-1147/)</span><br>
<span id="reference-6">[6] - Daniel Gomes, André Nogueira, João Miranda, and Miguel Costa. 2008. Introducing the portuguese web archive initiative.</span><br>
<span id="reference-7">[7] - [GPT Neo 1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B) / [GPT Neo 2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)</span><br>
<span id="reference-8">[8] - [The LAMBADA dataset: Word prediction requiring a broad discourse context](https://aclanthology.org/P16-1144/)</span><br>
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Exploring adding new token with new embeddings with a different/bigger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import pathlib\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "os.chdir('/home/yali/MEGA/Hack The Tockenizer/tests')\n",
    "sys.path.insert(1, pathlib.Path('..').resolve().as_posix())\n",
    "from src import utils, loader, hack\n",
    "from src.DatasetClass import ListDataset, TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MODEL = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "phrases: list[str] = hack.BENCHMARKS.benchmarks[1].prediction_prompts.to_list() # CalamePT dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loader.load_model_and_tokenizer(\n",
    "    model_name=MODEL,\n",
    "    device=DEVICE,\n",
    "    model_kwargs = { 'torch_dtype': torch.bfloat16},\n",
    "    tokenizer_kwargs={'padding_side': 'left'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find words generated\n",
    "\n",
    "Using the datasets we have, find which tokens the model originally generates.\n",
    "\n",
    "We will later create a `new_token` with the first word that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = hack.BENCHMARKS.benchmarks[1].prediction_prompts[0].strip(' ')     # Strip \"spaces\"\n",
    "print('Input phrase: `{}`'.format(phrase))\n",
    "\n",
    "generation = utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase=phrase,\n",
    "    device=DEVICE,\n",
    "    return_dict_in_generate=False,\n",
    "    output_logits=False,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "generated_phrase = tokenizer.decode(generation[0])\n",
    "print('Generated Phrase: `{}`'.format(generated_phrase))\n",
    "\n",
    "# Finding the first word generated\n",
    "new_token = re.findall('[a-z|A-Z| ]+', generated_phrase.replace(phrase, ''))[0]\n",
    "old_tokenization = tokenizer.encode(new_token)\n",
    "print('New token generated: `{}`(={})'.format(new_token, old_tokenization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new token to the model\n",
    "\n",
    "Using the word found in the previous step, add it as a new_token to the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(new_token)\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "print('New Token: `{}`(ID={})'.format(new_token, new_token_id))\n",
    "\n",
    "# Resize model embedding to include the new token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Initialize the embedding with the average of the embeddings of the previous tokenization\n",
    "embed = model.get_input_embeddings()\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_embed = torch.stack([embed.weight[t] for t in old_tokenization]).mean(dim=0).to(DEVICE)\n",
    "    _ = embed.weight[new_token_id].data.copy_(new_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the new generation with the new added token\n",
    "\n",
    "Validate the logits of the new added token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generation = utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase=phrase,\n",
    "    device=DEVICE,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate new embeddings\n",
    "\n",
    "Use the \"gradient\" approach to calculate new embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Baseline\n",
    "\n",
    "Generating for the first 10 phrases to validate the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] chegada, mas acabou derrotada.\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] público.\n",
      "Aqui está a tradução para ingl\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores] caíssem, e isso resultou em um\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] 120 pães e 6\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu] lugar na equipe. Por exemplo, um jogador deve\n",
      "<Phrase 5>[Os pássaros voaram alto no] final de semana, mas o tempo começou a ch\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] dia, e os olhos do jovem p\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das] árvores. O som dos rai\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] felizes. Apenas um pequeno problema\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] presente que havia recebido do pai.\n"
     ]
    }
   ],
   "source": [
    "original_generation = []\n",
    "for i in range(10):\n",
    "    original_generation.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(original_generation[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tokenizer\n",
    "\n",
    "Obtain the new tokens given the phrases we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Train a new portuguese vocabulary\n",
    "pt_tokenizer = hack.TokenizerHack(device=DEVICE).train_tokenizer(trainer_kwargs={'vocab_size': 10000})\n",
    "\n",
    "# Step 2. Find tokens in `pt_tokenizer` not in \n",
    "new_tokens = set(pt_tokenizer.get_vocab().keys())\n",
    "new_tokens = new_tokens.difference(set(tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Vocab + Embeddings\n",
    "\n",
    "Add new tokens to tokenizer and update embedding table to inlcude them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Initializing the embeddings for the new_tokens: 100%|██████████| 5040/5040 [00:00<00:00, 7892.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the original tokenizations\n",
    "original_tokenization = {t: tokenizer.encode(t) for t in new_tokens}\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Step 4. Calculate the new embeddings for the new tokens\n",
    "embed = model.get_input_embeddings().weight.clone().to('cpu')\n",
    "new_embed = model.get_input_embeddings()\n",
    "\n",
    "# Initialize the embedding using the weighted average model\n",
    "K = 5\n",
    "with torch.no_grad():\n",
    "    for new_token in tqdm.tqdm(new_tokens, desc='Initializing the embeddings for the new_tokens'):\n",
    "        new_token_id = tokenizer.encode(new_token)[0]\n",
    "        # Find the old embedding for the token\n",
    "        tokenization = original_tokenization[new_token]\n",
    "        token_embed = torch.stack([embed[t_id] for t_id in tokenization]).to(DEVICE)\n",
    "        # Calculating the embedding weights\n",
    "        embedding_weights = torch.asarray([K**i if K**i < 2**64 else 0 for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        # embedding_weights = torch.asarray([K**i for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        embedding_weights = embedding_weights / embedding_weights.sum()\n",
    "\n",
    "        # Create a new token embed using the weighted average of the embeddings\n",
    "        new_token_embed = torch.sum(token_embed * embedding_weights[:, None], dim=0)\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation After Adding Tokens\n",
    "\n",
    "Generating for the first 10 phrases after adding the tokens without any \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] chegada, ganhando o prêmio\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] público - O Jornal Económico\n",
      "\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores] caíssem e a estrada ficasse inter\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] o seu trabalho e ganhava um salário\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu]u trabalho. A nossa equipa é uma das\n",
      "<Phrase 5>[Os pássaros voaram alto no] céu, enquanto os animais de estima\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] dia. O vento soprava com for em\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das] árvores. O vento levou\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] os pratos, incluindo o pão\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] presente da avó. O que ela fez\n"
     ]
    }
   ],
   "source": [
    "generation_with_new_tokens = []\n",
    "for i in range(10):\n",
    "    generation_with_new_tokens.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(generation_with_new_tokens[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the embedding\n",
    "\n",
    "Use the gradients and so on to obtain the new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating the embeddings for the new tokens: 100%|██████████| 5040/5040 [2:10:48<00:00,  1.56s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Step 4.2 Using the training phrases to update the embedding weights\n",
    "learning_rate = 1e-6\n",
    "BATCH_SIZE = 8\n",
    "for new_token in tqdm.tqdm(new_tokens, desc='Updating the embeddings for the new tokens'):\n",
    "    new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "    new_token = tokenizer.decode(new_token_id)\n",
    "    phrases_to_generate_new_token = [p for phrase in phrases for p in phrase.split(new_token)[:-1] if new_token in phrase and len(p) > 0]\n",
    "\n",
    "    if len(phrases_to_generate_new_token) == 0: continue\n",
    "    # Creating the Batched dataset (to run generation for multiple phrases at the same time)\n",
    "    dataloader = DataLoader(\n",
    "        TextDataset(phrases_to_generate_new_token, tokenizer, max_length=max(len(tokenizer.tokenize(x)) for x in phrases_to_generate_new_token)),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    # Process the batches\n",
    "    for batch in tqdm.tqdm(dataloader,  desc=f'  Generating tokens for new_token=`{new_token}` ', leave=False):\n",
    "        # Move batch tensors to the correct device\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(DEVICE)\n",
    "\n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_logits=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Extract the generated sequences and their scores\n",
    "        generated_sequences = outputs.sequences\n",
    "        predicted_logits = outputs.logits\n",
    "\n",
    "        # Decode the input and generated sequences\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(input_texts)):\n",
    "                # generation.append({\n",
    "                #     'generated_sequences': generated_sequences[i].to('cpu'),\n",
    "                #     'prediction_scores': predicted_logits[0][i].to('cpu'),\n",
    "                #     'input_texts': input_texts[i],\n",
    "                #     'generated_texts': generated_texts[i],\n",
    "                #     'hidden_states': [hidden_state[i].to('cpu') for hidden_state in outputs.hidden_states[0]]\n",
    "                # })\n",
    "\n",
    "                logits = predicted_logits[0][i]\n",
    "                logit_gradient = logits.max() - logits[new_token_id]\n",
    "                embed_out = outputs.hidden_states[0][-1][i][-1]\n",
    "                # normalize embed_out\n",
    "                embed_out = embed_out / embed_out.norm()\n",
    "\n",
    "                embed_in = new_embed.weight[new_token_id]\n",
    "\n",
    "                # Update the embedding table\n",
    "                _ = new_embed.weight[new_token_id].data.copy_((embed_in + logit_gradient * embed_out * learning_rate).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] gravideztaneamenteórida 1992 semáfor registada provenientes doze estimada criados\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] estimada leite 1992 Foram bolas provenientes salarial 2001 Broo 193\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores] esquerdoamá 1992 vivem estimada provenientes registada doze semáfor cobre\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] 1992 semáfor semelhança ciclos provenientes estimada registada castan Foram salarial\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu]órida 1992 semelhança provenientes registada doze estimada Broo cobre salarial\n",
      "<Phrase 5>[Os pássaros voaram alto no] chumbo 1992 Foram semáfor estimada provenientes doze registada bolas salarial\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] 1992óridataneamente estimada Foram semáfor provenientes registada doze 2001\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das] gravideztation vivem 1992 semáfor provenientes estimada registada bolas castan\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] ursos semáfor 1992 estimada salarial provenientes Foram registada bolas Broo\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] 1992 estimada provenientes Foram semáfor registada bolas castan 193 doze\n"
     ]
    }
   ],
   "source": [
    "generation_with_new_tokens = []\n",
    "for i in range(10):\n",
    "    generation_with_new_tokens.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(generation_with_new_tokens[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  7.6250,   6.0625,   1.0078,  ..., 208.0000, 194.0000, 204.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.5000,   7.7500,   6.2812,  ..., 191.0000, 171.0000, 181.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.8125,   9.6875,   7.0312,  ..., 215.0000, 195.0000, 207.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 11.3750,  11.6250,   8.5625,  ..., 256.0000, 236.0000, 249.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 10.1875,  10.5000,   8.8125,  ..., 270.0000, 248.0000, 262.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.6250,  10.5625,   8.1875,  ..., 286.0000, 262.0000, 278.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.6250,  10.4375,   8.3125,  ..., 286.0000, 262.0000, 278.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.3125,  10.1250,   8.9375,  ..., 288.0000, 264.0000, 278.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.3125,   9.8125,   8.9375,  ..., 292.0000, 268.0000, 284.0000]],\n",
       "        device='cuda:0'),\n",
       " tensor([[  9.3125,   9.6875,   8.6875,  ..., 294.0000, 268.0000, 284.0000]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_generation = utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase=phrases[i],\n",
    "    device=DEVICE,\n",
    "    max_new_tokens=10,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "new_generation.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I MUST BE MISSING NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.255859375"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = new_embed.weight[:len(tokenizer) - len(new_tokens)]\n",
    "min_val = tmp.min().item()\n",
    "delta_val = tmp.max().item() - min_val\n",
    "\n",
    "# Normalize the added tokens\n",
    "with torch.no_grad():\n",
    "    for new_token_id in range(len(tokenizer) - len(new_tokens), len(tokenizer)):\n",
    "        embed_in = new_embed.weight[new_token_id]\n",
    "        embed_normalized = (embed_in - embed_in.min()) / (embed_in.max() - embed_in.min())\n",
    "        # Update the embedding table\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(embed_normalized * delta_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ela correu durante horas para alcançar a linha de gravideztaneamenteórida 1992 semáfor registada provenientes doze estimada criados'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_generation.sequences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

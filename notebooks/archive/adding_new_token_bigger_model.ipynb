{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Exploring adding new token with new embeddings with a different/bigger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import pathlib\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "os.chdir('/home/yali/MEGA/Hack The Tockenizer/tests')\n",
    "sys.path.insert(1, pathlib.Path('..').resolve().as_posix())\n",
    "from src import utils, loader, hack\n",
    "from src.DatasetClass import ListDataset, TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MODEL = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "phrases: list[str] = hack.BENCHMARKS.benchmarks[1].prediction_prompts.to_list() # CalamePT dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loader.load_model_and_tokenizer(\n",
    "    model_name=MODEL,\n",
    "    device=DEVICE,\n",
    "    model_kwargs = { 'torch_dtype': torch.bfloat16},\n",
    "    tokenizer_kwargs={'padding_side': 'left'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find words generated\n",
    "\n",
    "Using the datasets we have, find which tokens the model originally generates.\n",
    "\n",
    "We will later create a `new_token` with the first word that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input phrase: `Ela correu durante horas para alcançar a linha de`\n",
      "Generated Phrase: `Ela correu durante horas para alcançar a linha de chegada, mas acabou perdendo por apenas`\n",
      "New token generated: ` chegada`(=[96940, 2584])\n"
     ]
    }
   ],
   "source": [
    "phrase = hack.BENCHMARKS.benchmarks[1].prediction_prompts[0].strip(' ')     # Strip \"spaces\"\n",
    "print('Input phrase: `{}`'.format(phrase))\n",
    "\n",
    "generation = utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase=phrase,\n",
    "    device=DEVICE,\n",
    "    return_dict_in_generate=False,\n",
    "    output_logits=False,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "generated_phrase = tokenizer.decode(generation[0])\n",
    "print('Generated Phrase: `{}`'.format(generated_phrase))\n",
    "\n",
    "# Finding the first word generated\n",
    "new_token = re.findall('[a-z|A-Z| ]+', generated_phrase.replace(phrase, ''))[0]\n",
    "old_tokenization = tokenizer.encode(new_token)\n",
    "print('New token generated: `{}`(={})'.format(new_token, old_tokenization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the new token to the model\n",
    "\n",
    "Using the word found in the previous step, add it as a new_token to the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Token: ` chegada`(ID=151665)\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens(new_token)\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "print('New Token: `{}`(ID={})'.format(new_token, new_token_id))\n",
    "\n",
    "# Resize model embedding to include the new token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Initialize the embedding with the average of the embeddings of the previous tokenization\n",
    "embed = model.get_input_embeddings()\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_embed = torch.stack([embed.weight[t] for t in old_tokenization]).mean(dim=0).to(DEVICE)\n",
    "    _ = embed.weight[new_token_id].data.copy_(new_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the new generation with the new added token\n",
    "\n",
    "Validate the logits of the new added token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generation = utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase=phrase,\n",
    "    device=DEVICE,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate new embeddings\n",
    "\n",
    "Use the \"gradient\" approach to calculate new embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Baseline\n",
    "\n",
    "Generating for the first 10 phrases to validate the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] chegada e ganhou o prémio.\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] público.\n",
      "Aqui está uma tradução alternativa\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores] caíssem e danificassem o cas\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] 120 biscoitos, e gan\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu] papel na equipe. Quem é o jogador mais\n",
      "<Phrase 5>[Os pássaros voaram alto no] dia 10 de dezembro, quando o\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] dia, mas quando a noite chegou,\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das] árvores, refletindo o br\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] felizes. Como a comida era doce,\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] presente. Eram dois pães com re\n"
     ]
    }
   ],
   "source": [
    "original_generation = []\n",
    "for i in range(10):\n",
    "    original_generation.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(original_generation[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tokenizer\n",
    "\n",
    "Obtain the new tokens given the phrases we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Train a new portuguese vocabulary\n",
    "pt_tokenizer = hack.TokenizerHack(device=DEVICE).train_tokenizer(trainer_kwargs={'vocab_size': 10000})\n",
    "\n",
    "# Step 2. Find tokens in `pt_tokenizer` not in \n",
    "new_tokens = set(pt_tokenizer.get_vocab().keys())\n",
    "new_tokens = new_tokens.difference(set(tokenizer.vocab.keys()))\n",
    "# Removing the 'Ġ' tokens and fixing maybe some others\n",
    "new_tokens = set([tokenizer.decoder.decode([new_token]) for new_token in new_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Vocab + Embeddings\n",
    "\n",
    "Add new tokens to tokenizer and update embedding table to inlcude them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Initializing the embeddings for the new_tokens: 100%|██████████| 5040/5040 [00:00<00:00, 8284.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the original tokenizations\n",
    "original_tokenization = {t: tokenizer.encode(t) for t in new_tokens}\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Step 4. Calculate the new embeddings for the new tokens\n",
    "embed = model.get_input_embeddings().weight.clone().to('cpu')\n",
    "new_embed = model.get_input_embeddings()\n",
    "\n",
    "# Initialize the embedding using the weighted average model\n",
    "K = 5\n",
    "with torch.no_grad():\n",
    "    for new_token in tqdm.tqdm(new_tokens, desc='Initializing the embeddings for the new_tokens'):\n",
    "        new_token_id = tokenizer.encode(new_token)[0]\n",
    "        # Find the old embedding for the token\n",
    "        tokenization = original_tokenization[new_token]\n",
    "        token_embed = torch.stack([embed[t_id] for t_id in tokenization]).to(DEVICE)\n",
    "        # Calculating the embedding weights\n",
    "        embedding_weights = torch.asarray([K**i if K**i < 2**64 else 0 for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        # embedding_weights = torch.asarray([K**i for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        embedding_weights = embedding_weights / embedding_weights.sum()\n",
    "\n",
    "        # Create a new token embed using the weighted average of the embeddings\n",
    "        new_token_embed = torch.sum(token_embed * embedding_weights[:, None], dim=0)\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation After Adding Tokens\n",
    "\n",
    "Generating for the first 10 phrases after adding the tokens without any \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] meta. 2850. A mena es\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] público e foi um dos grandes destaques da no\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores]siasem aço cãorença de queb o p\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] 41500000000 kg de peixes\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu]u tempo para jogar. A equipa tem\n",
      "<Phrase 5>[Os pássaros voaram alto no] céu, enquanto o pãoeregrino estava\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] mto dois anos. E oeste mandate\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das]. \n",
      "Sobre aspeto folas da semente\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] impressionados com a qualidade. O que isso significa\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] aniversário do seu irmão mais velho\n"
     ]
    }
   ],
   "source": [
    "generation_with_new_tokens = []\n",
    "for i in range(10):\n",
    "    generation_with_new_tokens.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(generation_with_new_tokens[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the embedding\n",
    "\n",
    "Use the gradients and so on to obtain the new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating the embeddings for the new tokens: 100%|██████████| 5040/5040 [44:27<00:00,  1.89it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Step 4.2 Using the training phrases to update the embedding weights\n",
    "learning_rate = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "for new_token in tqdm.tqdm(new_tokens, desc='Updating the embeddings for the new tokens'):\n",
    "    new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "    new_token = tokenizer.decode(new_token_id)\n",
    "    phrases_to_generate_new_token = [p for phrase in phrases for p in phrase.split(new_token)[:-1] if new_token in phrase and len(p) > 0]\n",
    "\n",
    "    if len(phrases_to_generate_new_token) == 0: continue\n",
    "    # Creating the Batched dataset (to run generation for multiple phrases at the same time)\n",
    "    dataloader = DataLoader(\n",
    "        TextDataset(phrases_to_generate_new_token, tokenizer, max_length=max(len(tokenizer.tokenize(x)) for x in phrases_to_generate_new_token)),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    # Process the batches\n",
    "    for batch in tqdm.tqdm(dataloader,  desc=f'  Generating tokens for new_token=`{new_token}` ', leave=False):\n",
    "        # Move batch tensors to the correct device\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(DEVICE)\n",
    "\n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_logits=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Extract the generated sequences and their scores\n",
    "        generated_sequences = outputs.sequences\n",
    "        predicted_logits = outputs.logits\n",
    "\n",
    "        # Decode the input and generated sequences\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(input_texts)):\n",
    "                # generation.append({\n",
    "                #     'generated_sequences': generated_sequences[i].to('cpu'),\n",
    "                #     'prediction_scores': predicted_logits[0][i].to('cpu'),\n",
    "                #     'input_texts': input_texts[i],\n",
    "                #     'generated_texts': generated_texts[i],\n",
    "                #     'hidden_states': [hidden_state[i].to('cpu') for hidden_state in outputs.hidden_states[0]]\n",
    "                # })\n",
    "\n",
    "                logits = predicted_logits[0][i]\n",
    "                logit_gradient = logits.max() - logits[new_token_id]\n",
    "                embed_out = outputs.hidden_states[0][-1][i][-1]\n",
    "                # normalize embed_out\n",
    "                embed_out = embed_out / embed_out.norm()\n",
    "\n",
    "                embed_in = new_embed.weight[new_token_id]\n",
    "\n",
    "                # Update the embedding table\n",
    "                _ = new_embed.weight[new_token_id].data.copy_((embed_in + logit_gradient * embed_out * learning_rate).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Phrase 0>[ Ela correu durante horas para alcançar a linha de] meta.\n",
      " 12 324. La carrera dnt\n",
      "<Phrase 1>[Ela cantou tão bem no concerto que emocionou o] público - 102015500/0015/;22\n",
      "<Phrase 2>[Os ventos fortes causaram com que algumas árvores]sas vezes tenham queim usar mais de 19915\n",
      "<Phrase 3>[O Jorge trabalhava numa padaria. Todos os dias ele vendia] 1512000000000 gramas de pão\n",
      "<Phrase 4>[As equipas de futebol têm vários jogadores, e todos têm que respeitar o seu]u espaço no fundo do campo. Acho\n",
      "<Phrase 5>[Os pássaros voaram alto no] céu, e oitava párado foi o\n",
      "<Phrase 6>[O sol brilhou intensamente durante o] soluções de lhesuna, oeste que faz comem quei\n",
      "<Phrase 7>[A chuva caiu suavemente sobre as folhas das] da árvore, que está em flores\n",
      "<Phrase 8>[A comida estava deliciosa e deixou todos] felizes. Apenas um detalhe:\n",
      "<Phrase 9>[Era noite de Natal, e a criança sorriu ao receber o] pão com aleg mãe. A crã 1\n"
     ]
    }
   ],
   "source": [
    "generation_with_new_tokens = []\n",
    "for i in range(10):\n",
    "    generation_with_new_tokens.append(utils.generate(\n",
    "        model, tokenizer,\n",
    "        phrase=phrases[i],\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "    ))\n",
    "    print(\n",
    "        '<Phrase {}>[{}]{}'.format(i, phrases[i], tokenizer.decode(generation_with_new_tokens[i].sequences[0, -10:]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the rankings for all new tokens in phrases they SHOULD appear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating new logits after \"training\": 100%|██████████| 5040/5040 [56:06<00:00,  1.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for new_token in tqdm.tqdm(list(new_tokens), 'Calculating new logits after \"training\"'):\n",
    "    phrases_to_generate_new_token = [p for phrase in phrases for p in phrase.split(new_token)[:-1] if new_token in phrase and len(p) > 0]\n",
    "\n",
    "    if len(phrases_to_generate_new_token) == 0: continue\n",
    "    # Creating the Batched dataset (to run generation for multiple phrases at the same time)\n",
    "    dataloader = DataLoader(\n",
    "        TextDataset(phrases_to_generate_new_token, tokenizer, max_length=max(len(tokenizer.tokenize(x)) for x in phrases_to_generate_new_token)),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    # Process the batches\n",
    "    for batch in tqdm.tqdm(dataloader,  desc=f'  Generating tokens for new_token=`{new_token}` ', leave=False):\n",
    "        # Move batch tensors to the correct device\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(DEVICE)\n",
    "\n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_logits=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Extract the generated sequences and their scores\n",
    "        generated_sequences = outputs.sequences.to('cpu')\n",
    "        predicted_logits = outputs.logits[0].to('cpu')\n",
    "\n",
    "        # Decode the input and generated sequences\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "        \n",
    "        # Add generation to \"generations\"\n",
    "        results.extend(\n",
    "            [\n",
    "                {\n",
    "                    'new_token': new_token,\n",
    "                    'new_token_id': new_token_id, \n",
    "                    'rank': (logits>logits[new_token_id]).sum().item(),\n",
    "                    'logit': logits[new_token_id].item(),\n",
    "                    'maximum_logit': logits.max().item(),\n",
    "                    'generated_sequence': sequence\n",
    "                } \n",
    "                for logits, sequence in zip(predicted_logits, generated_texts)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame(results)\n",
    "results.to_csv('./gwen1.5_trained_model_logits.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

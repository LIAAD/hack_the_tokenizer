{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore if we can use the generation of the model to retrieve information regarding new tokens\n",
    "\n",
    "# TODO: Evaluate how different methodologies compare:\n",
    "#   1. Create new tokens and use the training-dataset to train the model on it\n",
    "#   2. Validate how the logits for the new_tokens varies according to:\n",
    "#       2.1 Using a random embedding for each new_token\n",
    "#       2.2 Using average of the previous tokenization - E(new_token) = [E(t1) + E(t2) + ... + E(tN) / N] where ORIGINAL_Tokenization(new_token) = [t1, t2, ..., tN]\n",
    "#       2.3 Using a weighted average of the previous tokenization - E(new_token) = [w1*E(1) + ... + wN*E(N)].\n",
    "#           2.3.1 Try first with w_i = w_{i+1} * K for some different K's\n",
    "#           2.3.2 Try to use a simple regressor model to find different w_i values using the existing merges\n",
    "#   3. Compare the logits \"RANK\" of all different steps as well\n",
    "#   4. Use the average of the phrases of a given prediction to obtain a new embedding\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import pathlib\n",
    "sys.path.insert(1, str(pathlib.Path('..').resolve()))\n",
    "from src import utils, loader, hack\n",
    "from src.DatasetClass import ListDataset\n",
    "\n",
    "import tqdm\n",
    "import transformers\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "NUM_TOKENS_TO_MERGE = 3\n",
    "# Load the model and tokenizer\n",
    "model_name = 'HuggingFaceTB/SmolLM-135M'\n",
    "model, tokenizer = loader.load_model_and_tokenizer(model_name=model_name, device=DEVICE, tokenizer_kwargs={'padding_side': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "Finding which tokens the model originally generates so we can calculate our metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Function\n",
    "\n",
    "Function which generates for a given \"str\" as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.models as models\n",
    "def generate(\n",
    "    model: models.llama.modeling_llama.LlamaForCausalLM,\n",
    "    tokenizer: models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast,\n",
    "    phrase: str,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    max_new_tokens=1,\n",
    "    **model_kwargs\n",
    "):\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    for key in inputs.keys(): inputs[key] = inputs[key].to(DEVICE)\n",
    "    return model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        output_scores=output_scores,\n",
    "        return_dict_in_generate=return_dict_in_generate,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        **model_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating tokens\n",
    "\n",
    "First, find out which tokens are generated by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 20\n",
    "phrases: list[str] = hack.BENCHMARKS.benchmarks[0].prediction_prompts.to_list() # SuperGluePTPT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, phrase in tqdm.tqdm(enumerate(phrases), total=len(phrases), desc='Original generation for inputs'):\n",
    "#     generation = generate(\n",
    "#         model,\n",
    "#         tokenizer,\n",
    "#         phrase,\n",
    "#         output_scores=False,\n",
    "#         return_dict_in_generate=False,\n",
    "#         max_new_tokens=MAX_NEW_TOKENS,\n",
    "#     )\n",
    "#     phrases[n] = {\n",
    "#         'original_text': phrase,\n",
    "#         'generated_tokens': [g.item() for g in generation[0][-MAX_NEW_TOKENS:]]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above but using Pipeline to paralellize the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Original generation for inputs:  79%|███████▉  | 12561/15942 [06:59<01:52, 30.09it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Original generation for inputs: 100%|██████████| 15942/15942 [09:07<00:00, 29.12it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "generator = transformers.pipeline(\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=DEVICE,\n",
    "    model_kwargs={'pad_token_id': tokenizer.eos_token_id},\n",
    ")\n",
    "generation = []\n",
    "for gen in tqdm.tqdm(generator(ListDataset(phrases),\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        batch_size=BATCH_SIZE\n",
    "    ),\n",
    "    desc=\"Original generation for inputs\",\n",
    "    total=len(phrases)\n",
    "):\n",
    "    generation.append(\n",
    "        {\n",
    "            'original_text': phrases[len(generation)],\n",
    "            'generated_text': gen[0]['generated_text'],\n",
    "            'generated_tokens': tokenizer.encode(gen[0]['generated_text'])[-MAX_NEW_TOKENS:]\n",
    "        }\n",
    "    )\n",
    "phrases = generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOKENS_TO_MERGE = 2\n",
    "TOP_N_TOKENS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining new tokens to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "generation = pd.DataFrame(phrases)\n",
    "generation['generation'] = generation['generated_tokens'].apply(lambda x: tokenizer.decode(x[:NUM_TOKENS_TO_MERGE]))\n",
    "top_tokens = generation.groupby(by=['generation'], as_index=False).count().rename(columns={'original_text': 'NUM_PHRASES'})\n",
    "top_tokens = top_tokens.sort_values(by=['NUM_PHRASES'], ascending=False)\n",
    "\n",
    "# Pick the top N tokens (disregarding the first because it is the word '.\\n')\n",
    "new_tokens = top_tokens.iloc[1:TOP_N_TOKENS+1]['generation'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different embedding aggregation methods\n",
    "\n",
    "By adding new tokens to the tokenizer and to the model's embeddings, finding the scores of all possible embedding calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_scores(model, tokenizer, phrase: str, token_id: int):\n",
    "    '''\n",
    "    Returns the score of a specific token_id when generating a new token with `phrase` as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Any\n",
    "        model to generate the phrase with\n",
    "    \n",
    "    tokenizer: Any\n",
    "        tokenizer to encode the given phrase\n",
    "    \n",
    "    phrase: str\n",
    "        phrase to give as input to the model\n",
    "    \n",
    "    token_id: int\n",
    "        token to retrieve the scores to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[Literal['score', 'rank', 'best_score'], float]\n",
    "    '''\n",
    "    generation = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        phrase,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "    scores = generation['scores'][0][0]\n",
    "    token_score = scores[token_id]\n",
    "    token_rank = (scores > token_score).sum()\n",
    "    return {'score': token_score.item(), 'rank': token_rank.item(), 'best_score': scores.max().item()}\n",
    "\n",
    "def obtain_results(model, tokenizer, method, new_tokens):\n",
    "    '''Function to gather results for a given token_list'''\n",
    "    output = []\n",
    "    phrases_tokens = generation.groupby(by=['generation'])\n",
    "    for new_token in tqdm.tqdm(new_tokens, desc=f'Calculating scores for <{method}>'):\n",
    "        token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "        output.append({\n",
    "            'token': new_token,\n",
    "            'token_id': token_id,\n",
    "            'method': method,\n",
    "            'scores': {\n",
    "                phrase: calculate_token_scores(model, tokenizer, phrase, token_id) \n",
    "                for phrase in phrases_tokens.get_group((new_token,))['original_text']\n",
    "            }\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Loading models and embeddings and resize them according to new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, t = loader.load_model_and_tokenizer(model_name=model_name, device=DEVICE)\n",
    "\n",
    "# Add all generated tokens to tokenizer and model.\n",
    "t.add_tokens(new_tokens)\n",
    "\n",
    "m.resize_token_embeddings(len(t))\n",
    "\n",
    "# For each methodology calculate the scores of the new_token\n",
    "embed = m.get_input_embeddings().weight.clone()\n",
    "new_embed = m.get_input_embeddings()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method\n",
    "\n",
    "Creating a Baseline with \"Random\" embedding to compare against the rest of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'Random Embedding - BASELINE'\n",
    "min_embed, max_embed = embed.min(), embed.max()\n",
    "with torch.no_grad():\n",
    "    for new_token in new_tokens:\n",
    "        new_token_id = t.convert_tokens_to_ids(new_token)\n",
    "        # Create a new RANDOM embedding within the ranges of our existing embeddings\n",
    "        new_token_embed = torch.rand(embed.shape[1]).to(DEVICE)\n",
    "        # Updating values to be within ranges of existing embed\n",
    "        new_token_embed *= max_embed - min_embed\n",
    "        new_token_embed += min_embed\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n",
    "results.extend(obtain_results(m, t, METHOD, new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Method\n",
    "Using the Average of the embeddings.\n",
    "Let $new\\_token = (t_1, t_2, \\dots, t_N)$\n",
    "$$\n",
    "    E(new\\_token) = \\frac{1}{N} \\times \\sum_{i=1}^{N}{E(t_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHOD = 'Average of Embeddings'\n",
    "with torch.no_grad():\n",
    "    for new_token in new_tokens:\n",
    "        new_token_id = t.encode(new_token)[0]\n",
    "        # Find the old embedding for the token\n",
    "        token_embed = torch.stack([embed[t_id] for t_id in tokenizer.encode(new_token)]).to(DEVICE)\n",
    "        # Create a new token embed using the average\n",
    "        new_token_embed = token_embed.mean(dim=0)\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n",
    "results.extend(obtain_results(m, t, METHOD, new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Method\n",
    "\n",
    "Weighted average where $E = Embedding$ and\n",
    "$$\n",
    "new\\_token = (t_1, t_2, ..., t_N) \\\\ \n",
    "E(new\\_token) = (w_1 \\times E(t_1), w_2 \\times E(t_2), \\dots, w_N \\times E(t_N)) \\\\\n",
    "w_i = w_{i+1} \\times K\n",
    "$$\n",
    "for some $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "METHOD = f'Weighted average with w_i = w_{{i+1}} * {K}'\n",
    "with torch.no_grad():\n",
    "    for new_token in new_tokens:\n",
    "        new_token_id = t.encode(new_token)[0]\n",
    "        # Find the old embedding for the token\n",
    "        token_embed = torch.stack([embed[t_id] for t_id in tokenizer.encode(new_token)]).to(DEVICE)\n",
    "        # Calculating the embedding weights\n",
    "        embedding_weights = torch.asarray([K**i for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        embedding_weights = embedding_weights / embedding_weights.sum()\n",
    "\n",
    "        # Create a new token embed using the weighted average of the embeddings\n",
    "        new_token_embed = torch.sum(token_embed * embedding_weights[:, None], dim=0)\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n",
    "results.extend(obtain_results(m, t, METHOD, new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering results\n",
    "\n",
    "First gathering the results as a pandas dataframe and saving it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for record in results:\n",
    "    for phrase in record['scores'].keys():\n",
    "        df.append({\n",
    "            'token': record['token'],\n",
    "            'token_id': record['token_id'],\n",
    "            'method': record['method'],\n",
    "            'phrase': phrase,\n",
    "            'new_token_rank': record['scores'][phrase]['rank'],\n",
    "            'new_token_score': record['scores'][phrase]['score'],\n",
    "            'generation_best_score': record['scores'][phrase]['best_score'],\n",
    "        })\n",
    "df = pd.DataFrame(df)\n",
    "tmp = df.groupby(by=['method'], as_index=False)[['generation_best_score', 'new_token_rank', 'new_token_score']].describe()\n",
    "tmp = tmp.melt(id_vars=[('method', '')])\n",
    "tmp.columns = ['method', 'metric', 'aggregation', 'value']\n",
    "tmp = tmp.sort_values(by=['metric', 'aggregation', 'method'], ascending=True)\n",
    "tmp.to_csv(f'./embedding_calculations_{NUM_TOKENS_TO_MERGE}_merged_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = df[df['method'] != 'Random Embedding - BASELINE'].groupby(by=['token', 'method'], as_index=False)[['new_token_rank']].mean()\n",
    "\n",
    "ax = graph_df.pivot(index=['token'], columns=['method'], values=['new_token_rank']).plot()\n",
    "ax.set_title = f'Top {TOP_N_TOKENS} new tokens of size {NUM_TOKENS_TO_MERGE}'\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(f'top{TOP_N_TOKENS}_tokens_ranks_size{NUM_TOKENS_TO_MERGE}.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the embedding from predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Tests\n",
    "\n",
    "Testing out how to use `hidden_states` to create new embedding.\n",
    "\n",
    "Using hidden state generated of the last sequence as the embedding for the new_token.\n",
    "\n",
    "The steps of what is done is as follows:\n",
    "1. **Generate** for a given phrase **1 token**\n",
    "1. Check the **hidden_states** of the generation\n",
    "1. Look into the **last layer** (or state) of the **hidden states**\n",
    "1. Look at the **Last sequence** of the last layer\n",
    "1. Normalize the vector using the **sigmoid** function.\n",
    "1. Use that as the **embedding** for the **new_token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = generation[generation['generation'].isin(new_tokens)].sort_values(by=['generation']).iloc[0]\n",
    "text = row['original_text']\n",
    "tokenization = row['generated_tokens'][:NUM_TOKENS_TO_MERGE]\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "out = model.generate(\n",
    "    inputs['input_ids'].to(DEVICE),\n",
    "    attention_mask = inputs['attention_mask'].to(DEVICE),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 1,\n",
    "    output_hidden_states=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "hidden_states = out.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: tensor(2040., device='cuda:0'),\n",
       " 1: tensor(2040., device='cuda:0'),\n",
       " 2: tensor(2040., device='cuda:0'),\n",
       " 3: tensor(2040., device='cuda:0'),\n",
       " 4: tensor(2040., device='cuda:0'),\n",
       " 5: tensor(2040., device='cuda:0'),\n",
       " 6: tensor(2040., device='cuda:0'),\n",
       " 7: tensor(2040., device='cuda:0'),\n",
       " 8: tensor(2040., device='cuda:0'),\n",
       " 9: tensor(2040., device='cuda:0'),\n",
       " 10: tensor(2040., device='cuda:0'),\n",
       " 11: tensor(2040., device='cuda:0'),\n",
       " 12: tensor(2040., device='cuda:0'),\n",
       " 13: tensor(2040., device='cuda:0'),\n",
       " 14: tensor(2040., device='cuda:0'),\n",
       " 15: tensor(2040., device='cuda:0'),\n",
       " 16: tensor(2040., device='cuda:0'),\n",
       " 17: tensor(2040., device='cuda:0'),\n",
       " 18: tensor(2040., device='cuda:0'),\n",
       " 19: tensor(2040., device='cuda:0'),\n",
       " 20: tensor(2040., device='cuda:0'),\n",
       " 21: tensor(2040., device='cuda:0'),\n",
       " 22: tensor(2040., device='cuda:0'),\n",
       " 23: tensor(2040., device='cuda:0'),\n",
       " 24: tensor(2040., device='cuda:0'),\n",
       " 25: tensor(2040., device='cuda:0'),\n",
       " 26: tensor(2040., device='cuda:0'),\n",
       " 27: tensor(2040., device='cuda:0'),\n",
       " 28: tensor(2040., device='cuda:0'),\n",
       " 29: tensor(2040., device='cuda:0'),\n",
       " 30: tensor(2040., device='cuda:0'),\n",
       " 31: tensor(2040., device='cuda:0'),\n",
       " 32: tensor(2040., device='cuda:0'),\n",
       " 33: tensor(2040., device='cuda:0'),\n",
       " 34: tensor(2040., device='cuda:0'),\n",
       " 35: tensor(2040., device='cuda:0'),\n",
       " 36: tensor(2040., device='cuda:0'),\n",
       " 37: tensor(2040., device='cuda:0'),\n",
       " 38: tensor(2040., device='cuda:0'),\n",
       " 39: tensor(2040., device='cuda:0'),\n",
       " 40: tensor(2040., device='cuda:0'),\n",
       " 41: tensor(2040., device='cuda:0'),\n",
       " 42: tensor(2040., device='cuda:0'),\n",
       " 43: tensor(2040., device='cuda:0'),\n",
       " 44: tensor(2040., device='cuda:0'),\n",
       " 45: tensor(2040., device='cuda:0'),\n",
       " 46: tensor(2040., device='cuda:0'),\n",
       " 47: tensor(2040., device='cuda:0'),\n",
       " 48: tensor(2040., device='cuda:0'),\n",
       " 49: tensor(2040., device='cuda:0'),\n",
       " 50: tensor(2040., device='cuda:0'),\n",
       " 51: tensor(2040., device='cuda:0'),\n",
       " 52: tensor(2040., device='cuda:0'),\n",
       " 53: tensor(2040., device='cuda:0'),\n",
       " 54: tensor(2040., device='cuda:0'),\n",
       " 55: tensor(2040., device='cuda:0'),\n",
       " 56: tensor(2040., device='cuda:0'),\n",
       " 57: tensor(2040., device='cuda:0'),\n",
       " 58: tensor(2040., device='cuda:0'),\n",
       " 59: tensor(2040., device='cuda:0'),\n",
       " 60: tensor(2040., device='cuda:0'),\n",
       " 61: tensor(2040., device='cuda:0'),\n",
       " 62: tensor(2040., device='cuda:0'),\n",
       " 63: tensor(2040., device='cuda:0'),\n",
       " 64: tensor(2040., device='cuda:0'),\n",
       " 65: tensor(2040., device='cuda:0'),\n",
       " 66: tensor(2040., device='cuda:0'),\n",
       " 67: tensor(2040., device='cuda:0'),\n",
       " 68: tensor(2040., device='cuda:0'),\n",
       " 69: tensor(2040., device='cuda:0'),\n",
       " 70: tensor(2040., device='cuda:0'),\n",
       " 71: tensor(2040., device='cuda:0'),\n",
       " 72: tensor(2040., device='cuda:0'),\n",
       " 73: tensor(2040., device='cuda:0'),\n",
       " 74: tensor(2040., device='cuda:0'),\n",
       " 75: tensor(2040., device='cuda:0'),\n",
       " 76: tensor(2040., device='cuda:0'),\n",
       " 77: tensor(2040., device='cuda:0'),\n",
       " 78: tensor(2040., device='cuda:0'),\n",
       " 79: tensor(2040., device='cuda:0'),\n",
       " 80: tensor(2040., device='cuda:0'),\n",
       " 81: tensor(2040., device='cuda:0'),\n",
       " 82: tensor(2040., device='cuda:0'),\n",
       " 83: tensor(2040., device='cuda:0'),\n",
       " 84: tensor(2040., device='cuda:0'),\n",
       " 85: tensor(2040., device='cuda:0'),\n",
       " 86: tensor(2040., device='cuda:0'),\n",
       " 87: tensor(2040., device='cuda:0'),\n",
       " 88: tensor(2040., device='cuda:0'),\n",
       " 89: tensor(2040., device='cuda:0'),\n",
       " 90: tensor(2040., device='cuda:0'),\n",
       " 91: tensor(2040., device='cuda:0'),\n",
       " 92: tensor(2040., device='cuda:0'),\n",
       " 93: tensor(2040., device='cuda:0'),\n",
       " 94: tensor(2040., device='cuda:0'),\n",
       " 95: tensor(2040., device='cuda:0'),\n",
       " 96: tensor(2040., device='cuda:0'),\n",
       " 97: tensor(2040., device='cuda:0'),\n",
       " 98: tensor(2040., device='cuda:0'),\n",
       " 99: tensor(2040., device='cuda:0'),\n",
       " 100: tensor(2040., device='cuda:0'),\n",
       " 101: tensor(2040., device='cuda:0'),\n",
       " 102: tensor(2040., device='cuda:0'),\n",
       " 103: tensor(2040., device='cuda:0'),\n",
       " 104: tensor(2040., device='cuda:0'),\n",
       " 105: tensor(2040., device='cuda:0'),\n",
       " 106: tensor(2040., device='cuda:0'),\n",
       " 107: tensor(2040., device='cuda:0'),\n",
       " 108: tensor(2040., device='cuda:0'),\n",
       " 109: tensor(2040., device='cuda:0'),\n",
       " 110: tensor(2040., device='cuda:0'),\n",
       " 111: tensor(2040., device='cuda:0'),\n",
       " 112: tensor(2040., device='cuda:0'),\n",
       " 113: tensor(2040., device='cuda:0'),\n",
       " 114: tensor(2040., device='cuda:0'),\n",
       " 115: tensor(2040., device='cuda:0'),\n",
       " 116: tensor(2040., device='cuda:0'),\n",
       " 117: tensor(2040., device='cuda:0'),\n",
       " 118: tensor(2040., device='cuda:0'),\n",
       " 119: tensor(2040., device='cuda:0'),\n",
       " 120: tensor(2040., device='cuda:0'),\n",
       " 121: tensor(2040., device='cuda:0'),\n",
       " 122: tensor(2040., device='cuda:0'),\n",
       " 123: tensor(2040., device='cuda:0'),\n",
       " 124: tensor(2040., device='cuda:0'),\n",
       " 125: tensor(2040., device='cuda:0'),\n",
       " 126: tensor(2040., device='cuda:0'),\n",
       " 127: tensor(2040., device='cuda:0'),\n",
       " 128: tensor(2040., device='cuda:0'),\n",
       " 129: tensor(2040., device='cuda:0'),\n",
       " 130: tensor(2040., device='cuda:0'),\n",
       " 131: tensor(2040., device='cuda:0'),\n",
       " 132: tensor(2040., device='cuda:0'),\n",
       " 133: tensor(2040., device='cuda:0'),\n",
       " 134: tensor(2040., device='cuda:0'),\n",
       " 135: tensor(2040., device='cuda:0'),\n",
       " 136: tensor(2040., device='cuda:0'),\n",
       " 137: tensor(2040., device='cuda:0'),\n",
       " 138: tensor(2040., device='cuda:0'),\n",
       " 139: tensor(2040., device='cuda:0'),\n",
       " 140: tensor(2040., device='cuda:0'),\n",
       " 141: tensor(2040., device='cuda:0'),\n",
       " 142: tensor(2040., device='cuda:0'),\n",
       " 143: tensor(2040., device='cuda:0'),\n",
       " 144: tensor(2040., device='cuda:0'),\n",
       " 145: tensor(2040., device='cuda:0'),\n",
       " 146: tensor(2040., device='cuda:0'),\n",
       " 147: tensor(2040., device='cuda:0'),\n",
       " 148: tensor(2040., device='cuda:0'),\n",
       " 149: tensor(2040., device='cuda:0'),\n",
       " 150: tensor(2040., device='cuda:0'),\n",
       " 151: tensor(2040., device='cuda:0'),\n",
       " 152: tensor(2040., device='cuda:0'),\n",
       " 153: tensor(2040., device='cuda:0'),\n",
       " 154: tensor(2040., device='cuda:0'),\n",
       " 155: tensor(2040., device='cuda:0'),\n",
       " 156: tensor(2040., device='cuda:0'),\n",
       " 157: tensor(2040., device='cuda:0'),\n",
       " 158: tensor(2040., device='cuda:0'),\n",
       " 159: tensor(2040., device='cuda:0'),\n",
       " 160: tensor(2040., device='cuda:0'),\n",
       " 161: tensor(2040., device='cuda:0'),\n",
       " 162: tensor(2040., device='cuda:0'),\n",
       " 163: tensor(2040., device='cuda:0'),\n",
       " 164: tensor(2040., device='cuda:0'),\n",
       " 165: tensor(2040., device='cuda:0'),\n",
       " 166: tensor(2040., device='cuda:0'),\n",
       " 167: tensor(2040., device='cuda:0'),\n",
       " 168: tensor(2040., device='cuda:0'),\n",
       " 169: tensor(2040., device='cuda:0'),\n",
       " 170: tensor(2040., device='cuda:0'),\n",
       " 171: tensor(2040., device='cuda:0'),\n",
       " 172: tensor(2040., device='cuda:0'),\n",
       " 173: tensor(2040., device='cuda:0'),\n",
       " 174: tensor(2040., device='cuda:0'),\n",
       " 175: tensor(2040., device='cuda:0'),\n",
       " 176: tensor(2040., device='cuda:0'),\n",
       " 177: tensor(2040., device='cuda:0'),\n",
       " 178: tensor(2040., device='cuda:0'),\n",
       " 179: tensor(2040., device='cuda:0'),\n",
       " 180: tensor(2040., device='cuda:0'),\n",
       " 181: tensor(2040., device='cuda:0'),\n",
       " 182: tensor(2040., device='cuda:0'),\n",
       " 183: tensor(2040., device='cuda:0'),\n",
       " 184: tensor(2040., device='cuda:0'),\n",
       " 185: tensor(2040., device='cuda:0'),\n",
       " 186: tensor(2040., device='cuda:0'),\n",
       " 187: tensor(2040., device='cuda:0'),\n",
       " 188: tensor(2040., device='cuda:0'),\n",
       " 189: tensor(2040., device='cuda:0'),\n",
       " 190: tensor(2040., device='cuda:0'),\n",
       " 191: tensor(2040., device='cuda:0'),\n",
       " 192: tensor(2040., device='cuda:0'),\n",
       " 193: tensor(2040., device='cuda:0'),\n",
       " 194: tensor(2040., device='cuda:0'),\n",
       " 195: tensor(2040., device='cuda:0'),\n",
       " 196: tensor(2040., device='cuda:0'),\n",
       " 197: tensor(2040., device='cuda:0'),\n",
       " 198: tensor(2040., device='cuda:0'),\n",
       " 199: tensor(2040., device='cuda:0'),\n",
       " 200: tensor(2040., device='cuda:0'),\n",
       " 201: tensor(2040., device='cuda:0'),\n",
       " 202: tensor(2040., device='cuda:0'),\n",
       " 203: tensor(2040., device='cuda:0'),\n",
       " 204: tensor(2040., device='cuda:0'),\n",
       " 205: tensor(2040., device='cuda:0'),\n",
       " 206: tensor(2040., device='cuda:0'),\n",
       " 207: tensor(2040., device='cuda:0'),\n",
       " 208: tensor(2040., device='cuda:0'),\n",
       " 209: tensor(2040., device='cuda:0'),\n",
       " 210: tensor(2040., device='cuda:0'),\n",
       " 211: tensor(2040., device='cuda:0'),\n",
       " 212: tensor(2040., device='cuda:0'),\n",
       " 213: tensor(2040., device='cuda:0'),\n",
       " 214: tensor(2040., device='cuda:0'),\n",
       " 215: tensor(2040., device='cuda:0'),\n",
       " 216: tensor(2040., device='cuda:0'),\n",
       " 217: tensor(2040., device='cuda:0'),\n",
       " 218: tensor(2040., device='cuda:0'),\n",
       " 219: tensor(2040., device='cuda:0'),\n",
       " 220: tensor(2040., device='cuda:0'),\n",
       " 221: tensor(2040., device='cuda:0'),\n",
       " 222: tensor(2040., device='cuda:0'),\n",
       " 223: tensor(2040., device='cuda:0'),\n",
       " 224: tensor(2040., device='cuda:0'),\n",
       " 225: tensor(2040., device='cuda:0'),\n",
       " 226: tensor(2040., device='cuda:0'),\n",
       " 227: tensor(2040., device='cuda:0'),\n",
       " 228: tensor(2040., device='cuda:0'),\n",
       " 229: tensor(2040., device='cuda:0'),\n",
       " 230: tensor(2040., device='cuda:0'),\n",
       " 231: tensor(2040., device='cuda:0'),\n",
       " 232: tensor(2040., device='cuda:0'),\n",
       " 233: tensor(2040., device='cuda:0'),\n",
       " 234: tensor(2040., device='cuda:0'),\n",
       " 235: tensor(2040., device='cuda:0'),\n",
       " 236: tensor(2040., device='cuda:0'),\n",
       " 237: tensor(2040., device='cuda:0'),\n",
       " 238: tensor(2040., device='cuda:0'),\n",
       " 239: tensor(2040., device='cuda:0'),\n",
       " 240: tensor(2040., device='cuda:0'),\n",
       " 241: tensor(2040., device='cuda:0'),\n",
       " 242: tensor(2040., device='cuda:0'),\n",
       " 243: tensor(2040., device='cuda:0'),\n",
       " 244: tensor(2040., device='cuda:0'),\n",
       " 245: tensor(2040., device='cuda:0'),\n",
       " 246: tensor(2040., device='cuda:0'),\n",
       " 247: tensor(2040., device='cuda:0'),\n",
       " 248: tensor(2040., device='cuda:0'),\n",
       " 249: tensor(2040., device='cuda:0'),\n",
       " 250: tensor(2040., device='cuda:0'),\n",
       " 251: tensor(2040., device='cuda:0'),\n",
       " 252: tensor(2040., device='cuda:0'),\n",
       " 253: tensor(2040., device='cuda:0'),\n",
       " 254: tensor(2040., device='cuda:0'),\n",
       " 255: tensor(2040., device='cuda:0'),\n",
       " 256: tensor(2040., device='cuda:0'),\n",
       " 257: tensor(2040., device='cuda:0'),\n",
       " 258: tensor(2040., device='cuda:0'),\n",
       " 259: tensor(2040., device='cuda:0'),\n",
       " 260: tensor(2040., device='cuda:0'),\n",
       " 261: tensor(2040., device='cuda:0'),\n",
       " 262: tensor(2040., device='cuda:0'),\n",
       " 263: tensor(2040., device='cuda:0'),\n",
       " 264: tensor(2040., device='cuda:0'),\n",
       " 265: tensor(2040., device='cuda:0'),\n",
       " 266: tensor(2040., device='cuda:0'),\n",
       " 267: tensor(2040., device='cuda:0'),\n",
       " 268: tensor(2040., device='cuda:0'),\n",
       " 269: tensor(2040., device='cuda:0'),\n",
       " 270: tensor(2040., device='cuda:0'),\n",
       " 271: tensor(2040., device='cuda:0'),\n",
       " 272: tensor(2040., device='cuda:0'),\n",
       " 273: tensor(2040., device='cuda:0'),\n",
       " 274: tensor(2040., device='cuda:0'),\n",
       " 275: tensor(2040., device='cuda:0'),\n",
       " 276: tensor(2040., device='cuda:0'),\n",
       " 277: tensor(2040., device='cuda:0'),\n",
       " 278: tensor(2040., device='cuda:0'),\n",
       " 279: tensor(2040., device='cuda:0'),\n",
       " 280: tensor(2040., device='cuda:0'),\n",
       " 281: tensor(2040., device='cuda:0'),\n",
       " 282: tensor(2040., device='cuda:0'),\n",
       " 283: tensor(2040., device='cuda:0'),\n",
       " 284: tensor(2040., device='cuda:0'),\n",
       " 285: tensor(2040., device='cuda:0'),\n",
       " 286: tensor(2040., device='cuda:0'),\n",
       " 287: tensor(2040., device='cuda:0'),\n",
       " 288: tensor(2040., device='cuda:0'),\n",
       " 289: tensor(2040., device='cuda:0'),\n",
       " 290: tensor(2040., device='cuda:0'),\n",
       " 291: tensor(2040., device='cuda:0'),\n",
       " 292: tensor(2040., device='cuda:0'),\n",
       " 293: tensor(2040., device='cuda:0'),\n",
       " 294: tensor(2040., device='cuda:0'),\n",
       " 295: tensor(2040., device='cuda:0'),\n",
       " 296: tensor(2040., device='cuda:0'),\n",
       " 297: tensor(2040., device='cuda:0'),\n",
       " 298: tensor(2040., device='cuda:0'),\n",
       " 299: tensor(2040., device='cuda:0'),\n",
       " 300: tensor(2040., device='cuda:0'),\n",
       " 301: tensor(2040., device='cuda:0'),\n",
       " 302: tensor(2040., device='cuda:0'),\n",
       " 303: tensor(2040., device='cuda:0'),\n",
       " 304: tensor(2040., device='cuda:0'),\n",
       " 305: tensor(2040., device='cuda:0'),\n",
       " 306: tensor(2040., device='cuda:0'),\n",
       " 307: tensor(2040., device='cuda:0'),\n",
       " 308: tensor(2040., device='cuda:0'),\n",
       " 309: tensor(2040., device='cuda:0'),\n",
       " 310: tensor(2040., device='cuda:0'),\n",
       " 311: tensor(2040., device='cuda:0'),\n",
       " 312: tensor(2040., device='cuda:0'),\n",
       " 313: tensor(2040., device='cuda:0'),\n",
       " 314: tensor(2040., device='cuda:0'),\n",
       " 315: tensor(2040., device='cuda:0'),\n",
       " 316: tensor(2040., device='cuda:0'),\n",
       " 317: tensor(2040., device='cuda:0'),\n",
       " 318: tensor(2040., device='cuda:0'),\n",
       " 319: tensor(2040., device='cuda:0'),\n",
       " 320: tensor(2040., device='cuda:0'),\n",
       " 321: tensor(2040., device='cuda:0'),\n",
       " 322: tensor(2040., device='cuda:0'),\n",
       " 323: tensor(2040., device='cuda:0'),\n",
       " 324: tensor(2040., device='cuda:0'),\n",
       " 325: tensor(2040., device='cuda:0'),\n",
       " 326: tensor(2040., device='cuda:0'),\n",
       " 327: tensor(2040., device='cuda:0'),\n",
       " 328: tensor(2040., device='cuda:0'),\n",
       " 329: tensor(2040., device='cuda:0'),\n",
       " 330: tensor(2040., device='cuda:0'),\n",
       " 331: tensor(2040., device='cuda:0'),\n",
       " 332: tensor(2040., device='cuda:0'),\n",
       " 333: tensor(2040., device='cuda:0'),\n",
       " 334: tensor(2040., device='cuda:0'),\n",
       " 335: tensor(2040., device='cuda:0'),\n",
       " 336: tensor(2040., device='cuda:0'),\n",
       " 337: tensor(2040., device='cuda:0'),\n",
       " 338: tensor(2040., device='cuda:0'),\n",
       " 339: tensor(2040., device='cuda:0'),\n",
       " 340: tensor(2040., device='cuda:0'),\n",
       " 341: tensor(2040., device='cuda:0'),\n",
       " 342: tensor(2040., device='cuda:0'),\n",
       " 343: tensor(2040., device='cuda:0'),\n",
       " 344: tensor(2040., device='cuda:0'),\n",
       " 345: tensor(2040., device='cuda:0'),\n",
       " 346: tensor(2040., device='cuda:0'),\n",
       " 347: tensor(2040., device='cuda:0'),\n",
       " 348: tensor(2040., device='cuda:0'),\n",
       " 349: tensor(2040., device='cuda:0'),\n",
       " 350: tensor(2040., device='cuda:0'),\n",
       " 351: tensor(2040., device='cuda:0'),\n",
       " 352: tensor(2040., device='cuda:0'),\n",
       " 353: tensor(2040., device='cuda:0'),\n",
       " 354: tensor(2040., device='cuda:0'),\n",
       " 355: tensor(2040., device='cuda:0'),\n",
       " 356: tensor(2040., device='cuda:0'),\n",
       " 357: tensor(2040., device='cuda:0'),\n",
       " 358: tensor(2040., device='cuda:0'),\n",
       " 359: tensor(2040., device='cuda:0'),\n",
       " 360: tensor(2040., device='cuda:0'),\n",
       " 361: tensor(2040., device='cuda:0'),\n",
       " 362: tensor(2040., device='cuda:0'),\n",
       " 363: tensor(2040., device='cuda:0'),\n",
       " 364: tensor(2040., device='cuda:0'),\n",
       " 365: tensor(2040., device='cuda:0'),\n",
       " 366: tensor(2040., device='cuda:0'),\n",
       " 367: tensor(2040., device='cuda:0'),\n",
       " 368: tensor(2040., device='cuda:0'),\n",
       " 369: tensor(2040., device='cuda:0'),\n",
       " 370: tensor(2040., device='cuda:0'),\n",
       " 371: tensor(2040., device='cuda:0'),\n",
       " 372: tensor(2040., device='cuda:0'),\n",
       " 373: tensor(2040., device='cuda:0')}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_layer = hidden_states[0][-1]\n",
    "last_hidden_layer.shape  # batch_size, sequence_length, hidden_size = (1, 374, 576)\n",
    "\n",
    "new_token = '<last_seq_of_last_hidden_layer_embedding>'\n",
    "tokenizer.add_tokens(new_token)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "\n",
    "embed = model.get_input_embeddings()\n",
    "\n",
    "scores = {}\n",
    "# Try out all the sequences in the last layer\n",
    "for sequence in tqdm.trange(last_hidden_layer.shape[1]):\n",
    "    new_token_embedding = last_hidden_layer[0][373]    # 374th elemet of size 576 (so last sequence of the last layer)\n",
    "\n",
    "    # Normalize it using Sigmoid function\n",
    "    new_token_embedding = 1 / (1 + torch.exp(-new_token_embedding))\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = embed.weight[new_token_id].data.copy_(new_token_embedding)\n",
    "\n",
    "\n",
    "    out = model.generate(\n",
    "        inputs['input_ids'].to(DEVICE),\n",
    "        attention_mask = inputs['attention_mask'].to(DEVICE),\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        max_new_tokens = 1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "    scores[sequence] = out.scores[0][0, -1]\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "All SEQUENCES return the same score, why is that?\n",
    "\n",
    "Score is higher than all other logits, so we will test next the average of all generations of a specific merge of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Calculating Embedding for new tokens\n",
    "\n",
    "Using last step's findings to create new embeddings for new_tokens. The steps for the calculation of a single \"new_token\" are as follow:\n",
    "1. Look at all phrases that generate \"new_token\" (More specifically, look at phrases where the next `N` tokens match the tokenization of \"new_token\")\n",
    "1. Use the **hidden_states** of all of them and pick the **last_state** for all generations\n",
    "1. Calculate the **average** of the **last_sequence** of all vectors previously obtained\n",
    "1. Update the **embed** of the model with said average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining hidden states: 100%|██████████| 1391/1391 [00:23<00:00, 59.91it/s]\n",
      "Validating generations match the new tokens: 100%|██████████| 1391/1391 [00:23<00:00, 58.52it/s]\n"
     ]
    }
   ],
   "source": [
    "new_token = new_tokens[0]\n",
    "tokenizer.add_tokens(new_token)\n",
    "new_token_id = tokenizer.encode(new_token)[0]\n",
    "model.resize_token_embeddings(new_token_id+1)\n",
    "\n",
    "phrases_new_token = generation[generation['generation'] == new_token]['original_text'].tolist()\n",
    "\n",
    "# Step 2.1\n",
    "#   Obtain hidden states (last sequence of last layer) for all phrases\n",
    "hidden_states_last_vector = []\n",
    "for phrase in tqdm.tqdm(phrases_new_token, desc='Obtaining hidden states'):\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    hidden_states_last_vector.append(model.generate(\n",
    "        inputs['input_ids'].to(DEVICE),\n",
    "        attention_mask=inputs['attention_mask'].to(DEVICE),\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        max_new_tokens = 1,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    ).hidden_states[0][-1][0, -1])\n",
    "hidden_states_last_vector = torch.stack(hidden_states_last_vector).to(DEVICE)\n",
    "\n",
    "new_token_embedding = hidden_states_last_vector.mean(dim=0)\n",
    "\n",
    "\n",
    "embed = model.get_input_embeddings()\n",
    "with torch.no_grad():\n",
    "    embed.weight[new_token_id] = new_token_embedding\n",
    "\n",
    "\n",
    "# Validate all phrases generate the \"new_token\"\n",
    "tokens_generated = []\n",
    "for phrase in tqdm.tqdm(phrases_new_token, desc='Validating generations match the new tokens'):\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    tokens_generated.append(model.generate(\n",
    "        inputs['input_ids'].to(DEVICE),\n",
    "        attention_mask=inputs['attention_mask'].to(DEVICE),\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        max_new_tokens = 1,\n",
    "    )[0, -1])\n",
    "assert all([t.item() == new_token_id for t in tokens_generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passagem: Lei do bom samaritano -- As leis do bom samaritano oferecem proteção jurídica às pessoas que prestam assistência razoável a quem está, ou pensa estar, ferido, doente, em perigo ou incapacitado. A proteção destina-se a reduzir a hesitação dos transeuntes em prestar assistência, por receio de serem processados ou acusados de lesão não intencional ou morte por negligência. Um exemplo de uma lei deste tipo em zonas de direito consuetudinário do Canadá: a doutrina do bom samaritano é um princípio jurídico que impede que um socorrista que tenha ajudado voluntariamente uma vítima em perigo seja processado por um ato ilícito. O seu objetivo é evitar que as pessoas se mostrem relutantes em ajudar um estranho em necessidade por receio de repercussões legais caso cometam algum erro no tratamento. Em contrapartida, uma lei de obrigação de socorro exige que as pessoas prestem assistência e responsabiliza quem não o fizer.\n",
      "Pergunta: As leis do bom samaritano protegem as pessoas que ajudam num acidente?\n",
      "Resposta (0-Verdade, 1-Mentira):\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "phrase = phrases_new_token[0]\n",
    "inputs = tokenizer(phrase, return_tensors='pt')\n",
    "print(tokenizer.decode(model.generate(\n",
    "    inputs['input_ids'].to(DEVICE),\n",
    "    attention_mask=inputs['attention_mask'].to(DEVICE),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 10,\n",
    ")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = loader.load_model_and_tokenizer(model_name=model_name, device=DEVICE, tokenizer_kwargs={'padding_side': 'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3789, -0.2188,  0.0276,  ...,  0.1924,  0.0388,  0.0461],\n",
       "        [-0.0371,  0.0315,  0.0220,  ..., -0.0464, -0.0113, -0.0359],\n",
       "        [-0.0178,  0.0322,  0.0254,  ..., -0.0226, -0.0186, -0.0376],\n",
       "        ...,\n",
       "        [ 0.1816, -0.0601, -0.0923,  ..., -0.0325, -0.2109, -0.1719],\n",
       "        [-0.0208,  0.0723,  0.0371,  ...,  0.1001, -0.1992,  0.1865],\n",
       "        [ 0.2695,  0.1309,  0.2412,  ..., -0.1152,  0.1855, -0.3691]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings().weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3789, -0.2188,  0.0276,  ...,  0.1924,  0.0388,  0.0461],\n",
       "        [-0.0371,  0.0315,  0.0220,  ..., -0.0464, -0.0113, -0.0359],\n",
       "        [-0.0178,  0.0322,  0.0254,  ..., -0.0226, -0.0186, -0.0376],\n",
       "        ...,\n",
       "        [ 0.1816, -0.0601, -0.0923,  ..., -0.0325, -0.2109, -0.1719],\n",
       "        [-0.0208,  0.0723,  0.0371,  ...,  0.1001, -0.1992,  0.1865],\n",
       "        [ 0.2695,  0.1309,  0.2412,  ..., -0.1152,  0.1855, -0.3691]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    inputs['input_ids'].to(DEVICE),\n",
    "    attention_mask = inputs['attention_mask'].to(DEVICE),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_logits=True,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49152])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([576])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.hidden_states[-1][-1][0, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1875, device='cuda:0')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits[-1].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

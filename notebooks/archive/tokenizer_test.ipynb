{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer test\n",
    "\n",
    "This notebook serves to test the behaviour of a tokenizer trained in english in portuguese text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cmd\n",
    "# conda install --yes pytorch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from src.utils import compute_perplexity, train_tokenizer\n",
    "\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "MODEL = \"microsoft/phi-1_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fbe64996664221bd992625f2cf067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yakim\\.cache\\huggingface\\hub\\models--microsoft--phi-1_5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aaac45062c43258aa81c95d692a482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a9e0e6da0d46908c8d41396543f0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88a233b426443978ee64b9a5722d140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a647a7831a440079ce2b5736f6f09f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab18301eadf4c9c8eb3b4ad1bd17a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94210bd6055240da96f4292503cd27da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed7f941a5404c3bba9c5ab060e8e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2ca09875844f8e8cbe923dd0227900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'ĠI', \"'m\", 'Ġa', 'Ġsingle', 'Ġsentence', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tkns = tokenizer.tokenize(\"Hello, I'm a single sentence!\")\n",
    "en_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spaces are converted in a special character (the Ġ ) in the tokenizer prior to BPE splitting mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process. [link](https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475/2?u=joaogante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ol',\n",
       " 'Ã¡',\n",
       " ',',\n",
       " 'Ġe',\n",
       " 'u',\n",
       " 'Ġsou',\n",
       " 'Ġu',\n",
       " 'ma',\n",
       " 'Ġfr',\n",
       " 'ase',\n",
       " 'Ġsim',\n",
       " 'ples',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tkns = tokenizer.tokenize(\"Olá, eu sou uma frase simples!\")\n",
    "pt_tkns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tokenizer splited the word `\"eu\"` into `\"Ġe\"` and `\"u\"` which is strange, since the `\"eu\"` is a very common word in Portuguese. Also note that the word `\"sentence\"` is keepet as a unique token while its equivilant in portugueses `\"frase\"` is splited into two tokens `\"Ġfr\"` and `\"ase\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in English: 8\n",
      "Number of tokens in Portuguese: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens in English: {len(en_tkns)}\")\n",
    "print(f\"Number of tokens in Portuguese: {len(pt_tkns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last remark, note that the number of tokens produced for the portuguese sentence is almost double the aomount of tokens produced for english. This is problem in the efeciency of the system as it requires much more compute to produce the text in portuguese than the text in english.\n",
    "\n",
    "Is there any way to limit this phenomne?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Preplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of English: 28.975732803344727\n",
      "Perplexity of Portuguese: 160.80287170410156\n"
     ]
    }
   ],
   "source": [
    "ppl_en = compute_perplexity(llm, tokenizer, \"Hello, I'm a single sentence!\")\n",
    "ppl_pt = compute_perplexity(llm, tokenizer, \"Olá, eu sou uma frase simples!\")\n",
    "print(f\"Perplexity of English: {ppl_en}\")\n",
    "print(f\"Perplexity of Portuguese: {ppl_pt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The portuguese sentence has a lower preplexity than the english sentence meaning that the sequence of words in the portuguesese sentence is less surprising than sequence of words in the english sentence. This is expected as the preplexity mesuare is used to evaluate how well the language model learned the training set. Since the phi model was only trained on english text it is normal that the portuguese text to have a much higher preplexity. The question is: can we further maintain or lower this value of preplexity for the portuguese text while lowering the amount of tokens generated?\n",
    "\n",
    "As a first approach let's test the following approach. We will start by selecting a portuguese corpus (lusa news probably). Second we will compute the preplexity of the phi-2 model on that corpus. This will give us a baseline to take as a reference. As a third step, we will train a tokenizer on the portuguse corpus. Then, we will check the tokens that are on the new vocabolary that were missing in the original one. The following step is to access what is the best way to cerate the embeddings for this new tokens to the orignal tokenizer so that the preplexity of the model gets lower on the portuguese corpus.\n",
    "\n",
    "The stratagy to create the new embeddings migth be by employing an aggregation strategy or by training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tokenizer in Portuguese text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = (DATA_DIR / \"sample.txt\").read_text()\n",
    "print(corpus[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = corpus.split(\"\\n\")\n",
    "print(f\"Number of lines: {len(lines)}\")\n",
    "\n",
    "lines = list(set(lines))\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "print(f\"Number of unique lines {len(set(lines))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pt = train_tokenizer(tokenizer, lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of tokens with this tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tkns = tokenizer.tokenize(\"Olá, eu sou uma frase simples!\")\n",
    "print(f\"Number of tokens in with original tokenizer: {len(pt_tkns)}\")\n",
    "\n",
    "pt_tkns = tokenizer_pt.tokenize(\"Olá, eu sou uma frase simples!\")\n",
    "print(f\"Number of tokens with new tokenizer: {len(pt_tkns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good. The number of tokens with the new tokenizer is lower than the original one. \n",
    "\n",
    "What are the tokens in new tokenizer that are not on the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_org = tokenizer.vocab.keys()\n",
    "vocab_new = tokenizer_pt.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = list(set(vocab_new) - set(vocab_org))\n",
    "print(f\"Number of new tokens: {len(new_tokens)}\")\n",
    "print(f\"(some) New tokens:\\n{new_tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are pretty frquent portuguese words that were missing from the original vocab. Lets now try to add the new tokens to the original vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A sample of the tokens to be added:\\n{new_tokens[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first take one token as an example and see how it would be tokenized by the oroginal tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = new_tokens[1]\n",
    "example = \" contrato\"\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(f\"Previous tokens: {tokens}\")\n",
    "\n",
    "new_token = \"\".join(tokens)\n",
    "print(f\"New token: {new_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now get the embeddings for this tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = llm.base_model\n",
    "token_embs = model.embed_tokens(torch.tensor(token_ids))\n",
    "token_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embs_agg = token_embs.mean(dim=0)\n",
    "token_embs_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add this a new token to the tokenizer and the new embedding to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of tokens before adding the token: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([new_token])\n",
    "new_token_id = tokenizer.vocab[new_token]\n",
    "print(f\"New token id: {new_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"O tipo nao tem contrato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new token has been added with token id 50295. Now we need to add that id to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.embed_tokens\n",
    "type(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miss match between the embeddings and the vocab size explained in this [chat](https://huggingface.co/bigscience/bloom/discussions/120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = embed.weight.data\n",
    "print(f\"Shape of weight matrix: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new tokens to the model\n",
    "weight = torch.cat([weight, token_embs_agg.unsqueeze(0)], dim=0)\n",
    "print(f\"Shape of weight matrix: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[new_token_id] = token_embs_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.weight.data = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert  torch.equal(llm.model.embed_tokens(torch.tensor(new_token_id)), token_embs_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now test if this reduces the preplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_original = AutoModelForCausalLM.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_original = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(llm_original, tokenizer_original, \"Olá, eu sou uma frase simples com a palavra incapacidade!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(llm, tokenizer, \"Olá, eu sou uma frase simples com a palavra incapacidade!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Olá, eu sou uma frase simples com a palavra incapacidade!\"\n",
    "tokenizer_original.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

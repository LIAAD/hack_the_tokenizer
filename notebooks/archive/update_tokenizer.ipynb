{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Iterable, Callable\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer, PreTrainedTokenizerFast, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a BPE for PT-PT\n",
    "\n",
    "Using the data we already collected for the benchmarks, creating a BPE based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from src import BENCHMARKS, BPE\n",
    "\n",
    "document_data = BENCHMARKS.get_training_data('list')\n",
    "\n",
    "encodings = BPE(document_data, vocab_size=1_000)\n",
    "encodings.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers, models, pre_tokenizers\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=1_000)\n",
    "\n",
    "# Generate the files for the tokenizer to train on\n",
    "with open('trainer.txt', 'w') as f: f.writelines(document_data)\n",
    "\n",
    "tokenizer.train(files=['trainer.txt'], trainer=trainer)\n",
    "tokenizer.encode('Olá o meu nome é Duarte').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = 'Olá o meu nome é Duarte'\n",
    "print(raw_text)\n",
    "print(encodings.from_id_to_tokens(encodings.tokenize(raw_text), byte_decode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding tokens not in model tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'HuggingFaceTB/SmolLM2-135M'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL, use_safetensors=True, torch_dtype= torch.bfloat16).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(' Augusta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab['ĠAugusta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokens = set(tokenizer.vocab.keys())\n",
    "bpe_tokens = set(\n",
    "    b''.join(\n",
    "        encodings.from_id_to_tokens([token])\n",
    "    ).decode('utf-8', errors='backslashreplace').replace(' ', 'Ġ')  # \"Ġ\" was used as \" \" in the tokenizer of the model\n",
    "    for token in encodings.vocab\n",
    ")\n",
    "\n",
    "# Obtain the tokens in BPE not present in the model\n",
    "tokens_to_add = bpe_tokens.difference(model_tokens)\n",
    "\n",
    "# Remove digits from tokens_to_add\n",
    "tokens_to_add = [token for token in tokens_to_add if not token.isdigit()]\n",
    "\n",
    "\n",
    "# Replace the \"last\" tokens from the model tokenizer\n",
    "new_vocab = {}\n",
    "\n",
    "vocab_revers = {v: k for k, v in tokenizer.vocab.items()}\n",
    "last_token_id = max(tokenizer.vocab.items(), key=lambda x: x[1])[1]\n",
    "for token_id in range(last_token_id+1):\n",
    "    if token_id > last_token_id - len(tokens_to_add):\n",
    "        new_vocab[tokens_to_add.pop(0)] = token_id\n",
    "        continue\n",
    "    new_vocab[vocab_revers[token_id]] = token_id\n",
    "new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_list', 'w', encoding='utf-8') as f:\n",
    "    for token, token_id in new_vocab.items():\n",
    "        f.write('{:05d} {}\\n'.format(token_id, token))\n",
    "\n",
    "\n",
    "# with open('token_list', 'w', encoding='utf-8') as f:\n",
    "#     for token_id in range(len(tokenizer.vocab)):\n",
    "#         f.write('{:05d} {}\\n'.format(token_id, tokenizer.convert_ids_to_tokens(token_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(' afterwards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings.from_id_to_tokens(encodings.tokenize('depois'), byte_decode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    pair: b''.join(encodings.from_id_to_tokens([encodings.merges[pair]])).decode('utf-8').replace(' ', 'Ġ')\n",
    "    for pair in encodings.merges.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = list(list(model.children())[0].children())[0]\n",
    "\n",
    "#  embeds(torch.Tensor([57]).int().to(DEVICE))  [THIS IS THE SAME AS THE BELLOW `params[57]`]\n",
    "params = list(embeds.parameters())[0]\n",
    "params[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = tokenizer('Olá, sabes falar português?', return_tensors='pt')\n",
    "output = model.generate(input_tokens['input_ids'].to(DEVICE), attention_mask=input_tokens['attention_mask'].to(DEVICE), pad_token_id=tokenizer.eos_token_id, max_new_tokens=100)\n",
    "tokenizer.decode(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

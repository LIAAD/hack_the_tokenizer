{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore some benchmarks for different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Iterable, Callable\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from src import BENCHMARKS\n",
    "BENCHMARKS.config['number_of_evaluations'] = 1\n",
    "BENCHMARKS.config['parallel_batch_size'] = 16\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    # 'HuggingFaceTB/SmolLM2-135M': BENCHMARKS.config.copy(),\n",
    "    # 'NOVA-vision-language/GlorIA-1.3B': BENCHMARKS.config.copy(),\n",
    "    'PORTULAN/gervasio-7b-portuguese-ptpt-decoder': {\n",
    "        'parallel_tasks': False,\n",
    "        'parallel_batch_size': 1,\n",
    "        'model_kwargs': {\n",
    "            'torch_dtype': torch.bfloat16,\n",
    "        },\n",
    "        'generate_model_kwargs': {\n",
    "            'do_sample': False,\n",
    "            'top_p': None,\n",
    "            'temperature': None,\n",
    "            'pad_token_id': 50257,\n",
    "            'return_legacy_cache': False,\n",
    "            \n",
    "        },\n",
    "        'no_config_copy_benchmarks': [],\n",
    "        'tqdm_description': '<{MODEL}> Running {benchmark_name} Benchmark',\n",
    "        'number_of_evaluations': 1\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "for MODEL in MODEL_CONFIGS.keys():\n",
    "    print('{:-^100s}\\n\\n{: ^100s}\\n\\n{:-^100s}'.format('', f'Running benchmarks for model `{MODEL}`', ''))\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, use_safetensors=True, **MODEL_CONFIGS[MODEL]['model_kwargs']).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    BENCHMARKS.config = MODEL_CONFIGS[MODEL]\n",
    "    BENCHMARKS.run(model, tokenizer)\n",
    "    # Freeing up model from GPU memory\n",
    "    model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = BENCHMARKS.get_results()\n",
    "\n",
    "with open('benchmark_results.pkl', 'wb') as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('benchmark_results.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for model in res.keys():\n",
    "    df.append({\n",
    "        'Model': model\n",
    "    })\n",
    "    for benchmark in res[model].keys():\n",
    "        df[-1][benchmark] = res[model][benchmark]['result']\n",
    "pd.DataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS.benchmarks[0].df # Yes/No answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS.benchmarks[1].df # Predict the last word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS: dict[str, dict[str, Iterable[Any] | Callable]] = {}\n",
    "'''\n",
    "    Should be a dictionary with the following as entries:\n",
    "    {\n",
    "        'data': Iterable[Any],\n",
    "        'evaluation': Callable[Any, PreTrainedTokenizer | PreTrainedTokenizerFast, TypeOfData]  -> This function will be called with the \"data\" value\n",
    "    }\n",
    "'''\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MODEL = 'NOVA-vision-language/GlorIA-1.3B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Max\n",
    "def soft_max(arr): return arr.exp() / arr.exp().sum()\n",
    "\n",
    "def get_first_word(original_text: str, predicted_text: str):\n",
    "    predicted_text = predicted_text.replace(original_text, '')\n",
    "    # Regex to find first word\n",
    "    first_word = re.search(r'\\b\\w+\\b', predicted_text)\n",
    "    first_word = first_word.group() if first_word else None\n",
    "    return first_word\n",
    "\n",
    "def load_dataset_to_dataframe(*args, data_dir=None, dataset_types=['train', 'validation', 'test'], **kwargs):\n",
    "    ds = load_dataset(*args, data_dir=data_dir, **kwargs)\n",
    "    output = []\n",
    "    for ds_type in dataset_types:\n",
    "        output.append(ds[ds_type].to_pandas())\n",
    "        output[-1]['Dataset Type'] = ds_type\n",
    "    return pd.concat(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlorIA-1.3B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    use_safetensors=True\n",
    ").to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Benchmark Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CALAME-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CALAME-PT dataset onto a Pandas DataFrame\n",
    "df_handwritten = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_handwritten_only.jsonl\", lines=True)\n",
    "df_handwritten['Source'] = 'Handwritten'\n",
    "df_generated = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_gen_only.jsonl\", lines=True)\n",
    "df_generated['Source'] = 'Generated'\n",
    "calame_pt_df = pd.concat([df_handwritten, df_generated])[['id', 'sentence', 'last_word']]\n",
    "\n",
    "# Defining the Benchmark function for CALAME-PT\n",
    "def benchmark_calamept(\n",
    "    model,\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    dataset: list[dict[str, str]],\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    },\n",
    "    parallel = False\n",
    "):\n",
    "    # NOTE: We could improve this benchmark by using the PROBABILITY Distribution of the actual token.\n",
    "    benchmark_output = {}\n",
    "    if not parallel:\n",
    "        for data in tqdm.tqdm(dataset, desc='Running CALAME-PT Benchmark'):\n",
    "            # Retrieving data from the dictionary\n",
    "            n, predicted_text, correct_word = data['id'], data['sentence'], data['last_word']\n",
    "\n",
    "            input_tokens = tokenizer.encode(predicted_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            prediction = model.generate(\n",
    "                input_tokens,\n",
    "                max_length=input_tokens.size()[1] + 5,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            prediction = tokenizer.decode(prediction[0])\n",
    "            predicted_word = get_first_word(predicted_text, prediction)\n",
    "            benchmark_output[n] = {'text': predicted_text, 'prediction': predicted_word, 'correct_word': correct_word}\n",
    "    else:\n",
    "        input_tokens = [d['sentence'] for d in dataset]\n",
    "        max_length = max(len(d) for d in input_tokens) + 5\n",
    "        input_tokens = tokenizer.encode(input_tokens, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(DEVICE)\n",
    "        predictions = model.generate(\n",
    "            input_tokens,\n",
    "            max_length=max_length,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        for n, pred in enumerate(predictions):\n",
    "            predicted_text = tokenizer.decode(pred[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            predicted_word = get_first_word(dataset[n]['sentence'], predicted_text)\n",
    "            benchmark_output[n] = {'text': dataset[n]['sentence'], 'prediction': predicted_word, 'correct_word': dataset[n]['last_word']}\n",
    "\n",
    "    accurate_preds = sum(ben['prediction'] == ben['correct_word'] for ben in benchmark_output.values())\n",
    "    return {\n",
    "        'benchmark': 'CALAME-PT',\n",
    "        'accuracy': accurate_preds / len(BENCHMARKS['CALAME-PT']['data']),\n",
    "        'accurate_predictions': accurate_preds,\n",
    "        'wrong_predictions': len(BENCHMARKS['CALAME-PT']['data']) - accurate_preds,\n",
    "        'benchmark_predictions': benchmark_output,\n",
    "        'model': MODEL\n",
    "    }\n",
    "\n",
    "# Adding it onto benchmarks dictionary\n",
    "BENCHMARKS['CALAME-PT'] =  {\n",
    "    'data': calame_pt_df.to_dict('records'),\n",
    "    'data-dataframe': calame_pt_df,\n",
    "    'evaluation': benchmark_calamept,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPERGLUE PT-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superglue_benchmarks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. BoolQ\n",
    "\n",
    "Boolean Question task, consists of determining wether a given question is true or false based on a given passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superglue_boolq_df = load_dataset_to_dataframe('PORTULAN/extraglue', data_dir='data/boolq_pt-PT')\n",
    "\n",
    "def benchmark_task_boolq(\n",
    "    model,\n",
    "    tokenizer:  PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    dataset,\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    },\n",
    "    parallel = True,\n",
    "    parallel_group_size = 32\n",
    "):\n",
    "    benchmark_output: list[dict[str, str]] = []\n",
    "    # Prepare input texts\n",
    "    input_texts = []\n",
    "    for data in dataset:\n",
    "        passage, question = data['passage'], data['question']\n",
    "        input_texts.append(f'Passagem: {passage}\\nPergunta: {question}\\nResposta (0-Verdade, 1-Mentira):')\n",
    "\n",
    "    # Obtain the predictions using the model generation (either parallel or not)\n",
    "    predictions = []\n",
    "    if not parallel:\n",
    "        # Predict one input_text at a time\n",
    "        for input_text in tqdm.tqdm(input_texts, desc='Running Superglue ptPT Benchamrk - Task BoolQ'):\n",
    "            input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            predictions.append(model.generate(\n",
    "                input_tokens,\n",
    "                max_length=input_tokens.size()[1] + 5,\n",
    "                **model_kwargs\n",
    "            ))\n",
    "    else:\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        # Create group of tokens (if there are too many tokens, GPU may not have enough memory)\n",
    "        groups_of_tokens = [\n",
    "            tokenizer(input_texts[i*parallel_group_size: (i+1)*parallel_group_size], return_tensors='pt', padding=True, padding_side='left')\n",
    "            for i in range(len(input_texts) // parallel_group_size)\n",
    "        ]\n",
    "        predictions = []\n",
    "        for tokens in tqdm.tqdm(groups_of_tokens, desc='Running Superglue ptPT Benchmark - Task BoolQ'):\n",
    "            token_inputs, attention_mask = tokens['input_ids'].to(DEVICE),tokens['attention_mask'].to(DEVICE)\n",
    "            predictions.extend(model.generate(\n",
    "                token_inputs,\n",
    "                attention_mask = attention_mask,\n",
    "                max_length = token_inputs.shape[1] + 5,  # Generate 5 aditional tokens\n",
    "                pad_token_id = tokenizer.eos_token_id\n",
    "            ))\n",
    "            # Clearing GPU memory\n",
    "            del token_inputs, attention_mask\n",
    "\n",
    "    # After obtaining predictions start the evaluation process\n",
    "    for data, input_text, prediction in zip(dataset, input_texts, predictions):\n",
    "        # Retrieve the actual answer from the prediction\n",
    "        prediction = tokenizer.decode(prediction, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        predicted_answer = get_first_word(input_text, prediction)\n",
    "        if not predicted_answer in ['1', '0']:\n",
    "            predicted_answer = '-1' if predicted_answer is None else '1' if predicted_answer.lower() == 'sim' else '0'\n",
    "        benchmark_output.append({\n",
    "            'idx': data['idx'], 'input_text': input_text, 'prediction_text': prediction, 'prediction_label': predicted_answer, 'correct_label': data['label']\n",
    "        })\n",
    "    accurate_preds = sum(ben['prediction_label'].strip()[:1] == str(ben['correct_label']) for ben in benchmark_output)\n",
    "    return {\n",
    "        'benchmark': 'Superglue pt-PT: Task BoolQ',\n",
    "        'accuracy': accurate_preds / len(dataset),\n",
    "        'accurate_predictions': accurate_preds,\n",
    "        'wrong_predictions': len(dataset) - accurate_preds,\n",
    "        'benchmark_predictions': benchmark_output,\n",
    "        'model': MODEL\n",
    "    }\n",
    "BENCHMARKS['SuperGLUE-PTPT: Task BoolQ'] = {\n",
    "    'data': superglue_boolq_df.to_dict('records'),\n",
    "    'data-dataframe': superglue_boolq_df,\n",
    "    'evaluation': benchmark_task_boolq,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. CB\n",
    "Commitment Bank task, consist of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading GLUE PTPT dataset onto a Pandas DataFrame\n",
    "# superglue_ptpt_df = []\n",
    "# for task in [\n",
    "#     'axb_pt-PT',\n",
    "#     'axg_pt-PT',\n",
    "#     'boolq_pt-PT',  # BoolQ (Boolean Questions) QA task where the goal is to determine whether a given question is true or false based on a given passage.\n",
    "#     'cb_pt-PT',     # CB (CommitmentBank) CommitmentBank is a dataset of 1,000 sentences from the Wall Street Journal annotated with a commitment rating.\n",
    "#     'copa_pt-PT',   # COPA (Choice of Plausible Alternatives) QA task where the goal is to select the most plausible alternative to a given premise.\n",
    "#     'mnli_matched_pt-PT',\n",
    "#     'mnli_mismatched_pt-PT',\n",
    "#     'mrpc_pt-PT',\n",
    "#     'multirc_pt-PT',    #  MultiRC (Multi-Sentence Reading Comprehension) QA task where the goal is to read a passage and answer multiple-choice questions about it.\n",
    "#     'qnli_pt-PT',\n",
    "#     'rte_pt-PT',\n",
    "#     'sst2_pt-PT',\n",
    "#     'stsb_pt-PT',\n",
    "#     'wnli_pt-PT'\n",
    "# ]:\n",
    "#     ds = load_dataset(\"PORTULAN/extraglue\", data_dir='data/{}'.format(task), num_proc=5)\n",
    "#     tmp = []\n",
    "#     for dataset_type in ['train', 'validation', 'test']:\n",
    "#         if ds.get(dataset_type, None) is None: continue\n",
    "#         tmp.append(ds[dataset_type].to_pandas())\n",
    "#         tmp[-1]['Dataset Type'] = dataset_type\n",
    "#     superglue_ptpt_df.append(pd.concat(tmp).reset_index(drop=True))\n",
    "#     superglue_ptpt_df[-1]['Task Name'] = task\n",
    "# superglue_ptpt_df = pd.concat(superglue_ptpt_df).reset_index(drop=True)\n",
    "# superglue_ptpt_df = superglue_ptpt_df[['Task Name', 'Dataset Type'] + superglue_ptpt_df.columns.difference(['Task Name', 'Dataset Type']).tolist()]\n",
    "\n",
    "# # Defining the Benchmark function for CALAME-PT\n",
    "# def benchmark_superglueptpt(\n",
    "#     model,\n",
    "#     tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "#     dataset: list[dict[str, str]],\n",
    "#     model_kwargs = {\n",
    "#         'do_sample': False,\n",
    "#         'temperature': None,\n",
    "#         'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "#         # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "#         # 'return_dict_in_generate': True,\n",
    "#         'return_legacy_cache': False\n",
    "#     },\n",
    "#     parallel = False\n",
    "# ):\n",
    "#     return None\n",
    "\n",
    "\n",
    "# # Adding it onto benchmarks dictionary\n",
    "# BENCHMARKS['SuperGLUE-PTPT'] =  {\n",
    "#     'data': superglue_ptpt_df.to_dict('records'),\n",
    "#     'data-dataframe': superglue_ptpt_df,\n",
    "#     'evaluation': benchmark_superglueptpt,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE PT-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading GLUE PTPT dataset onto a Pandas DataFrame\n",
    "# glue_ptpt_df = []\n",
    "# # Iterate through 4 different GLUE taks (Read more here: https://openreview.net/pdf?id=rJ4km2R5t7)\n",
    "# for task in [\n",
    "#     \"mrpc\",     # MRPC - The Microsoft Research Paraphrase Corpus   (Similarity and Paraphrase Task)\n",
    "#     \"stsb\",     # STS-B - Semantic Textual Similarity Benchmark     (Similarity and Paraphrase Task)\n",
    "#     \"rte\",      # RTE - Recognizing Textual Entailment              (Inference Task)\n",
    "#     \"wnli\"      # WNLI - Winograd Schema Challenge                  (Inference Task)\n",
    "# ]:\n",
    "#     ds = load_dataset(\"PORTULAN/extraglue\", task)\n",
    "#     # Transform datasets to pandas\n",
    "#     tmp = []\n",
    "#     for dataset_type in ['train', 'validation', 'test']:\n",
    "#         tmp.append(ds[dataset_type].to_pandas())\n",
    "#         tmp[-1]['Dataset Name'] = task\n",
    "#         tmp[-1]['Dataset Type'] = dataset_type\n",
    "#     glue_ptpt_df.append(pd.concat(tmp).reset_index(drop=True))\n",
    "# glue_ptpt_df = pd.concat(glue_ptpt_df).reset_index(drop=True)\n",
    "\n",
    "# # Defining the Benchmark function for GLUE PTPT\n",
    "# def benchmark_glueptpt(\n",
    "#     model,\n",
    "#     tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "#     dataset: list[dict[str, str]],\n",
    "#     model_kwargs = {\n",
    "#         'do_sample': False,\n",
    "#         'temperature': None,\n",
    "#         'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "#         # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "#         # 'return_dict_in_generate': True,\n",
    "#         'return_legacy_cache': False\n",
    "#     },\n",
    "#     parallel = False\n",
    "# ):\n",
    "#     # NOTE: We could improve this benchmark by using the PROBABILITY Distribution of the actual token.\n",
    "#     benchmark_output = {}\n",
    "#     if not parallel:\n",
    "#         for data in tqdm.tqdm(dataset, desc='Running CALAME-PT Benchmark'):\n",
    "#             # Retrieving data from the dictionary\n",
    "#             n, predicted_text, correct_word = data['id'], data['sentence'], data['last_word']\n",
    "\n",
    "#             input_tokens = tokenizer.encode(predicted_text, return_tensors=\"pt\").to(DEVICE)\n",
    "#             prediction = model.generate(\n",
    "#                 input_tokens,\n",
    "#                 max_length=input_tokens.size()[1] + 5,\n",
    "#                 **model_kwargs\n",
    "#             )\n",
    "#             prediction = tokenizer.decode(prediction[0])\n",
    "#             predicted_word = get_first_word(predicted_text, prediction)\n",
    "#             benchmark_output[n] = {'text': predicted_text, 'prediction': predicted_word, 'correct_word': correct_word}\n",
    "#     else:\n",
    "#         input_tokens = [d['sentence'] for d in dataset]\n",
    "#         max_length = max(len(d) for d in input_tokens) + 5\n",
    "#         input_tokens = tokenizer.encode(input_tokens, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(DEVICE)\n",
    "#         predictions = model.generate(\n",
    "#             input_tokens,\n",
    "#             max_length=max_length,\n",
    "#             **model_kwargs\n",
    "#         )\n",
    "#         for n, pred in enumerate(predictions):\n",
    "#             predicted_text = tokenizer.decode(pred[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "#             predicted_word = get_first_word(dataset[n]['sentence'], predicted_text)\n",
    "#             benchmark_output[n] = {'text': dataset[n]['sentence'], 'prediction': predicted_word, 'correct_word': dataset[n]['last_word']}\n",
    "\n",
    "#     accurate_preds = sum(ben['prediction'] == ben['correct_word'] for ben in benchmark_output.values())\n",
    "#     return {\n",
    "#         'benchmark': 'CALAME-PT',\n",
    "#         'accuracy': accurate_preds / len(BENCHMARKS['CALAME-PT']['data']),\n",
    "#         'accurate_predictions': accurate_preds,\n",
    "#         'wrong_predictions': len(BENCHMARKS['CALAME-PT']['data']) - accurate_preds,\n",
    "#         'benchmark_predictions': benchmark_output,\n",
    "#         'model': MODEL\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Adding it onto benchmarks dictionary\n",
    "# BENCHMARKS['GLUE-PTPT'] =  {\n",
    "#     'data': glue_ptpt_df.to_dict('records'),\n",
    "#     'data-pandas': glue_ptpt_df,\n",
    "#     'evaluation': benchmark_glueptpt,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity & Fertility\n",
    "\n",
    "We will use ALL datasets to calculate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(arr):\n",
    "    return arr.exp() / arr.exp().sum()\n",
    "\n",
    "def real_likelihood(pred_dist, real_token):\n",
    "    pred_dist = soft_max(pred_dist)\n",
    "    dist_real_token = pred_dist[0, real_token]\n",
    "    return dist_real_token.log()\n",
    "\n",
    "def perplexity(input_tokens, model) -> float:\n",
    "    perplex = 0\n",
    "    for n in range(1, input_tokens.shape[0]):\n",
    "        test_tokens = input_tokens[:n].reshape(1, n)\n",
    "        real_token = input_tokens[n]\n",
    "        predicted_distribution = model.generate(\n",
    "            test_tokens,\n",
    "            max_length=n+1,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id = tokenizer.eos_token_id\n",
    "        )['scores'][0]\n",
    "        perplex += real_likelihood(predicted_distribution, real_token)\n",
    "    return torch.exp(-perplex / input_tokens.shape[0]).to('cpu').item()\n",
    "\n",
    "def compute_perplexities(model, tokens, text: list[str], parallel_group_size=10) -> list[float]:\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    # Create batch of tokens (if there are too many tokens, GPU may not have enough memory)\n",
    "    perplexities = []\n",
    "    for tks in tqdm.tqdm(tokens['input_ids'], 'Computing Perplexity'):\n",
    "        perplexities.append(perplexity(tks, model))\n",
    "    return perplexities\n",
    "\n",
    "def compute_fertilities(tokenizer, tokens, text:  list[str]) -> list[float]:\n",
    "    word_counts = [len(re.findall('\\w+', t)) for t in text]\n",
    "    tokens_without_padding = [\n",
    "        [token for token in tks if token not in (tokenizer.eos_token, tokenizer.pad_token)]\n",
    "        for tks in tokens['input_ids'].tolist()\n",
    "    ]\n",
    "    fertilities = [len(tks) / n_words if n_words > 0 else None for tks, n_words in zip (tokens_without_padding, word_counts)]\n",
    "    return fertilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Add pad_token to generate and tokenize everything in parallel\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "for benchmark in tqdm.tqdm(list(BENCHMARKS.keys()), desc='Calculating Perplexity & Fertility for all benchmarks'):\n",
    "    df: pd.DataFrame = BENCHMARKS[benchmark]['data-dataframe']\n",
    "    perplexity_cols, fertility_cols = [], []\n",
    "    for col in df.select_dtypes('object').columns:\n",
    "        perplexity_cols.append(f'PERPLEXITY[{col}]')\n",
    "        fertility_cols.append(f'FERTILITY[{col}]')\n",
    "        tokens = tokenizer(df[col].tolist(), return_tensors='pt', padding=True)\n",
    "        df[fertility_cols[-1]] = compute_fertilities(tokenizer, tokens, df[col].tolist())\n",
    "        # tokens = tokens.to(DEVICE)\n",
    "        # df[perplexity_cols[-1]] = compute_perplexities(model, tokens, df[col].tolist())\n",
    "    BENCHMARKS[benchmark]['data-dataframe'] = df\n",
    "    # BENCHMARKS[benchmark]['Perplexity'] = df[perplexity_cols].sum().sum() / (df[perplexity_cols].shape[1] * df.shape[0])\n",
    "    BENCHMARKS[benchmark]['Fertility'] = df[fertility_cols].sum().sum() / (df[fertility_cols].shape[1] * df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for benchmark, benchmark_info in BENCHMARKS.items():\n",
    "    results.append(benchmark_info['evaluation'](model, tokenizer, benchmark_info['data']))\n",
    "\n",
    "\n",
    "# Saving the results in a pikcle file to fetch them later\n",
    "with open('benchmark_results.pkl', 'wb') as f: pickle.dump(results, f)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.pivot(index='model', columns='benchmark', values='accuracy').reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

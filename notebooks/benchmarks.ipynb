{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(arr):\n",
    "    return arr.exp() / arr.exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# GlorIA-1.3B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'NOVA-vision-language/GlorIA-1.3B',\n",
    "    use_safetensors=True\n",
    ").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained('NOVA-vision-language/GlorIA-1.3B')\n",
    "input_tokens = tokenizer.encode(\"O meu nome Ã©\", return_tensors=\"pt\").to('cuda')\n",
    "output = model.generate(\n",
    "    input_tokens,\n",
    "    max_length=100,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    return_legacy_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    input_tokens,\n",
    "    max_length=input_tokens.shape[1]+50,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    return_legacy_cache=False\n",
    ")\n",
    "scores = output[1]\n",
    "scores = list(map(soft_max, scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_ids = [score.argmax().item() for score in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3313, 11, 3606, 436, 565, 258, 3515, 9854, 261, 17400, 261, 11798, 13, 198, 198, 198, 198, 46, 2651, 3131, 261, 1008, 261, 3565, 364, 302, 2411, 261, 6085, 11, 287, 1290, 4436, 327, 2193, 754, 2618, 261, 4680, 11, 348, 20154, 3714, 298, 1257, 13, 198, 198, 198, 198]\n"
     ]
    }
   ],
   "source": [
    "print(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   46,  2651,  1007,   364,  3313,    11,  3606,   436,   565,   258,\n",
       "          3515,  9854,   261, 17400,   261, 11798,    13,   198,   198,   198,\n",
       "           198,    46,  2651,  3131,   261,  1008,   261,  3565,   364,   302,\n",
       "          2411,   261,  6085,    11,   287,  1290,  4436,   327,  2193,   754,\n",
       "          2618,   261,  4680,    11,   348, 20154,  3714,   298,  1257,    13,\n",
       "           198,   198,   198,   198]], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3313"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   46,  2651,  1007,   364,  3313,    11,  3606,   436,   565,   258,\n",
       "          3515,  9854,   261, 17400,   261, 11798,    13,   198,   198,   198,\n",
       "           198,    46,  2651,  3131,   261,  1008,   261,  3565,   364,   302,\n",
       "          2411,   261,  6085,    11,   287,  1290,  4436,   327,  2193,   754,\n",
       "          2618,   261,  4680,    11,   348, 20154,  3714,   298,  1257,    13,\n",
       "           198,   198,   198,   198]], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[109.9425, 109.4950, 105.2128,  ..., 106.0118, 101.8323,  31.0431]],\n",
       "        device='cuda:0'),)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CALAME-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALAME-PT\n",
    "import pandas as pd\n",
    "\n",
    "df_handwritten = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_handwritten_only.jsonl\", lines=True)\n",
    "df_handwritten['Source'] = 'Handwritten'\n",
    "\n",
    "df_generated = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_gen_only.jsonl\", lines=True)\n",
    "df_generated['Source'] = 'Generated'\n",
    "\n",
    "df = pd.concat([df_handwritten, df_generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

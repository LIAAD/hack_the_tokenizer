{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore some benchmarks for different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from typing import Any, Iterable, Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS: dict[str, dict[str, Iterable[Any] | Callable]] = {}\n",
    "'''\n",
    "    Should be a dictionary with the following as entries:\n",
    "    {\n",
    "        'data': Iterable[Any],\n",
    "        'evaluation': Callable[Any, PreTrainedTokenizer | PreTrainedTokenizerFast, TypeOfData]  -> This function will be called with the \"data\" value\n",
    "    }\n",
    "'''\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "MODEL = 'NOVA-vision-language/GlorIA-1.3B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Max\n",
    "def soft_max(arr): return arr.exp() / arr.exp().sum()\n",
    "\n",
    "def get_first_word(original_text: str, predicted_text: str):\n",
    "    predicted_text = predicted_text.replace(original_text, '')\n",
    "    # Regex to find first word\n",
    "    first_word = re.search(r'\\b\\w+\\b', predicted_text)\n",
    "    first_word = first_word.group() if first_word else None\n",
    "    return first_word\n",
    "\n",
    "def load_dataset_to_dataframe(*args, data_dir=None, dataset_types=['train', 'validation', 'test'], **kwargs):\n",
    "    ds = load_dataset(*args, data_dir=data_dir, **kwargs)\n",
    "    output = []\n",
    "    for ds_type in dataset_types:\n",
    "        output.append(ds[ds_type].to_pandas())\n",
    "        output[-1]['Dataset Type'] = ds_type\n",
    "    return pd.concat(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlorIA-1.3B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    use_safetensors=True\n",
    ").to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Benchmark Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CALAME-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CALAME-PT dataset onto a Pandas DataFrame\n",
    "df_handwritten = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_handwritten_only.jsonl\", lines=True)\n",
    "df_handwritten['Source'] = 'Handwritten'\n",
    "df_generated = pd.read_json(\"hf://datasets/NOVA-vision-language/calame-pt/calamept_gen_only.jsonl\", lines=True)\n",
    "df_generated['Source'] = 'Generated'\n",
    "calame_pt_df = pd.concat([df_handwritten, df_generated])[['id', 'sentence', 'last_word']]\n",
    "\n",
    "# Defining the Benchmark function for CALAME-PT\n",
    "def benchmark_calamept(\n",
    "    model,\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    dataset: list[dict[str, str]],\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    },\n",
    "    parallel = False\n",
    "):\n",
    "    # NOTE: We could improve this benchmark by using the PROBABILITY Distribution of the actual token.\n",
    "    benchmark_output = {}\n",
    "    if not parallel:\n",
    "        for data in tqdm.tqdm(dataset, desc='Running CALAME-PT Benchmark'):\n",
    "            # Retrieving data from the dictionary\n",
    "            n, predicted_text, correct_word = data['id'], data['sentence'], data['last_word']\n",
    "\n",
    "            input_tokens = tokenizer.encode(predicted_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            prediction = model.generate(\n",
    "                input_tokens,\n",
    "                max_length=input_tokens.size()[1] + 5,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            prediction = tokenizer.decode(prediction[0])\n",
    "            predicted_word = get_first_word(predicted_text, prediction)\n",
    "            benchmark_output[n] = {'text': predicted_text, 'prediction': predicted_word, 'correct_word': correct_word}\n",
    "    else:\n",
    "        input_tokens = [d['sentence'] for d in dataset]\n",
    "        max_length = max(len(d) for d in input_tokens) + 5\n",
    "        input_tokens = tokenizer.encode(input_tokens, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(DEVICE)\n",
    "        predictions = model.generate(\n",
    "            input_tokens,\n",
    "            max_length=max_length,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        for n, pred in enumerate(predictions):\n",
    "            predicted_text = tokenizer.decode(pred[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            predicted_word = get_first_word(dataset[n]['sentence'], predicted_text)\n",
    "            benchmark_output[n] = {'text': dataset[n]['sentence'], 'prediction': predicted_word, 'correct_word': dataset[n]['last_word']}\n",
    "\n",
    "    accurate_preds = sum(ben['prediction'] == ben['correct_word'] for ben in benchmark_output.values())\n",
    "    return {\n",
    "        'benchmark': 'CALAME-PT',\n",
    "        'accuracy': accurate_preds / len(BENCHMARKS['CALAME-PT']['data']),\n",
    "        'accurate_predictions': accurate_preds,\n",
    "        'wrong_predictions': len(BENCHMARKS['CALAME-PT']['data']) - accurate_preds,\n",
    "        'benchmark_predictions': benchmark_output,\n",
    "        'model': MODEL\n",
    "    }\n",
    "\n",
    "# Adding it onto benchmarks dictionary\n",
    "BENCHMARKS['CALAME-PT'] =  {\n",
    "    'data': calame_pt_df.to_dict('records'),\n",
    "    'data-dataframe': calame_pt_df,\n",
    "    'evaluation': benchmark_calamept,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPERGLUE PT-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "superglue_benchmarks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. BoolQ\n",
    "\n",
    "Boolean Question task, consists of determining wether a given question is true or false based on a given passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "superglue_boolq_df = load_dataset_to_dataframe('PORTULAN/extraglue', data_dir='data/boolq_pt-PT')\n",
    "\n",
    "def benchmark_task_boolq(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    }\n",
    "):\n",
    "    benchmark_output = []\n",
    "    for data in tqdm.tqdm(dataset, desc='Running Superglue ptPT Benchamrk - Task BoolQ'):\n",
    "        # Retrieving data from the dictionary\n",
    "        idx, passage, question, correct_label = data['idx'], data['passage'], data['question'], data['label']\n",
    "\n",
    "        input_text = f'Passagem: {passage}\\nPergunta: {question}\\nResposta (0-Verdade, 1-Mentira):'\n",
    "\n",
    "        input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "        prediction = model.generate(\n",
    "            input_tokens,\n",
    "            max_length=input_tokens.size()[1] + 5,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        prediction = tokenizer.decode(prediction[0])\n",
    "        predicted_answer = get_first_word(input_text, prediction)\n",
    "        if not predicted_answer in ['1', '0']:\n",
    "            predicted_answer = '-1' if predicted_answer is None else '1' if predicted_answer.lower() == 'sim' else '0'\n",
    "        benchmark_output.append({\n",
    "            'idx': idx, 'input_text': input_text, 'prediction_answer': predicted_answer, 'correct_label': correct_label\n",
    "        })\n",
    "\n",
    "    accurate_preds = sum(ben['prediction_answer'].strip()[0] == ben['correct_label'] for ben in benchmark_output.values())\n",
    "    return {\n",
    "        'benchmark': 'Superglue pt-PT: Task BoolQ',\n",
    "        'accuracy': accurate_preds / len(dataset),\n",
    "        'accurate_predictions': accurate_preds,\n",
    "        'wrong_predictions': len(dataset) - accurate_preds,\n",
    "        'benchmark_predictions': benchmark_output,\n",
    "        'model': MODEL\n",
    "    }\n",
    "\n",
    "superglue_benchmarks['BoolQ'] = {\n",
    "    'data': superglue_boolq_df.to_dict('records'),\n",
    "    'data-dataframe': superglue_boolq_df,\n",
    "    'evaluation': benchmark_task_boolq,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Superglue ptPT Benchamrk - Task BoolQ:   0%|          | 0/15942 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Superglue ptPT Benchamrk - Task BoolQ:   1%|          | 111/15942 [00:27<1:05:04,  4.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m boolQ \u001b[38;5;241m=\u001b[39m superglue_benchmarks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoolQ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43mboolQ\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevaluation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboolQ\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mbenchmark_task_boolq\u001b[1;34m(model, tokenizer, dataset, model_kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassagem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPergunta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResposta (0-Verdade, 1-Mentira):\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 24\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     25\u001b[0m     input_tokens,\n\u001b[0;32m     26\u001b[0m     max_length\u001b[38;5;241m=\u001b[39minput_tokens\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m prediction \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(prediction[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     30\u001b[0m predicted_answer \u001b[38;5;241m=\u001b[39m get_first_word(input_text, prediction)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\generation\\utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2044\u001b[0m     )\n\u001b[0;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2048\u001b[0m         input_ids,\n\u001b[0;32m   2049\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2050\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2051\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2052\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2053\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2054\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2055\u001b[0m     )\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2067\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\generation\\utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:1043\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1043\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1059\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:806\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    795\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    796\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    797\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    803\u001b[0m         cache_position,\n\u001b[0;32m    804\u001b[0m     )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:513\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    511\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    512\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 513\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    523\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:463\u001b[0m, in \u001b[0;36mGPTNeoAttention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    455\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    462\u001b[0m ):\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:315\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    312\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[0;32m    313\u001b[0m     key, value \u001b[38;5;241m=\u001b[39m layer_past\u001b[38;5;241m.\u001b[39mupdate(key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id, cache_kwargs)\n\u001b[1;32m--> 315\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    318\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\yakim\\Documents\\MEGA\\03. Vida Académica\\03. Mestrado Ciencias Computadores\\Dissertacao\\.conda\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:266\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    263\u001b[0m query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    264\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 266\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Apply sliding window masking for local attention layers\u001b[39;00m\n\u001b[0;32m    269\u001b[0m query_length, key_length \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), key\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "boolQ = superglue_benchmarks['BoolQ']\n",
    "\n",
    "boolQ['evaluation'](model, tokenizer, boolQ['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. CB\n",
    "\n",
    "Commitment Bank task, consist of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GLUE PTPT dataset onto a Pandas DataFrame\n",
    "superglue_ptpt_df = []\n",
    "for task in [\n",
    "    'axb_pt-PT',\n",
    "    'axg_pt-PT',\n",
    "    'boolq_pt-PT',  # BoolQ (Boolean Questions) QA task where the goal is to determine whether a given question is true or false based on a given passage.\n",
    "    'cb_pt-PT',     # CB (CommitmentBank) CommitmentBank is a dataset of 1,000 sentences from the Wall Street Journal annotated with a commitment rating.\n",
    "    'copa_pt-PT',   # COPA (Choice of Plausible Alternatives) QA task where the goal is to select the most plausible alternative to a given premise.\n",
    "    'mnli_matched_pt-PT',\n",
    "    'mnli_mismatched_pt-PT',\n",
    "    'mrpc_pt-PT',\n",
    "    'multirc_pt-PT',    #  MultiRC (Multi-Sentence Reading Comprehension) QA task where the goal is to read a passage and answer multiple-choice questions about it.\n",
    "    'qnli_pt-PT',\n",
    "    'rte_pt-PT',\n",
    "    'sst2_pt-PT',\n",
    "    'stsb_pt-PT',\n",
    "    'wnli_pt-PT'\n",
    "]:\n",
    "    ds = load_dataset(\"PORTULAN/extraglue\", data_dir='data/{}'.format(task), num_proc=5)\n",
    "    tmp = []\n",
    "    for dataset_type in ['train', 'validation', 'test']:\n",
    "        if ds.get(dataset_type, None) is None: continue\n",
    "        tmp.append(ds[dataset_type].to_pandas())\n",
    "        tmp[-1]['Dataset Type'] = dataset_type\n",
    "    superglue_ptpt_df.append(pd.concat(tmp).reset_index(drop=True))\n",
    "    superglue_ptpt_df[-1]['Task Name'] = task\n",
    "superglue_ptpt_df = pd.concat(superglue_ptpt_df).reset_index(drop=True)\n",
    "superglue_ptpt_df = superglue_ptpt_df[['Task Name', 'Dataset Type'] + superglue_ptpt_df.columns.difference(['Task Name', 'Dataset Type']).tolist()]\n",
    "\n",
    "# Defining the Benchmark function for CALAME-PT\n",
    "def benchmark_superglueptpt(\n",
    "    model,\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    dataset: list[dict[str, str]],\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    },\n",
    "    parallel = False\n",
    "):\n",
    "    return None\n",
    "\n",
    "\n",
    "# Adding it onto benchmarks dictionary\n",
    "BENCHMARKS['SuperGLUE-PTPT'] =  {\n",
    "    'data': superglue_ptpt_df.to_dict('records'),\n",
    "    'data-dataframe': superglue_ptpt_df,\n",
    "    'evaluation': benchmark_superglueptpt,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE PT-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GLUE PTPT dataset onto a Pandas DataFrame\n",
    "glue_ptpt_df = []\n",
    "# Iterate through 4 different GLUE taks (Read more here: https://openreview.net/pdf?id=rJ4km2R5t7)\n",
    "for task in [\n",
    "    \"mrpc\",     # MRPC - The Microsoft Research Paraphrase Corpus   (Similarity and Paraphrase Task)\n",
    "    \"stsb\",     # STS-B - Semantic Textual Similarity Benchmark     (Similarity and Paraphrase Task)\n",
    "    \"rte\",      # RTE - Recognizing Textual Entailment              (Inference Task)\n",
    "    \"wnli\"      # WNLI - Winograd Schema Challenge                  (Inference Task)\n",
    "]:\n",
    "    ds = load_dataset(\"PORTULAN/extraglue\", task)\n",
    "    # Transform datasets to pandas\n",
    "    tmp = []\n",
    "    for dataset_type in ['train', 'validation', 'test']:\n",
    "        tmp.append(ds[dataset_type].to_pandas())\n",
    "        tmp[-1]['Dataset Name'] = task\n",
    "        tmp[-1]['Dataset Type'] = dataset_type\n",
    "    glue_ptpt_df.append(pd.concat(tmp).reset_index(drop=True))\n",
    "glue_ptpt_df = pd.concat(glue_ptpt_df).reset_index(drop=True)\n",
    "\n",
    "# Defining the Benchmark function for GLUE PTPT\n",
    "def benchmark_glueptpt(\n",
    "    model,\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    "    dataset: list[dict[str, str]],\n",
    "    model_kwargs = {\n",
    "        'do_sample': False,\n",
    "        'temperature': None,\n",
    "        'pad_token_id': 50257, # This is the tokenizer.eos_token_id\n",
    "        # 'output_scores': True,            # INCLUDE LATER to get the probability distribution and use that as a benchmark\n",
    "        # 'return_dict_in_generate': True,\n",
    "        'return_legacy_cache': False\n",
    "    },\n",
    "    parallel = False\n",
    "):\n",
    "    # NOTE: We could improve this benchmark by using the PROBABILITY Distribution of the actual token.\n",
    "    benchmark_output = {}\n",
    "    if not parallel:\n",
    "        for data in tqdm.tqdm(dataset, desc='Running CALAME-PT Benchmark'):\n",
    "            # Retrieving data from the dictionary\n",
    "            n, predicted_text, correct_word = data['id'], data['sentence'], data['last_word']\n",
    "\n",
    "            input_tokens = tokenizer.encode(predicted_text, return_tensors=\"pt\").to(DEVICE)\n",
    "            prediction = model.generate(\n",
    "                input_tokens,\n",
    "                max_length=input_tokens.size()[1] + 5,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            prediction = tokenizer.decode(prediction[0])\n",
    "            predicted_word = get_first_word(predicted_text, prediction)\n",
    "            benchmark_output[n] = {'text': predicted_text, 'prediction': predicted_word, 'correct_word': correct_word}\n",
    "    else:\n",
    "        input_tokens = [d['sentence'] for d in dataset]\n",
    "        max_length = max(len(d) for d in input_tokens) + 5\n",
    "        input_tokens = tokenizer.encode(input_tokens, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(DEVICE)\n",
    "        predictions = model.generate(\n",
    "            input_tokens,\n",
    "            max_length=max_length,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        for n, pred in enumerate(predictions):\n",
    "            predicted_text = tokenizer.decode(pred[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            predicted_word = get_first_word(dataset[n]['sentence'], predicted_text)\n",
    "            benchmark_output[n] = {'text': dataset[n]['sentence'], 'prediction': predicted_word, 'correct_word': dataset[n]['last_word']}\n",
    "\n",
    "    accurate_preds = sum(ben['prediction'] == ben['correct_word'] for ben in benchmark_output.values())\n",
    "    return {\n",
    "        'benchmark': 'CALAME-PT',\n",
    "        'accuracy': accurate_preds / len(BENCHMARKS['CALAME-PT']['data']),\n",
    "        'accurate_predictions': accurate_preds,\n",
    "        'wrong_predictions': len(BENCHMARKS['CALAME-PT']['data']) - accurate_preds,\n",
    "        'benchmark_predictions': benchmark_output,\n",
    "        'model': MODEL\n",
    "    }\n",
    "\n",
    "\n",
    "# Adding it onto benchmarks dictionary\n",
    "BENCHMARKS['GLUE-PTPT'] =  {\n",
    "    'data': glue_ptpt_df.to_dict('records'),\n",
    "    'data-pandas': glue_ptpt_df,\n",
    "    'evaluation': benchmark_glueptpt,    # Should be a function that receives 3 args: model, tokenizer and dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "\n",
    "We will use ALL datasets to calculate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for benchmark_info in BENCHMARKS.values():\n",
    "    benchmark_info['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for benchmark, benchmark_info in BENCHMARKS.items():\n",
    "    results.append(benchmark_info['evaluation'](model, tokenizer, benchmark_info['data']))\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

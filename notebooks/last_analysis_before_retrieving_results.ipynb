{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08318759",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Calculating benchmarks results for a bigger model (Qwen2.5-1.5B) before and after adding the new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3ba48",
   "metadata": {},
   "source": [
    "Initial imports and variable initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affd098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import pathlib\n",
    "import tqdm\n",
    "import os\n",
    "import re\n",
    "os.chdir('/home/yali/MEGA/Hack The Tockenizer/tests')\n",
    "sys.path.insert(1, pathlib.Path('..').resolve().as_posix())\n",
    "from src import utils, loader, hack\n",
    "from src.DatasetClass import ListDataset, TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "DEVICE                  = 'cuda'\n",
    "GENERATION_BATCH_SIZE   = 8\n",
    "MODEL                   = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "MODEL_GEN_KWARGS = dict(top_p=None, top_k=None, temperature=None, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f3141",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3e80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loader.load_model_and_tokenizer(\n",
    "    model_name=MODEL,\n",
    "    device=DEVICE,\n",
    "    model_kwargs = { 'torch_dtype': torch.bfloat16},\n",
    "    tokenizer_kwargs={'padding_side': 'left'}\n",
    ")\n",
    "original_tokenizer = loader.load_model_and_tokenizer(\n",
    "    model_name=MODEL,\n",
    "    device='cpu',\n",
    "    model_kwargs = { 'torch_dtype': torch.bfloat16},\n",
    "    tokenizer_kwargs={'padding_side': 'left'}\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985d183",
   "metadata": {},
   "source": [
    "# CalamePT Benchmark\n",
    "\n",
    "Considering **ONLY** CalamePT, currently not considering the SuperGluePTPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.benchmark as Benchmark\n",
    "from src.benchmark.CalamePT import CalamePT\n",
    "\n",
    "# Removing \"SuperGluePTPT\" from the Benchmarks\n",
    "benchmark = Benchmark.Benchmarks([CalamePT()])\n",
    "\n",
    "# Adding the Batch Size (to generate in parallel)\n",
    "benchmark.config['parallel_batch_size'] = GENERATION_BATCH_SIZE\n",
    "benchmark.config['max_new_tokens']      = max(len(tokenizer.encode(x)) for x in CalamePT().df['last_word'].values) + 1   # Maximum tokenization of predicted words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b2e69",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5bb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<Qwen/Qwen2.5-1.5B-Instruct> Calculating inferences for inputs: 100%|██████████| 260/260 [01:29<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`CalamePT` Accuracy for Baseline Model `Qwen/Qwen2.5-1.5B-Instruct` = 49.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_results = benchmark.run(model, tokenizer, generation_kwargs=MODEL_GEN_KWARGS, store_generation_data=False)\n",
    "print(f\"`CalamePT` Accuracy for Baseline Model `{model.name_or_path}` = {benchmark_results['CalamePT']['result']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0059b1",
   "metadata": {},
   "source": [
    "## Model with additional Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84468a91",
   "metadata": {},
   "source": [
    "### Adding new tokens to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9e40",
   "metadata": {},
   "source": [
    "1. Fetching the new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 3572/5040 [00:32<00:13, 107.15it/s]"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "#           Train Tokenizer with PT Dataset               \n",
    "# ----------------------------------------------------\n",
    "# Step 1. Train a new portuguese vocabulary\n",
    "# TODO: Find a way to fix the training... Maybe use numpy random to set a seed?\n",
    "#   TODO: Verify that the `new_tokens` list is always the same (ignoring order)\n",
    "pt_tokenizer = hack.TokenizerHack(device=DEVICE).train_tokenizer(trainer_kwargs={'vocab_size': 10000})\n",
    "\n",
    "# Step 2. Find tokens in `pt_tokenizer` not in \n",
    "new_tokens = set(pt_tokenizer.get_vocab().keys())\n",
    "new_tokens = new_tokens.difference(set(tokenizer.vocab.keys()))\n",
    "\n",
    "# Removing the 'Ġ' tokens and fixing maybe some others\n",
    "new_tokens = set([tokenizer.decoder.decode([new_token]) for new_token in new_tokens])\n",
    "\n",
    "# Remove the tokens which may be \"contained\" in any of the original tokens (for instance, \"publ\" is contained in \"publico\" so \"publ\" will be removed)\n",
    "__new_tokens = []\n",
    "tokenizer_vocab_keys = list(tokenizer.decode(x) for x in range(len(tokenizer)))\n",
    "for new_token in tqdm.tqdm(new_tokens, total=len(new_tokens)):\n",
    "    add_new_token = True\n",
    "    for token in tokenizer_vocab_keys:\n",
    "        if token.startswith(new_token):\n",
    "            add_new_token = False\n",
    "            break\n",
    "    if add_new_token: __new_tokens.append(new_token)\n",
    "new_tokens = set(__new_tokens)\n",
    "\n",
    "######### BELLOW SECTION IS THE LAST LOOP BUT RUNNING IN PARALLEL USING joblib\n",
    "# import joblib as jb\n",
    "# import tqdm\n",
    "\n",
    "# def should_add_token(new_token, vocab_keys):\n",
    "#     for token in vocab_keys:\n",
    "#         if token.startswith(new_token):\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# def filter_new_tokens(new_tokens, tokenizer_vocab_keys):\n",
    "#     vocab_keys = list(tokenizer_vocab_keys)  # Avoid repeated conversions in workers\n",
    "    \n",
    "#     # Parallel processing with generator output\n",
    "#     results = jb.Parallel(n_jobs=7, backend=\"loky\", return_as=\"generator\")(\n",
    "#         jb.delayed(should_add_token)(new_token, vocab_keys)\n",
    "#         for new_token in new_tokens\n",
    "#     )\n",
    "    \n",
    "#     # Wrap results in tqdm for progress tracking\n",
    "#     for new_token, should_add in tqdm.tqdm(\n",
    "#         zip(new_tokens, results),\n",
    "#         total=len(new_tokens),\n",
    "#         desc=\"Filtering tokens\"\n",
    "#     ):\n",
    "#         if should_add:\n",
    "#             yield new_token\n",
    "\n",
    "# # Usage:\n",
    "# tokenizer_vocab_keys = list(tokenizer.decode(x) for x in range(len(tokenizer)))\n",
    "# new_tokens_filtered_gen = filter_new_tokens(new_tokens, tokenizer_vocab_keys)\n",
    "# new_tokens = set(new_tokens_filtered_gen)  # Consume the generator and convert to set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35a32b",
   "metadata": {},
   "source": [
    "2. Update the model Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "#               Update Model Vocabulary               \n",
    "# ----------------------------------------------------\n",
    "# Save the original tokenizations\n",
    "original_tokenization = {t: tokenizer.encode(t) for t in new_tokens}    # Necessary for the training bellow\n",
    "tokenizer.add_tokens(list(new_tokens))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92817b",
   "metadata": {},
   "source": [
    "3. Initialize the embeddings using the Weighted average $w_i = w_{i+1} \\times K$ with $K=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Calculate the new embeddings for the new tokens\n",
    "embed = model.get_input_embeddings().weight.clone().to('cpu')\n",
    "new_embed = model.get_input_embeddings()\n",
    "\n",
    "# Initialize the embedding using the weighted average model\n",
    "K = 1.5 # Tested for values [1, 2, 3, 4, 5, 0.9, 0.8, 1.1, 1.2, ..., 1.6] and the best was 1.5 with (3.13%)\n",
    "with torch.no_grad():\n",
    "    for new_token in tqdm.tqdm(new_tokens, desc='Initializing the embeddings for the new_tokens'):\n",
    "        new_token_id = tokenizer.encode(new_token)[0]\n",
    "        # Find the old embedding for the token\n",
    "        tokenization = original_tokenization[new_token]\n",
    "        token_embed = torch.stack([embed[t_id] for t_id in tokenization]).to(DEVICE)\n",
    "        # Calculating the embedding weights\n",
    "        embedding_weights = torch.asarray([K**i if K**i < 2**64 else 0 for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        # embedding_weights = torch.asarray([K**i for i in range(token_embed.shape[0], 0, -1)]).to(DEVICE)\n",
    "        embedding_weights = embedding_weights / embedding_weights.sum()\n",
    "\n",
    "        # Create a new token embed using the weighted average of the embeddings\n",
    "        new_token_embed = torch.sum(token_embed * embedding_weights[:, None], dim=0)\n",
    "        # new_token_embed = token_embed[0]\n",
    "        # Update embedding of the new_token in the hacked_model\n",
    "        _ = new_embed.weight[new_token_id].data.copy_(new_token_embed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc889e",
   "metadata": {},
   "source": [
    "### Benchmark Computation\n",
    "\n",
    "Calculating the results with the added tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f11654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model name\n",
    "model.name_or_path = f'{MODEL}-ADDED_TOKENS_INIT_K={K}'\n",
    "\n",
    "# Update the \"max_new_tokens\" (since we added new_tokens, we may have a different value than previously)\n",
    "benchmark.config['max_new_tokens'] = max(len(tokenizer.encode(x)) for x in CalamePT().df['last_word'].values) + 1   # Maximum tokenization of predicted words\n",
    "benchmark_results_new_tokens = benchmark.run(model, tokenizer, original_tokenizer, generation_kwargs=MODEL_GEN_KWARGS, store_generation_data=False)\n",
    "print(f\"`CalamePT` Accuracy for Baseline  Model `{model.name_or_path}` = {benchmark_results_new_tokens['CalamePT']['result']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7857619",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = ' Ela correu durante horas para alcançar a linha de'\n",
    "max_new_tokens = 10\n",
    "generation = tokenizer.decode(utils.generate(\n",
    "    model, tokenizer,\n",
    "    phrase,\n",
    "    DEVICE, \n",
    "    False, False,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    **MODEL_GEN_KWARGS\n",
    ")[0])\n",
    "print(f'Generation with new tokenizer = `{generation}`')\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(phrase)\n",
    "input_ids = [token for x in input_ids for token in original_tokenization.get(tokenizer.convert_ids_to_tokens(x), [x])]\n",
    "\n",
    "for n in range(10):\n",
    "    generation = model.generate(torch.Tensor([input_ids]).long().to(DEVICE), max_new_tokens=1, **MODEL_GEN_KWARGS)\n",
    "    generated_id = generation[0, -1].item()\n",
    "    input_ids.extend(\n",
    "        [token for token in original_tokenization.get(tokenizer.convert_ids_to_tokens(generated_id), [generated_id])]\n",
    "    )\n",
    "print(f'Generation with \"old tokenizer\" = `{tokenizer.decode(input_ids)}`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a8dd2",
   "metadata": {},
   "source": [
    "## Model with additional Tokens + \"Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67017999",
   "metadata": {},
   "source": [
    "Training the model (already has the added tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "#     Update weights of Embeddings of new_tokens      \n",
    "# ----------------------------------------------------\n",
    "# Step 4.2 Using the training phrases to update the embedding weights\n",
    "learning_rate = 1e-6\n",
    "training_phrases: list[str] = CalamePT().prediction_prompts.to_list() # CalamePT dataset \n",
    "for new_token in tqdm.tqdm(new_tokens, desc='Updating the embeddings for the new tokens'):\n",
    "    new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "    new_token = tokenizer.decode(new_token_id)\n",
    "    phrases_to_generate_new_token = [p for phrase in training_phrases for p in phrase.split(new_token)[:-1] if new_token in phrase and len(p) > 0]\n",
    "\n",
    "    if len(phrases_to_generate_new_token) == 0: continue\n",
    "    # Creating the Batched dataset (to run generation for multiple phrases at the same time)\n",
    "    dataloader = DataLoader(\n",
    "        TextDataset(phrases_to_generate_new_token, tokenizer, max_length=max(len(tokenizer.tokenize(x)) for x in phrases_to_generate_new_token)),\n",
    "        batch_size=GENERATION_BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    # Process the batches\n",
    "    for batch in tqdm.tqdm(dataloader,  desc=f'  Generating tokens for new_token=`{new_token}` ', leave=False):\n",
    "        # Move batch tensors to the correct device\n",
    "        input_ids = batch['input_ids'].squeeze(1).to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].squeeze(1).to(DEVICE)\n",
    "\n",
    "        # Generate text\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_logits=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            **MODEL_GEN_KWARGS\n",
    "        )\n",
    "\n",
    "        # Extract the generated sequences and their scores\n",
    "        generated_sequences = outputs.sequences\n",
    "        predicted_logits = outputs.logits\n",
    "\n",
    "        # Decode the input and generated sequences\n",
    "        input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        generated_texts = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(input_texts)):\n",
    "                logits = predicted_logits[0][i]\n",
    "                logit_gradient = logits.max() - logits[new_token_id]\n",
    "                embed_out = outputs.hidden_states[0][-1][i][-1]\n",
    "                # normalize embed_out\n",
    "                embed_out = embed_out / embed_out.norm()\n",
    "\n",
    "                embed_in = new_embed.weight[new_token_id]\n",
    "\n",
    "                # Update the embedding table\n",
    "                _ = new_embed.weight[new_token_id].data.copy_((embed_in + logit_gradient * embed_out * learning_rate).to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc32e0",
   "metadata": {},
   "source": [
    "### Update model name\n",
    "\n",
    "Changing the model name to include the \"added_tokens\" to distinguish between the older model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.name_or_path = f'{model.name_or_path} [TRAINED]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0d075",
   "metadata": {},
   "source": [
    "### Run the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results_new_tokens_trained = benchmark.run(model, tokenizer, original_tokenizer,  generation_kwargs=MODEL_GEN_KWARGS)\n",
    "print(f\"`CalamePT` Accuracy for Baseline Model `{model.name_or_path}` = {benchmark_results_new_tokens_trained['CalamePT']['result']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd79133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out which \"new_tokens\" were generated\n",
    "tmp = [x['generated_ids'].max().item() for x in benchmark_results_new_tokens_trained['CalamePT']['results'][0]['benchmark_predictions']]\n",
    "# len([t for t in tmp if t>len(original_tokenizer)])\n",
    "\n",
    "# Adding flag of \"new_token_generated\" in benchmark results\n",
    "original_tokenizer_size = len(original_tokenizer)\n",
    "for i, test in tqdm.tqdm(enumerate(benchmark_results_new_tokens_trained['CalamePT']['results'][0]['benchmark_predictions'])):\n",
    "    test['has_new_token'] = len([t for t in test['generated_ids'] if t>original_tokenizer_size]) > 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_w_new_tokens = [a.copy() for a in benchmark_results_new_tokens_trained['CalamePT']['results'][0]['benchmark_predictions'] if a['has_new_token']]\n",
    "def word_has_new_token(word: str, token_ids, new_token_start_id: int):\n",
    "    i = 1\n",
    "    cur_tokens = [token_ids[0]]\n",
    "    while tokenizer.decode(cur_tokens).strip().lower() != word.lower() and i<len(token_ids):\n",
    "        cur_tokens.append(token_ids[i])\n",
    "        i+=1\n",
    "    return len([t for t in cur_tokens if new_token_start_id < t]) > 0\n",
    "\n",
    "new_token_start_id = len(original_tokenizer)\n",
    "for i, r in enumerate(results_w_new_tokens):\n",
    "    # Find out if \"predicted_word\" is made up of any \"new_token\"\n",
    "    results_w_new_tokens[i]['new_token_in_word'] = word_has_new_token(r['prediction'], r['generated_ids'][-8:], new_token_start_id)\n",
    "    results_w_new_tokens[i]['correct_prediction'] = r['prediction'].strip().lower() == r['correct_word'].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b329dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([153052])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58904438",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([153665])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\" público\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.benchmarks[0].df[(' '+benchmark.benchmarks[0].df['last_word']).apply(lambda x: tokenizer.encode(x)[0] >= 151665 and len(tokenizer.tokenize(x)) == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78763c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[r for r in results_w_new_tokens if r['new_token_in_word'] and r['correct_prediction']]\n",
    "# results_w_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d79d58",
   "metadata": {},
   "source": [
    "SAVE ALL Benchmarks in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/yali/MEGA/Hack The Tockenizer/tests/qwen2.5-benchmark-results_V2_run2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(benchmark.get_results(), f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a18d80",
   "metadata": {},
   "source": [
    "# Analysing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('/home/yali/MEGA/Hack The Tockenizer/tests/qwen2.5-benchmark-results_V2_run2.json', 'r') as f:\n",
    "    results: dict[str, dict[str, dict]] = json.load(f)\n",
    "\n",
    "data = []\n",
    "for model_name in results.keys():\n",
    "    for benchmark_name in results[model_name].keys():\n",
    "        for epoch_n, result in enumerate(results[model_name][benchmark_name]['results']):\n",
    "            result: dict\n",
    "            data.append(result.copy())\n",
    "            data[-1]['model_name'] = model_name\n",
    "            data[-1]['epoch_number'] = epoch_n\n",
    "\n",
    "            r = data[-1].pop('benchmark_predictions')\n",
    "            data[-1]['original_text']   = [gen[\"text\"]          for gen in r]\n",
    "            data[-1]['expected_word']   = [gen[\"correct_word\"]  for gen in r]\n",
    "            data[-1]['predicted_word']  = [gen[\"prediction\"]    for gen in r]\n",
    "\n",
    "df = pd.DataFrame(data).explode(['original_text', 'expected_word', 'predicted_word'])\n",
    "# Changing the model name to only 2 letters, the first specifies if the model has been initialized or not, and the second one specifies if it has been trained or not\n",
    "df['model_name'] = df['model_name'].map({\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct':                                   'BB',   # Baseline,    Baseline  \n",
    "    'Qwen/Qwen2.5-1.5B-Instruct [TRAINED]':                         'BT',   # Baseline,    Trained  (currently not existing)\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct-ADDED_TOKENS_INIT_K=1.5':           'IB',   # Initialized, Baseline\n",
    "    'Qwen/Qwen2.5-1.5B-Instruct-ADDED_TOKENS_INIT_K=1.5 [TRAINED]': 'IT',   # Initialized, Trained\n",
    "})\n",
    "\n",
    "\n",
    "df = df.pivot(\n",
    "    index='original_text', columns=['model_name'], values=['predicted_word', 'expected_word']\n",
    ")\n",
    "df.columns = ['_'.join(col) for col in df.columns]\n",
    "df = df[['predicted_word_BB', 'predicted_word_IB', 'predicted_word_IT', 'expected_word_BB']].reset_index().rename(columns={'expected_word_BB': 'expected_word'})\n",
    "\n",
    "for model in [f'{init}{train}' for init in ['B', 'I'] for train in ['B', 'T']]:\n",
    "    # There is no \"BaselineTrained\" model, YET\n",
    "    if model == 'BT':\n",
    "        continue\n",
    "    df[f'correct_prediction_{model}'] = df[f'predicted_word_{model}'] == df['expected_word']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0bb2e",
   "metadata": {},
   "source": [
    "Saving to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/yali/MEGA/Hack The Tockenizer/tests/qwen2.5-benchmark-results_V2_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next STEP: TRAIN the baseline model with a \"normal\" approach for the same amount of time and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a15626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next STEP: Remover os tokens que sao \"contidos\" em algum dos tokens no tokenizador original + Usar \"palavras\" do corpus como \"new_tokens\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

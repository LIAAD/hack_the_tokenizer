{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook serves to understand how the perplexity of a model is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cmd\n",
    "# conda install -q -c nvidia cuda-python --yes\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia --yes\n",
    "# conda install -q transformers --yes\n",
    "# conda install -q plotly --yes\n",
    "# conda install -q nbformat --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the model\n",
    "\n",
    "Using this small model [(SmolLM2-135M)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M), we load it onto the device (preference GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with full precision (538.06 MB)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Loading the model onto the device\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "print(f'Model loaded with full precision ({model.get_memory_footprint() / 1e6:.2f} MB)')\n",
    "\n",
    "\n",
    "# # Loading the model (torch.bfloat16) onto the device\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n",
    "# print(f'Model loaded with torch.bfloat16 ({model.get_memory_footprint() / 1e6:.2f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is the force that holds the Earth and the Moon together.\n",
      "\n",
      "The Moon is a satellite of the Earth. It is a rocky body that orbits the Earth. The Moon is the only natural satellite of the Earth.\n",
      "\n",
      "The Moon\n"
     ]
    }
   ],
   "source": [
    "# Testing the model \n",
    "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining Perplexity\n",
    "\n",
    "Using some code found online, defining perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text { Perplexity }=\\exp \\left(-\\frac{1}{t} \\sum_i^t \\log p_\\theta\\left(x_i \\mid x_{\\text {context }}\\right)\\right)$$\n",
    "- $x_i$ represents the token that is generated\n",
    "- $x_{\\text {context }}$ represents the preceding tokens that the current generated token is conditioned on.\n",
    "\n",
    "source: [https://docs.kolena.com/metrics/perplexity/](https://docs.kolena.com/metrics/perplexity/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(arr):\n",
    "    return arr.exp() / arr.exp().sum()\n",
    "\n",
    "def real_likelihood(pred_dist, real_token):\n",
    "    pred_dist = soft_max(pred_dist)\n",
    "    dist_real_token = pred_dist[0, real_token]\n",
    "    return dist_real_token.log()\n",
    "\n",
    "def perplexity(input_tokens) -> float:\n",
    "    perplex = 0\n",
    "    for n in range(1, input_tokens.shape[1]):\n",
    "        test_tokens = input_tokens[:, :n].to(device)\n",
    "        real_token = input_tokens[0, n]\n",
    "        predicted_distribution = model.generate(\n",
    "            test_tokens,\n",
    "            max_length=n+1,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            pad_token_id = tokenizer.eos_token_id\n",
    "        )['scores'][0]\n",
    "        perplex += real_likelihood(predicted_distribution, real_token)\n",
    "    return torch.exp(-perplex / input_tokens.shape[1]).to('cpu').item()\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text: str | list[str]) -> float:\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
    "    loss = model(\n",
    "        input_ids=inputs[\"input_ids\"], labels=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
    "    ).loss\n",
    "    return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading the text to evaluate the model on\n",
    "\n",
    "Using a few sentences in different languages, evaluate the model perplexity on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_texts = [\n",
    "    {\n",
    "        \"language\": \"pt\",\n",
    "        \"text\": \"O meu nome é João. Tenho 20 anos e gosto de comer. Hoje à noite vou comer batatas.\",\n",
    "        \"desc\": \"Talking about myself.\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"text\": \"My name is John. I am 20 years old and I like to eat. Tonight I will eat potatoes.\",\n",
    "        \"desc\": \"Talking about myself.\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": 'es',\n",
    "        'text': 'Mi nombre es Juan. Tengo 20 años y me gusta comer. Esta noche comeré papas.',\n",
    "        'desc': 'Talking about myself.'\n",
    "    },\n",
    "    {\n",
    "        'language': 'fr',\n",
    "        'text': 'Mon nom est Jean. J\\'ai vingt ans et j\\'aime manger. Ce soir, je vais manger des pommes de terre.',\n",
    "        'desc': 'Talking about myself.'\n",
    "    },\n",
    "    # Talking about favorite books and authors\n",
    "    {\n",
    "        \"language\": \"pt\",\n",
    "        \"text\": \"Eu gosto de ler livros. Meu autor favorito é Machado de Assis.\",\n",
    "        \"desc\": \"Talking about favorite books and authors\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"text\": \"I enjoy reading books. My favorite author is Jane Austen.\",\n",
    "        \"desc\": \"Talking about favorite books and authors\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"es\",\n",
    "        \"text\": \"Me gusta leer libros. Mi autor favorito es Gabriel García Márquez.\",\n",
    "        \"desc\": \"Talking about favorite books and authors\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"fr\",\n",
    "        \"text\": \"J'aime lire des livres. Mon auteur préféré est Victor Hugo.\",\n",
    "        \"desc\": \"Talking about favorite books and authors\"\n",
    "    },\n",
    "    # Talking about visiting family\n",
    "    {\n",
    "        \"language\": \"pt\",\n",
    "        \"text\": \"Amanhã vou visitar meus avós. Faz tempo que não os vejo.\",\n",
    "        \"desc\": \"Talking about visiting family\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"text\": \"Tomorrow I'm visiting my grandparents. It's been a while since I last saw them.\",\n",
    "        \"desc\": \"Talking about visiting family\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"es\",\n",
    "        \"text\": \"Mañana visitaré a mis abuelos. Hace tiempo que no los veo.\",\n",
    "        \"desc\": \"Talking about visiting family\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"fr\",\n",
    "        \"text\": \"Demain, je vais rendre visite à mes grands-parents. Ça fait longtemps que je ne les ai pas vus.\",\n",
    "        \"desc\": \"Talking about visiting family\"\n",
    "    },\n",
    "\n",
    "    # Talking about weekend plans\n",
    "    {\n",
    "        \"language\": \"pt\",\n",
    "        \"text\": \"No fim de semana, quero ir à praia e relaxar um pouco.\",\n",
    "        \"desc\": \"Talking about weekend plans\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"text\": \"This weekend, I want to go to the beach and relax a bit.\",\n",
    "        \"desc\": \"Talking about weekend plans\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"es\",\n",
    "        \"text\": \"Este fin de semana quiero ir a la playa y relajarme un poco.\",\n",
    "        \"desc\": \"Talking about weekend plans\"\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"fr\",\n",
    "        \"text\": \"Ce week-end, je veux aller à la plage et me détendre un peu.\",\n",
    "        \"desc\": \"Talking about weekend plans\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lan: pt | Talking about myself. - perplexity = 32.49676513671875  | 36.00029373168945\n",
      "lan: en | Talking about myself. - perplexity = 9.010916709899902  | 9.875325202941895\n",
      "lan: es | Talking about myself. - perplexity = 20.19571304321289  | 22.251785278320312\n",
      "lan: fr | Talking about myself. - perplexity = 14.724316596984863  | 15.866511344909668\n",
      "lan: pt | Talking about favorite books and authors - perplexity = 39.04146194458008  | 45.7850227355957\n",
      "lan: en | Talking about favorite books and authors - perplexity = 16.815593719482422  | 21.733985900878906\n",
      "lan: es | Talking about favorite books and authors - perplexity = 25.389171600341797  | 30.100793838500977\n",
      "lan: fr | Talking about favorite books and authors - perplexity = 45.63804244995117  | 54.74489212036133\n",
      "lan: pt | Talking about visiting family - perplexity = 106.76671600341797  | 132.0194091796875\n",
      "lan: en | Talking about visiting family - perplexity = 10.95315933227539  | 12.511032104492188\n",
      "lan: es | Talking about visiting family - perplexity = 37.82791519165039  | 44.62025833129883\n",
      "lan: fr | Talking about visiting family - perplexity = 33.0004768371582  | 36.689064025878906\n",
      "lan: pt | Talking about weekend plans - perplexity = 71.52934265136719  | 88.55420684814453\n",
      "lan: en | Talking about weekend plans - perplexity = 11.346708297729492  | 13.496374130249023\n",
      "lan: es | Talking about weekend plans - perplexity = 35.30097198486328  | 41.50889587402344\n",
      "lan: fr | Talking about weekend plans - perplexity = 35.91947937011719  | 41.699913024902344\n"
     ]
    }
   ],
   "source": [
    "for n, input_text in enumerate(eval_texts):\n",
    "    input_tokens = tokenizer.encode(input_text['text'], return_tensors=\"pt\").to(device)\n",
    "    eval_texts[n]['perplexity'] = perplexity(input_tokens)\n",
    "    eval_texts[n]['compute_perplexity'] = compute_perplexity(model, tokenizer, input_text['text'])\n",
    "    print(f'lan: {input_text[\"language\"]} | {input_text[\"desc\"]} - perplexity = {input_text[\"perplexity\"]}  | {input_text[\"compute_perplexity\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A quick analysis\n",
    "\n",
    "Quickly comparing perplexity between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "df = pd.DataFrame(eval_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index='desc', columns='language', values='perplexity').plot(kind='line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

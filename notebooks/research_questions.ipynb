{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f649a05",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook serves as a guideline to answer all the research questions proposed in the Thesis document\n",
    "\n",
    "We will go over the following questions:\n",
    "1. Section *1.2. Objectives*\n",
    "    1. Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?\n",
    "    1. How much can inference efficiency be improved by adding language-specific tokens to the model’s vocabulary?\n",
    "    1. What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?\n",
    "1. Section *4. Exploratory Analysis and Design Rationale* -> *4.1. Research Questions*\n",
    "    1. What impact does tokenizer adaptation have on model performance for European Portuguese? Does it achieve comparable performance to the baseline model?\n",
    "    1. How does tokenizer adaptation affect token efficiency (as measured by the Fertility metric) for Portuguese text processing?\n",
    "    1. Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?\n",
    "    1. What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?\n",
    "    1. Does a model loose performance in English after going through the hacking process?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e837e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import hack_tokenizer\n",
    "from hack_tokenizer.utils import constants\n",
    "\n",
    "with open(constants.DATA_DIR / 'tokenizer_pt-pt.txt', 'r') as f:\n",
    "    DATASET_TOKENIZER = f.readlines()\n",
    "with open(constants.DATA_DIR / 'calamept_dataset.txt', 'r') as f:\n",
    "    DATASET_TRAINING = f.readlines()\n",
    "ENGLISH_ONLY_MODELS = ['HuggingFaceTB/SmolLM2-135M']   # English-only models which we ran the evaluation on\n",
    "MULTI_LANGUAL_MODELS = [\"Qwen/Qwen2.5-1.5B-Instruct\", \"HuggingFaceTB/SmolLM3-3B\"] # Multi-language models which we ran the evaluations on\n",
    "ALL_MODELS = ENGLISH_ONLY_MODELS + MULTI_LANGUAL_MODELS\n",
    "NUMBER_NEW_TOKENS = constants.NUMBER_NEW_TOKENS\n",
    "EMBED_INIT_METHOD = constants.EMBED_INIT_METHOD\n",
    "BATCH_SIZE  = constants.GENERATION_BATCH_SIZE\n",
    "DEVICE      = constants.DEVICE\n",
    "# TEMPERATURE = constants.TEMPERATURE\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P       = 0.9\n",
    "TOP_K       = 100\n",
    "\n",
    "\n",
    "# ASCII Color Codes\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Color:\n",
    "    name: str\n",
    "    code: str\n",
    "    _RESET: str = '\\x1b[0m'\n",
    "    def __str__(self): return f'{self._RESET}\\x1b[{self.code}'\n",
    "    @property\n",
    "    def bold(self): return f'{self._RESET}\\x1b[1;{self.code}'\n",
    "class Colors:\n",
    "    BLACK   = Color('Black',    '30m')\n",
    "    RED     = Color('RED',      '31m')\n",
    "    GREEN   = Color('GREEN',    '32m')\n",
    "    YELLOW  = Color('YELLOW',   '33m')\n",
    "    BLUE    = Color('BLUE',     '34m')\n",
    "    PURPLE  = Color('PURPLE',   '35m')\n",
    "    CYAN    = Color('CYAN',     '36m')\n",
    "    WHITE   = Color('WHITE',    '37m')\n",
    "    RESET   = Color('NO_COLOR', '0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab54cd3",
   "metadata": {},
   "source": [
    "# Section 1.2 - Objectives\n",
    "\n",
    "Answering all questions in section 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6bd4a",
   "metadata": {},
   "source": [
    "## Research Question 1.2.1\n",
    "Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df5e1e",
   "metadata": {},
   "source": [
    "### Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'MMLU', 'CalamePT', 'SupergluePTPT']]\n",
    "df_121[['number_new_tokens', 'model_type', 'CalamePT', 'SupergluePTPT']].to_html()\n",
    "# df_121 = df_121.query('model in @ENGLISH_ONLY_MODELS')\n",
    "# print(df_121[['model', 'number_new_tokens', 'model_type', 'CalamePT', 'SupergluePTPT']].to_html())   # To add to Markdown bellow\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fc910",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "\n",
    "Using an actual LLM to answer some questions to see if it has similar performance for Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2110fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing tokens \"contained\" within any token of original tokenizer: 100%|██████████| 6874/6874 [00:12<00:00, 555.62it/s]\n",
      "Initializing the embeddings for the new_tokens: 100%|██████████| 5000/5000 [00:00<00:00, 8716.18it/s]\n",
      "Updating the embeddings for the new tokens: 100%|██████████| 5000/5000 [06:53<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================================\n",
      "\n",
      "                                                                                  MODEL: `HuggingFaceTB/SmolLM2-135M`                                                                                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mPara calcular a raiz quadrada de um número manualmente,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m retorna uma posição.\n",
      "\n",
      "```python\n",
      "x = 10\n",
      "y = 20\n",
      "print(f\"A raiz quadrada de {x} e {y} é {x ** \u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mTo calculate the square root of a number by hand,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m you would divide the number into equal parts and then subtract the results to get the square root of the result. In Python, we can use the built-in `sqrt()` function to calculate the square root of a number.\n",
      "\n",
      "###\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mPara calcular a raiz quadrada de um número manualmente,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m utilizando o\n",
      "tipo de cálculo de fatorial. Por exemplo, se uma quantidade de\n",
      "pemares a pouco máais de um alcance, seja 110000\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mTo calculate the square root of a number by hand,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m the digits of the number can be marked on a piece of paper, or a digital calculator will do the job for you.\n",
      "\n",
      "Step 14: Choose a suitable scale\n",
      "\n",
      "Use a calculator or a digital scale to scale the numbers\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mPara calcular a raiz quadrada de um número manualmente,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m\n",
      "e construa um número quadrado em duas partes\n",
      "aixa e a linda,\n",
      "\n",
      "A matoê estra:\n",
      "A matoê do primeiro num méênico da raiz\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mTo calculate the square root of a number by hand,\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m the rule is to take the negative of the first digit, then multiply the result by itself, and then subtract the result.\n",
      "\n",
      "### Why Square Root Calculator?\n",
      "\n",
      "The square root of 1833195073330500\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNos dias de hoje, democracia é o sistema politico\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m. Ao\n",
      "demorar por 6 dias para que seja o seu emissar, a dor da elemência eleitora\n",
      "atual aumentará e o próprio p\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNowadays, democracy is the political system\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m that allows citizens to choose the leaders who will guide them in their chosen path. They are also known as the people.\n",
      "\n",
      "DEMOCRACY - The people who rule a country or an organization.\n",
      "\n",
      "DEPARTMENT\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNos dias de hoje, democracia é o sistema politico\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m que ele, na quais seriam\n",
      "a diferença, de forma fatoriva, de uma média de 013020 00010000000010000000000000001000\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNowadays, democracy is the political system\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m of a country or country region where the people, including elected representatives, are able to choose the leaders of the country. In other words, the people are free to govern and choose their leaders and policies for their country.\n",
      "\n",
      "In the\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNos dias de hoje, democracia é o sistema politico\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m, o que não se encontra nis. A que também é o democracia.\n",
      "\n",
      "EIuão de Hic� e da C�ria de Io Paulo.\n",
      "\n",
      "Hic�\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mNowadays, democracy is the political system\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m that allows citizens to vote for their representatives, so that they can elect a government. In a democracy, people vote for politicians to represent them. This is similar to a democracy where people elect representatives who vote for those politicians.\n",
      "\n",
      "A\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mO poema seguinte contém várias palavras-chave: azul, borboleta e sol:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m e sí, e também, e física e próximo.\n",
      "\n",
      "O poema também tem como seu último último, o estudante de\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mBASELINE[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mThe following poem contains several keywords: Blue, Butterfly, and Sun:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m\n",
      "\n",
      "Here is the beginning of the poem:\n",
      "\n",
      "Blue, Butterfly, and Sun\n",
      "\n",
      "Blue, Butterfly, and Sun\n",
      "\n",
      "Blue, Butterfly, and Sun\n",
      "\n",
      "Blue, Butterfly, and Sun\n",
      "\n",
      "Blue, Butterfly\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mO poema seguinte contém várias palavras-chave: azul, borboleta e sol:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m céu de la tierra, y v�rgosos deixa.\n",
      "\n",
      "Vídeo como nome: «C�icio-dei alcoba e sol», com aumos dos d�cimentoimens\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_NO_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mThe following poem contains several keywords: Blue, Butterfly, and Sun:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m\n",
      "\n",
      "Blue Butterfly\n",
      "\n",
      "I am blue,\n",
      "The butterfly of my mind\n",
      "\n",
      "I feel my mind\n",
      "Throbbing in my breast,\n",
      "And I cannot tell what to do.\n",
      "\n",
      "I cannot tell what to do,\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[PT]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mO poema seguinte contém várias palavras-chave: azul, borboleta e sol:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m “Este estar�á por esqueço da esquina do lado do que a leitura seja frente ao largo do espaço”. Para outras palavras-chave,\u001b[0m\u001b[31m</RESPONSE>\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;32mINITIALIZED_TRAINING[EN]:\u001b[0m\u001b[31m <PROMPT>\u001b[0m\u001b[33mThe following poem contains several keywords: Blue, Butterfly, and Sun:\u001b[0m\u001b[31m</PROMPT><RESPONSE>\u001b[0m\u001b[36m\n",
      "\n",
      "Blue Butterfly\n",
      "\n",
      "Blue Butterfly\n",
      "\n",
      "I feel blue\n",
      "\n",
      "A butterfly has flown\n",
      "\n",
      "Across the blue sky\n",
      "\n",
      "And I feel sad\n",
      "\n",
      "I feel sad and angry\n",
      "\n",
      "Ia like the butterfly\n",
      "\n",
      "A\u001b[0m\u001b[31m</RESPONSE>\n"
     ]
    }
   ],
   "source": [
    "PROMPTS = [\n",
    "    {\n",
    "        'PT': 'Para calcular a raiz quadrada de um número manualmente,',\n",
    "        'EN': 'To calculate the square root of a number by hand,'\n",
    "    },\n",
    "    {\n",
    "        'PT': 'Nos dias de hoje, democracia é o sistema politico',\n",
    "        'EN': 'Nowadays, democracy is the political system'\n",
    "    },\n",
    "    {\n",
    "        'PT': 'O poema seguinte contém várias palavras-chave: azul, borboleta e sol:',\n",
    "        'EN': 'The following poem contains several keywords: Blue, Butterfly, and Sun:'\n",
    "    }\n",
    "]\n",
    "RESPONSES = []\n",
    "\n",
    "def obtain_responses(prompt_kwargs):\n",
    "    responses = []\n",
    "    for prompt in PROMPTS:\n",
    "        responses.append({})\n",
    "        for language in prompt.keys():\n",
    "            generation = hacker.prompt(content=prompt[language], **prompt_kwargs)\n",
    "            responses[-1][language] = ''.join(generation)\n",
    "    return responses\n",
    "\n",
    "for model_name in ENGLISH_ONLY_MODELS:\n",
    "    model, tokenizer   = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)\n",
    "    encoding_tokenizer = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)[1]\n",
    "    hacker = hack_tokenizer.hack.ModelHacker(\n",
    "        dataset=DATASET_TOKENIZER,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    prompt_kwargs = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'encoding_tokenizer': encoding_tokenizer,\n",
    "        'max_new_tokens': 50,\n",
    "        'stop_words': ['<|im_end|>', '<|endoftext|>'],\n",
    "        'temperature': TEMPERATURE,\n",
    "        'top_p': TOP_P,\n",
    "        'top_k': TOP_K,\n",
    "        'print_response': False,\n",
    "        'num_tokens_generated_at_once': 4,\n",
    "    }\n",
    "\n",
    "    # Obtain responses BEFORE hacking\n",
    "    RESPONSES.append({'BASELINE': obtain_responses(prompt_kwargs)})\n",
    "\n",
    "    model, tokenizer = hacker.hack(\n",
    "        model, tokenizer,\n",
    "        encoding_tokenizer,\n",
    "        num_tokens=NUMBER_NEW_TOKENS,\n",
    "        embed_initializer_method=EMBED_INIT_METHOD,\n",
    "        show_progress=True,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # Go over all the prompts and check responses\n",
    "    prompt_kwargs.update({'model': model, 'tokenizer': tokenizer})\n",
    "    RESPONSES[-1]['INITIALIZED_NO_TRAINING'] = obtain_responses(prompt_kwargs)\n",
    "\n",
    "    # Train model and test again\n",
    "    hacker.train(model, tokenizer, encoding_tokenizer, DATASET_TRAINING, hacker.new_tokens, batch_size=BATCH_SIZE, show_progress=True)\n",
    "\n",
    "    # Go over all the prompts and check responses\n",
    "    prompt_kwargs.update({'model': model, 'tokenizer': tokenizer})\n",
    "    RESPONSES[-1]['INITIALIZED_TRAINING'] = obtain_responses(prompt_kwargs)\n",
    "\n",
    "\n",
    "    # Visually print all prompts\n",
    "    print(f'{\"\":=^200s}\\n\\n{\"MODEL: `\" + model_name + \"`\": ^200s}\\n\\n')\n",
    "    for n, prompt in enumerate(PROMPTS):\n",
    "        # Print responses\n",
    "        for model_type in RESPONSES[-1].keys():\n",
    "            for lan in RESPONSES[-1][model_type][n].keys():\n",
    "                # Use regex to remove multiple newlines to a maximum of 2\n",
    "                response = re.sub(r'\\n{3,}', '\\n\\n', RESPONSES[-1][model_type][n][lan])\n",
    "                print(f'\\n\\n{Colors.GREEN.bold}{model_type}[{lan}]:{Colors.RED} <PROMPT>{Colors.YELLOW}{prompt[lan]}{Colors.RED}</PROMPT><RESPONSE>{Colors.CYAN}{response}{Colors.RED}</RESPONSE>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9e924",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The answer to this question was **NO**:\n",
    "By looking into the data itself, it doesn't seem like the model gets better at Portuguese (`CalamePT` and `SupergluePTPT`) by adding new tokens and initializing the embedding table only.\n",
    "One interesting take away is that the model `Qwen/Qwen2.5-1.5B-Instruct` saw a slight drop in both `CalamePT` and `SupergluePTPT` when adding 7500 tokens.\n",
    "However, the model `HuggingFaceTB/SmolLM2-135M` showed a slight increase. Neither of these changes are statistically significant, but further investigation may reveal more details\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>model</th> <th>number_new_tokens</th> <th>model_type</th> <th>CalamePT</th> <th>SupergluePTPT</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>0</td> <td>BASELINE</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>1</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>2</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>3</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>4</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>5</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>6</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>7</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>0</td> <td>BASELINE</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>8</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>9</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>10</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>11</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>12</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>13</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>14</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>0</td> <td>BASELINE</td> <td>0.496146</td> <td>0.402396</td> </tr> <tr> <th>15</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.496146</td> <td>0.398570</td> </tr> <tr> <th>16</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.496146</td> <td>0.397943</td> </tr> <tr> <th>17</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.496146</td> <td>0.402835</td> </tr> <tr> <th>18</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.496146</td> <td>0.402083</td> </tr> <tr> <th>19</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.495665</td> <td>0.401393</td> </tr> <tr> <th>20</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.495665</td> <td>0.400640</td> </tr> </tbody> </table> \n",
    "\n",
    "Also, By looking into the output of the `notebook cell` above, we can see that the quality of the English response is still higher than that of the Portuguese one. And it doesn't seem to change after we've added new tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa32cbc",
   "metadata": {},
   "source": [
    "## Research Question 1.2.2\n",
    "How much can inference efficiency be improved by adding language-specific tokens to the model’s vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa84943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'FertilityOutput', 'FertilityBoost']]\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396d4d1",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Yes -> By observing the results of `FertilityBoost`, it's clear to see that adding the new tokens can increase generation speed by upwards of `16%`. This increase, however, is mosttly seen in the lowest, mono-lingual model. It has not been explored if this increase is due to the mono-lingual aspect or the model-size. (future work: Explore bigger mono-lingual models & small multi-lingual models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777775b2",
   "metadata": {},
   "source": [
    "## Research Question 1.2.3\n",
    "What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb961dc8",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "In order to obtain an answer to this question, we're going to evaluation `new_tokens` logits when they are **expected** to be predicted by using pre-determined phrases where we know those tokens should exist.\n",
    "\n",
    "Then, we compare the results of multiple initialization methods to determine the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e51a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "#                       STEP 1. Fetch a dataset\n",
    "#\n",
    "# In step 1 we're fetching a dataset and selecting a random number of lines which CONTAIN any added token\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "CHOSEN_LINES = None\n",
    "CHOSEN_TOKENS = {}\n",
    "NEW_TOKENS = None\n",
    "\n",
    "def calculate_token_scores(model, tokenizer, phrase: str, token_id: int):\n",
    "    '''\n",
    "    Returns the score of a specific token_id when generating a new token with `phrase` as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Any\n",
    "        model to generate the phrase with\n",
    "    \n",
    "    tokenizer: Any\n",
    "        tokenizer to encode the given phrase\n",
    "    \n",
    "    phrase: str\n",
    "        phrase to give as input to the model\n",
    "    \n",
    "    token_id: int\n",
    "        token to retrieve the scores to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[Literal['score', 'rank', 'best_score'], float]\n",
    "    '''\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    for key in inputs.keys(): inputs[key] = inputs[key].to(model.device)\n",
    "    generation = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "    scores = generation['scores'][0][0]\n",
    "    token_score = scores[token_id]\n",
    "    token_rank = (scores > token_score).sum()\n",
    "    return {'score': token_score.item(), 'rank': token_rank.item(), 'best_score': scores.max().item()}\n",
    "\n",
    "def choose_random_line(file, file_size: int) -> bytes:\n",
    "    \"\"\"\n",
    "    Seek to a random byte position and return the next full line.\n",
    "    \n",
    "    Args:\n",
    "        file: A file object opened in binary mode.\n",
    "        file_size: The total size of the file in bytes.\n",
    "        \n",
    "    Returns:\n",
    "        A full line (bytes), starting from the next newline after the random byte.\n",
    "    \"\"\"\n",
    "    byte_pos = np.random.randint(0, file_size - 1)\n",
    "    file.seek(byte_pos)\n",
    "    \n",
    "    # Skip partial line\n",
    "    file.readline()\n",
    "    \n",
    "    # Return the next full line\n",
    "    return file.readline()\n",
    "\n",
    "def sample_lines(file_path: str, sample_size: int, musthave_chars_list: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Randomly sample a given number of unique lines from a large file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file.\n",
    "        sample_size: Number of unique lines to sample.\n",
    "        \n",
    "    Returns:\n",
    "        A list of decoded strings (lines).\n",
    "    \"\"\"\n",
    "    global CHOSEN_LINES\n",
    "    if CHOSEN_LINES is not None:\n",
    "        return CHOSEN_LINES\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Find #SAMPLE_SIZE phrases containing any of the new tokens\n",
    "    selected_lines = set()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while len(selected_lines) < sample_size:\n",
    "            line = choose_random_line(f, file_size)\n",
    "            if line and line.strip() and any(t in line.decode() for t in musthave_chars_list):  # Ignore empty lines and lines which don't contain any added token\n",
    "                selected_lines.add(line.decode())\n",
    "    CHOSEN_LINES = list(selected_lines)\n",
    "    return CHOSEN_LINES\n",
    "\n",
    "def print_prefix(model: str, method: str, start: pd.Timestamp):\n",
    "    timestamp_print = f'{pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")} | {(pd.Timestamp.now() - start).total_seconds():<5.2f} sec'\n",
    "    return f'[`{model}`/{\"`\"+method+\"`\":<17s} @ {timestamp_print}]'\n",
    "\n",
    "def analyze_init_method(model_name: str, num_new_tokens: int, embed_init_method: str, dataset_path: str, sample_size: int):\n",
    "    global CHOSEN_TOKENS, NEW_TOKENS\n",
    "    start = pd.Timestamp.now()\n",
    "    print_args = [model_name, embed_init_method, start]\n",
    "    print(f'{print_prefix(*print_args)} - Initializing model and tokenizer')\n",
    "    # \"Hacking\" a model and training a tokenizer to get \"new_tokens\"\n",
    "    model, tokenizer   = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)\n",
    "    encoding_tokenizer = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)[1]\n",
    "    hacker = hack_tokenizer.hack.ModelHacker(dataset=DATASET_TOKENIZER, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(f'{print_prefix(*print_args)} - Hacking model and tokenizer')\n",
    "    model, tokenizer = hacker.hack(\n",
    "        model, tokenizer,\n",
    "        encoding_tokenizer,\n",
    "        num_tokens=num_new_tokens,\n",
    "        embed_initializer_method=embed_init_method,\n",
    "        show_progress=False,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    assert NEW_TOKENS is None or all(t in NEW_TOKENS for t in hacker.new_tokens)\n",
    "    NEW_TOKENS = set(t for t in hacker.new_tokens)\n",
    "\n",
    "    # Select lines\n",
    "    lines = sample_lines(dataset_path, sample_size, hacker.new_tokens)\n",
    "\n",
    "    # Iterate over them\n",
    "    scores = []\n",
    "    for line in tqdm.tqdm(lines, desc=f'{print_prefix(*print_args)} - Analyzing results'):\n",
    "        # Pick a random `new_token` in `line` to simulate a generation for it\n",
    "        new_tokens_in_line = [t for t in hacker.new_tokens if t in line]\n",
    "        # Find the random token for the line\n",
    "        split_token = CHOSEN_TOKENS.get(line, np.random.choice(new_tokens_in_line, 1)[0].item())\n",
    "        # Update the random token for the line\n",
    "        CHOSEN_TOKENS[line] = split_token\n",
    "        split_token = np.random.choice(new_tokens_in_line, 1)[0].item()   # Randomly choose a token to split the word on\n",
    "        splitted_line = line.split(split_token)\n",
    "        eval_phrase = splitted_line.pop(0)\n",
    "        while len(eval_phrase) < 10 and len(splitted_line) > 0:    # While we don't have 10 characters, continuously expand the phrase\n",
    "            eval_phrase += split_token + splitted_line.pop(0)\n",
    "        # Calculate the score for it\n",
    "        scores.append(calculate_token_scores(model, encoding_tokenizer, eval_phrase, tokenizer.encode(split_token)))\n",
    "    print(f'{print_prefix(*print_args)} - Finished Analysis')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e11b85",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "By using the Auxiliary functions, we're going to Analyze all different methods available to compare how the scores of the initializations compare with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`mean`            @ 2025-09-03 21:25:39 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`mean`            @ 2025-09-03 21:25:43 | 3.40  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`mean`            @ 2025-09-03 21:25:58 | 18.12 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 78.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`mean`            @ 2025-09-03 21:26:10 | 30.91 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`min`             @ 2025-09-03 21:26:10 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`min`             @ 2025-09-03 21:26:12 | 2.05  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`min`             @ 2025-09-03 21:26:27 | 16.44 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 77.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`min`             @ 2025-09-03 21:26:40 | 29.40 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`max`             @ 2025-09-03 21:26:40 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`max`             @ 2025-09-03 21:26:42 | 2.04  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`max`             @ 2025-09-03 21:26:56 | 16.50 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`max`             @ 2025-09-03 21:27:09 | 28.87 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.25)`  @ 2025-09-03 21:27:09 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.25)`  @ 2025-09-03 21:27:11 | 1.84  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.25)`  @ 2025-09-03 21:27:26 | 17.13 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 81.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.25)`  @ 2025-09-03 21:27:38 | 29.47 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.5)`   @ 2025-09-03 21:27:38 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.5)`   @ 2025-09-03 21:27:40 | 1.83  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.5)`   @ 2025-09-03 21:27:55 | 17.08 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.5)`   @ 2025-09-03 21:28:08 | 29.45 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.75)`  @ 2025-09-03 21:28:08 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.75)`  @ 2025-09-03 21:28:10 | 2.04  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.75)`  @ 2025-09-03 21:28:25 | 17.31 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`quantile(0.75)`  @ 2025-09-03 21:28:37 | 29.77 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(0.5)` @ 2025-09-03 21:28:37 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(0.5)` @ 2025-09-03 21:28:40 | 2.69  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(0.5)` @ 2025-09-03 21:28:55 | 17.92 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(0.5)` @ 2025-09-03 21:29:08 | 30.32 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.0)` @ 2025-09-03 21:29:08 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.0)` @ 2025-09-03 21:29:10 | 1.83  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.0)` @ 2025-09-03 21:29:25 | 16.83 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.0)` @ 2025-09-03 21:29:37 | 29.27 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.5)` @ 2025-09-03 21:29:37 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.5)` @ 2025-09-03 21:29:39 | 2.16  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.5)` @ 2025-09-03 21:29:54 | 17.25 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(1.5)` @ 2025-09-03 21:30:07 | 29.68 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.0)` @ 2025-09-03 21:30:07 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.0)` @ 2025-09-03 21:30:09 | 2.77  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.0)` @ 2025-09-03 21:30:25 | 17.78 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.0)` @ 2025-09-03 21:30:37 | 30.22 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.5)` @ 2025-09-03 21:30:37 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.5)` @ 2025-09-03 21:30:40 | 2.89  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.5)` @ 2025-09-03 21:30:55 | 18.00 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(2.5)` @ 2025-09-03 21:31:07 | 30.42 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.0)` @ 2025-09-03 21:31:07 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.0)` @ 2025-09-03 21:31:09 | 1.85  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.0)` @ 2025-09-03 21:31:24 | 16.76 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 79.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.0)` @ 2025-09-03 21:31:37 | 29.28 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.5)` @ 2025-09-03 21:31:37 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.5)` @ 2025-09-03 21:31:39 | 1.84  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.5)` @ 2025-09-03 21:31:54 | 16.98 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(3.5)` @ 2025-09-03 21:32:06 | 29.37 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.0)` @ 2025-09-03 21:32:06 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.0)` @ 2025-09-03 21:32:08 | 2.08  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.0)` @ 2025-09-03 21:32:23 | 17.19 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.0)` @ 2025-09-03 21:32:36 | 29.64 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.5)` @ 2025-09-03 21:32:36 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.5)` @ 2025-09-03 21:32:38 | 2.36  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.5)` @ 2025-09-03 21:32:53 | 17.36 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 80.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(4.5)` @ 2025-09-03 21:33:05 | 29.79 sec] - Finished Analysis\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(5.0)` @ 2025-09-03 21:33:06 | 0.00  sec] - Initializing model and tokenizer\n",
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(5.0)` @ 2025-09-03 21:33:07 | 1.83  sec] - Hacking model and tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(5.0)` @ 2025-09-03 21:33:22 | 16.97 sec] - Analyzing results: 100%|██████████| 1000/1000 [00:12<00:00, 79.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[`HuggingFaceTB/SmolLM2-135M`/`weighted_drop(5.0)` @ 2025-09-03 21:33:35 | 29.55 sec] - Finished Analysis\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SAMPLE_SIZE = 1_000\n",
    "MODEL = 'HuggingFaceTB/SmolLM2-135M'\n",
    "NUMBER_NEW_TOKENS = constants.NUMBER_NEW_TOKENS\n",
    "DATASET_PATH = constants.DATA_DIR / 'FULL_opensubtitles_pt-pt.txt'\n",
    "AVAILABLE_INIT_METHODS = [\n",
    "    'mean',\n",
    "    'min',\n",
    "    'max',\n",
    "    'quantile(0.25)', 'quantile(0.5)', 'quantile(0.75)'\n",
    "] + [f'weighted_drop({i/10:.1f})' for i in range(5, 51, 5)]\n",
    "\n",
    "# Results path (to store results as it takes a bit of time to run)\n",
    "RQ124_RESULTS_PATH = 'RQ_1.2.4_Results.json'\n",
    "\n",
    "# Iterating over all available init methods\n",
    "if not os.path.isfile(RQ124_RESULTS_PATH):\n",
    "    results = {}\n",
    "    for embed_init_method in AVAILABLE_INIT_METHODS:\n",
    "        results[embed_init_method] = analyze_init_method(MODEL, NUMBER_NEW_TOKENS, embed_init_method, DATASET_PATH, SAMPLE_SIZE)\n",
    "    with open(RQ124_RESULTS_PATH, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "with open(RQ124_RESULTS_PATH, 'r') as f:\n",
    "    results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4f0c9",
   "metadata": {},
   "outputs": [
        {
        "data": {
        "text/plain": [
        "{'max': 39,\n",
        " 'mean': 36,\n",
        " 'min': 100,\n",
        " 'quantile(0.25)': 96,\n",
        " 'quantile(0.5)': 31,\n",
        " 'quantile(0.75)': 32,\n",
        " 'weighted_drop(0.5)': 26,\n",
        " 'weighted_drop(1.0)': 37,\n",
        " 'weighted_drop(1.5)': 47,\n",
        " 'weighted_drop(2.0)': 53,\n",
        " 'weighted_drop(2.5)': 81,\n",
        " 'weighted_drop(3.0)': 89,\n",
        " 'weighted_drop(3.5)': 79,\n",
        " 'weighted_drop(4.0)': 99,\n",
        " 'weighted_drop(4.5)': 126,\n",
        " 'weighted_drop(5.0)': 127}"
        ]
        },
        "execution_count": 8,
        "metadata": {},
        "output_type": "execute_result"
        }
    ],
   "source": [
    "df = []\n",
    "for key in results.keys():\n",
    "    for n, i in enumerate(results[key]):\n",
    "        i.update({'method': key, 'phrase_id': n})\n",
    "    df += results[key]\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "\n",
    "# Count number of times each method is \"number 1\" in rank (meaning how many times each method had the lowest \"rank\")\n",
    "min_values = df.groupby(by=['phrase_id'], as_index=False)[['rank']].min()\n",
    "df = df.merge(min_values, how='left', on=['phrase_id'], suffixes=('', '_min'))\n",
    "df['min_ranked'] = df['rank'] == df['rank_min']\n",
    "evaluation = df.groupby(by=['method'])['min_ranked'].sum()\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# evaluation.plot(kind='bar', y='min_ranked')\n",
    "\n",
    "\n",
    "# df.pivot(\n",
    "#     index='phrase_id', columns='method', values='rank'\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# df.plot(x='phrase_id', figsize=(20, 10), logy=True, logx=True)\n",
    "evaluation.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4008eaa",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "After evaluating 5 different methods, we decided to utilize `weighted_drop`.\n",
    "\n",
    "More specifically, `weighted_drop(2)`. The reason to choose `weighted_drop(2)` Vs the `weighted_drop(5)` is that by using $K=5$ (in $weighted\\_drop(K)$), we would pretty much not include **any** information of tokens other than the first one.\n",
    "\n",
    "For that reason, we assigned the **first** tested `weighted_drop` bigger than all the others.\n",
    "\n",
    "Final answer: `weighted_drop(2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef42cdb",
   "metadata": {},
   "source": [
    "# Section 4.1. Research Questions\n",
    "\n",
    "Answering all questions in section 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55757a13",
   "metadata": {},
   "source": [
    "## Research Question 4.1.1\n",
    "What impact does tokenizer adaptation have on model performance for European Portuguese? Does it achieve comparable performance to the baseline model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32328de9",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Refer to [Research Question 1.2.1](#research-question-121)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c5a91",
   "metadata": {},
   "source": [
    "\n",
    "## Research Question 4.1.2\n",
    "How does tokenizer adaptation affect token efficiency (as measured by the `FertilityBoost` metric) for Portuguese text processing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd71eb1",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Refer to [Research Question 1.2.2](#research-question-122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a05f77",
   "metadata": {},
   "source": [
    "\n",
    "## Research Question 4.1.3\n",
    "Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6128ee6",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "Evaluate all metrics & benchmarks before and after adapation in different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'FertilityOutput', 'FertilityBoost', 'MMLU', 'CalamePT', 'SupergluePTPT']]\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31af0b6",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "No -> It does **not** seem that our method impacts all models equally. We were not able to understand what causes a model to be prune to the changes in results, but from our analysis it became clear not all models benefit the same from our `tokenizer hacking`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fade086",
   "metadata": {},
   "source": [
    "\n",
    "## Research Question 4.1.4\n",
    "What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fdb31",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Refer to [Research Question 1.2.3](#research-question-123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d550ec",
   "metadata": {},
   "source": [
    "\n",
    "## Research Question 4.1.5\n",
    "Does a model loose performance in English after going through the hacking process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75084c3",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "Compare `MMLU` benchmark **before** and **after** applying both `initialization` and `training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98384b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'MMLU']]\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c56cef",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "NO -> From our analysis, no model lost performance in `English` after going through **both** the tokenizer adapation and training steps. Meaning this method can be applied without loosing performance in the original language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

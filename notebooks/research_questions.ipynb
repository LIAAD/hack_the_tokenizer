{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f649a05",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook serves as a guideline to answer all the research questions proposed in the Thesis document\n",
    "\n",
    "We will go over the following questions:\n",
    "1. Section *1.2. Objectives*\n",
    "    1. Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?\n",
    "    1. Can tokenizer adaptation accelerate the training process for new language adaptation?\n",
    "    1. How much can inference efficiency be improved by adding language-specific tokens to the model’s vocabulary?\n",
    "    1. What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?\n",
    "1. Section *4. Exploratory Analysis and Design Rationale* -> *4.1. Research Questions*\n",
    "    1. What impact does tokenizer adaptation have on model performance for European Portuguese? Does it achieve comparable performance to the baseline model?\n",
    "    1. How does tokenizer adaptation affect token efficiency (as measured by the Fertility metric) for Portuguese text processing?\n",
    "    1. Does tokenizer adaptation affect all models equally, or are there differences based on model architecture and size?\n",
    "    1. What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?\n",
    "    1. Does a model loose performance in English after going through the hacking process?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e837e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import hack_tokenizer\n",
    "from hack_tokenizer.utils import constants\n",
    "\n",
    "with open('../data/tokenizer_pt-pt.txt', 'r') as f:\n",
    "    DATASET_TOKENIZER = f.read()\n",
    "ENGLISH_ONLY_MODELS = ['HuggingFaceTB/SmolLM2-135M']   # English-only models which we ran the evaluation on\n",
    "MULTI_LANGUAL_MODELS = [\"Qwen/Qwen2.5-1.5B-Instruct\", \"HuggingFaceTB/SmolLM3-3B\"] # Multi-language models which we ran the evaluations on\n",
    "NUMBER_NEW_TOKENS = constants.NUMBER_NEW_TOKENS\n",
    "EMBED_INIT_METHOD = constants.EMBED_INIT_METHOD\n",
    "BATCH_SIZE  = constants.GENERATION_BATCH_SIZE\n",
    "DEVICE      = constants.DEVICE\n",
    "# TEMPERATURE = constants.TEMPERATURE\n",
    "TEMPERATURE = 0.8\n",
    "TOP_P       = 0.9\n",
    "TOP_K       = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab54cd3",
   "metadata": {},
   "source": [
    "# Section 1.2 - Objectives\n",
    "\n",
    "Answering all questions in section 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6bd4a",
   "metadata": {},
   "source": [
    "## Research Question 1.2.1\n",
    "Is it possible for an English-trained model to achieve comparable performance in European Portuguese by strategically modifying the tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df5e1e",
   "metadata": {},
   "source": [
    "### Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'MMLU', 'CalamePT', 'SupergluePTPT']]\n",
    "df_121[['number_new_tokens', 'model_type', 'CalamePT', 'SupergluePTPT']].to_html()\n",
    "# df_121 = df_121.query('model in @ENGLISH_ONLY_MODELS')\n",
    "# print(df_121[['model', 'number_new_tokens', 'model_type', 'CalamePT', 'SupergluePTPT']].to_html())   # To add to Markdown bellow\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fc910",
   "metadata": {},
   "source": [
    "### Qualitative Analysis\n",
    "\n",
    "Using an actual LLM to answer some questions to see if it has similar performance for Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = [\n",
    "    {\n",
    "        'PT': 'Para calcular a raiz quadrada de um número manualmente,',\n",
    "        'EN': 'To calculate the square root of a number by hand,'\n",
    "    },\n",
    "    {\n",
    "        'PT': 'Nos dias de hoje, democracia é o sistema politico',\n",
    "        'EN': 'Nowadays, democracy is the political system'\n",
    "    },\n",
    "    {\n",
    "        'PT': 'O poema seguinte contém várias palavras-chave: azul, borboleta e sol:',\n",
    "        'EN': 'The following poem contains several keywords: Blue, Butterfly, and Sun:'\n",
    "    }\n",
    "]\n",
    "RESPONSES = []\n",
    "\n",
    "def obtain_responses(prompt_kwargs):\n",
    "    responses = []\n",
    "    for prompt in PROMPTS:\n",
    "        responses.append({})\n",
    "        for language in prompt.keys():\n",
    "            generation = hacker.prompt(content=prompt[language], **prompt_kwargs)\n",
    "            responses[-1][language] = ''.join(generation)\n",
    "    return responses\n",
    "\n",
    "for model_name in ENGLISH_ONLY_MODELS:\n",
    "    model, tokenizer   = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)\n",
    "    encoding_tokenizer = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)[1]\n",
    "    hacker = hack_tokenizer.hack.ModelHacker(\n",
    "        dataset=DATASET_TOKENIZER,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    prompt_kwargs = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'encoding_tokenizer': encoding_tokenizer,\n",
    "        'max_new_tokens': 50,\n",
    "        'stop_words': ['<|im_end|>', '<|endoftext|>'],\n",
    "        'temperature': TEMPERATURE,\n",
    "        'top_p': TOP_P,\n",
    "        'top_k': TOP_K,\n",
    "        'print_response': False\n",
    "    }\n",
    "\n",
    "    # Obtain responses BEFORE hacking\n",
    "    RESPONSES.append({'BASELINE': obtain_responses(prompt_kwargs)})\n",
    "\n",
    "    model, tokenizer = hacker.hack(\n",
    "        model, tokenizer,\n",
    "        encoding_tokenizer,\n",
    "        num_tokens=NUMBER_NEW_TOKENS,\n",
    "        embed_initializer_method=EMBED_INIT_METHOD,\n",
    "        show_progress=True,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # Go over all the prompts and check responses\n",
    "    prompt_kwargs.update({'model': model, 'tokenizer': tokenizer})\n",
    "    RESPONSES[-1]['INITIALIZED_NO_TRAINING'] = obtain_responses(prompt_kwargs)\n",
    "\n",
    "    # Visually print all prompts\n",
    "    print(f'{\"\":=^200s}\\n\\n{\"MODEL: `\" + model_name + \"`\": ^200s}\\n\\n')\n",
    "    for n, prompt in enumerate(PROMPTS):\n",
    "        # Print responses\n",
    "        for model_type in RESPONSES[-1].keys():\n",
    "            for lan in RESPONSES[-1][model_type][n].keys():\n",
    "                # Use regex to remove multiple newlines to a maximum of 2\n",
    "                response = re.sub(r'\\n{3,}', '\\n\\n', RESPONSES[-1][model_type][n][lan])\n",
    "                print(f'{model_type}[{lan}]: <PROMPT>{prompt[lan]}</PROMPT> <RESPONSE>{response}</RESPONSE>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9e924",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "The answer to this question was **NO**:\n",
    "By looking into the data itself, it doesn't seem like the model gets better at Portuguese (`CalamePT` and `SupergluePTPT`) by adding new tokens and initializing the embedding table only.\n",
    "One interesting take away is that the model `Qwen/Qwen2.5-1.5B-Instruct` saw a slight drop in both `CalamePT` and `SupergluePTPT` when adding 7500 tokens.\n",
    "However, the model `HuggingFaceTB/SmolLM2-135M` showed a slight increase. Neither of these changes are statistically significant, but further investigation may reveal more details\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\"> <thead> <tr style=\"text-align: right;\"> <th></th> <th>model</th> <th>number_new_tokens</th> <th>model_type</th> <th>CalamePT</th> <th>SupergluePTPT</th> </tr> </thead> <tbody> <tr> <th>0</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>0</td> <td>BASELINE</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>1</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>2</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.014678</td> </tr> <tr> <th>3</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>4</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>5</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>6</th> <td>HuggingFaceTB/SmolLM2-135M</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.135356</td> <td>0.015055</td> </tr> <tr> <th>7</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>0</td> <td>BASELINE</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>8</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>9</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>10</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>11</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>12</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>13</th> <td>HuggingFaceTB/SmolLM3-3B</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.585260</td> <td>0.496864</td> </tr> <tr> <th>14</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>0</td> <td>BASELINE</td> <td>0.496146</td> <td>0.402396</td> </tr> <tr> <th>15</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>1000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.496146</td> <td>0.398570</td> </tr> <tr> <th>16</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>1000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.496146</td> <td>0.397943</td> </tr> <tr> <th>17</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>5000</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.496146</td> <td>0.402835</td> </tr> <tr> <th>18</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>5000</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.496146</td> <td>0.402083</td> </tr> <tr> <th>19</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>7500</td> <td>INITIALIZED_NO_TRAINING</td> <td>0.495665</td> <td>0.401393</td> </tr> <tr> <th>20</th> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>7500</td> <td>INITIALIZED_WITH_TRAINING</td> <td>0.495665</td> <td>0.400640</td> </tr> </tbody> </table> \n",
    "\n",
    "Also, By looking into the output of the `notebook cell` above, we can see that the quality of the English response is still higher than that of the Portuguese one. And it doesn't seem to change after we've added new tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1091a1",
   "metadata": {},
   "source": [
    "## Research Question 1.2.2\n",
    "Can tokenizer adaptation accelerate the training process for new language adaptation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc444007",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "```\n",
    "============================================================\n",
    "                 REMOVE THIS QUESTION\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa32cbc",
   "metadata": {},
   "source": [
    "## Research Question 1.2.3\n",
    "How much can inference efficiency be improved by adding language-specific tokens to the model’s vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa84943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df_121 = pd.read_csv('RESULTS_SUMMARY_20250823164827.csv')\n",
    "df_121 = df_121[['number_new_tokens', 'model', 'model_type', 'FertilityOutput', 'FertilityBoost']]\n",
    "df_121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396d4d1",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Yes -> By observing the results of `FertilityBoost`, it's clear to see that adding the new tokens can increase generation speed by upwards of `16%`. This increase, however, is mosttly seen in the lowest, mono-lingual model. It has not been explored if this increase is due to the mono-lingual aspect or the model-size. (future work: Explore bigger mono-lingual models & small multi-lingual models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777775b2",
   "metadata": {},
   "source": [
    "\n",
    "## Research Question 1.2.4\n",
    "What embedding initialization strategies are most effective for integrating new tokens into a pre-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb961dc8",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "In order to obtain an answer to this question, we're going to evaluation `new_tokens` logits when they are **expected** to be predicted by using pre-determined phrases where we know those tokens should exist.\n",
    "\n",
    "Then, we compare the results of multiple initialization methods to determine the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e51a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "#                       STEP 1. Fetch a dataset\n",
    "#\n",
    "# In step 1 we're fetching a dataset and selecting a random number of lines which CONTAIN any added token\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def calculate_token_scores(model, tokenizer, phrase: str, token_id: int):\n",
    "    '''\n",
    "    Returns the score of a specific token_id when generating a new token with `phrase` as input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Any\n",
    "        model to generate the phrase with\n",
    "    \n",
    "    tokenizer: Any\n",
    "        tokenizer to encode the given phrase\n",
    "    \n",
    "    phrase: str\n",
    "        phrase to give as input to the model\n",
    "    \n",
    "    token_id: int\n",
    "        token to retrieve the scores to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[Literal['score', 'rank', 'best_score'], float]\n",
    "    '''\n",
    "    inputs = tokenizer(phrase, return_tensors='pt')\n",
    "    for key in inputs.keys(): inputs[key] = inputs[key].to(model.device)\n",
    "    generation = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        max_new_tokens=1,\n",
    "    )\n",
    "    scores = generation['scores'][0][0]\n",
    "    token_score = scores[token_id]\n",
    "    token_rank = (scores > token_score).sum()\n",
    "    return {'score': token_score.item(), 'rank': token_rank.item(), 'best_score': scores.max().item()}\n",
    "\n",
    "def choose_random_line(file, file_size: int) -> bytes:\n",
    "    \"\"\"\n",
    "    Seek to a random byte position and return the next full line.\n",
    "    \n",
    "    Args:\n",
    "        file: A file object opened in binary mode.\n",
    "        file_size: The total size of the file in bytes.\n",
    "        \n",
    "    Returns:\n",
    "        A full line (bytes), starting from the next newline after the random byte.\n",
    "    \"\"\"\n",
    "    byte_pos = np.random.randint(0, file_size - 1)\n",
    "    file.seek(byte_pos)\n",
    "    \n",
    "    # Skip partial line\n",
    "    file.readline()\n",
    "    \n",
    "    # Return the next full line\n",
    "    return file.readline()\n",
    "\n",
    "def sample_lines(file_path: str, sample_size: int, musthave_chars_list: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Randomly sample a given number of unique lines from a large file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file.\n",
    "        sample_size: Number of unique lines to sample.\n",
    "        \n",
    "    Returns:\n",
    "        A list of decoded strings (lines).\n",
    "    \"\"\"\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    # Find #SAMPLE_SIZE phrases containing any of the new tokens\n",
    "    selected_lines = set()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while len(selected_lines) < sample_size:\n",
    "            line = choose_random_line(f, file_size)\n",
    "            if line and line.strip() and any(t in line.decode() for t in musthave_chars_list):  # Ignore empty lines and lines which don't contain any added token\n",
    "                selected_lines.add(line.decode())\n",
    "    return list(selected_lines)\n",
    "\n",
    "def print_prefix(model: str, method: str, start: pd.Timestamp):\n",
    "    timestamp_print = f'{pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")} | {(pd.Timestamp.now() - start).total_seconds():<5.2f} sec'\n",
    "    return f'[`{model}`/{\"`\"+method+\"`\":<17s} @ {timestamp_print}]'\n",
    "\n",
    "def analyze_init_method(model_name: str, num_new_tokens: int, embed_init_method: str, dataset_path: str, sample_size: int):\n",
    "    start = pd.Timestamp.now()\n",
    "    print_args = [model_name, embed_init_method, start]\n",
    "    print(f'{print_prefix(*print_args)} - Initializing model and tokenizer')\n",
    "    # \"Hacking\" a model and training a tokenizer to get \"new_tokens\"\n",
    "    model, tokenizer   = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)\n",
    "    encoding_tokenizer = hack_tokenizer.utils.loader.load_model_and_tokenizer(model_name, DEVICE)[1]\n",
    "    hacker = hack_tokenizer.hack.ModelHacker(dataset=DATASET_TOKENIZER, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(f'{print_prefix(*print_args)} - Hacking model and tokenizer')\n",
    "    model, tokenizer = hacker.hack(\n",
    "        model, tokenizer,\n",
    "        encoding_tokenizer,\n",
    "        num_tokens=num_new_tokens,\n",
    "        embed_initializer_method=embed_init_method,\n",
    "        show_progress=False,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # Select lines\n",
    "    lines = sample_lines(dataset_path, sample_size, hacker.new_tokens)\n",
    "\n",
    "    # Iterate over them\n",
    "    scores = []\n",
    "    for line in tqdm.tqdm(lines, desc=f'{print_prefix(*print_args)} - Analyzing results'):\n",
    "        # Pick a random `new_token` in `line` to simulate a generation for it\n",
    "        new_tokens_in_line = [t for t in hacker.new_tokens if t in line]\n",
    "        split_token = np.random.choice(new_tokens_in_line, 1)[0].item()   # Randomly choose a token to split the word on\n",
    "        splitted_line = line.split(split_token)\n",
    "        eval_phrase = splitted_line.pop(0)\n",
    "        while len(eval_phrase) < 10 and len(splitted_line) > 0:    # While we don't have 10 characters, continuously expand the phrase\n",
    "            eval_phrase += split_token + splitted_line.pop(0)\n",
    "        # Calculate the score for it\n",
    "        scores.append(calculate_token_scores(model, encoding_tokenizer, eval_phrase, tokenizer.encode(split_token)))\n",
    "    print(f'{print_prefix(*print_args)} - Finished Analysis')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e11b85",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "By using the Auxiliary functions, we're going to Analyze all different methods available to compare how the scores of the initializations compare with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297e431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "SAMPLE_SIZE = 1_000\n",
    "MODEL = 'HuggingFaceTB/SmolLM2-135M'\n",
    "NUMBER_NEW_TOKENS = constants.NUMBER_NEW_TOKENS\n",
    "DATASET_PATH = constants.DATA_DIR / 'FULL_opensubtitles_pt-pt.txt'\n",
    "AVAILABLE_INIT_METHODS = [\n",
    "    'mean',\n",
    "    'min',\n",
    "    'max',\n",
    "    'quantile(0.25)', 'quantile(0.5)', 'quantile(0.75)'\n",
    "] + [f'weighted_drop({i/10:.1f})' for i in range(5, 51, 5)]\n",
    "\n",
    "# Results path (to store results as it takes a bit of time to run)\n",
    "RQ124_RESULTS_PATH = 'RQ_1.2.4_Results.json'\n",
    "\n",
    "# Iterating over all available init methods\n",
    "if not os.path.isfile(RQ124_RESULTS_PATH):\n",
    "    results = {}\n",
    "    for embed_init_method in AVAILABLE_INIT_METHODS:\n",
    "        results[embed_init_method] = analyze_init_method(MODEL, NUMBER_NEW_TOKENS, embed_init_method, DATASET_PATH, SAMPLE_SIZE)\n",
    "    with open(RQ124_RESULTS_PATH, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "with open(RQ124_RESULTS_PATH, 'r') as f:\n",
    "    results = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfa4f0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "method=%{x}<br>min_ranked=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "max",
          "mean",
          "min",
          "quantile(0.25)",
          "quantile(0.5)",
          "quantile(0.75)",
          "weighted_drop(0.5)",
          "weighted_drop(1.0)",
          "weighted_drop(1.5)",
          "weighted_drop(2.0)",
          "weighted_drop(2.5)",
          "weighted_drop(3.0)",
          "weighted_drop(3.5)",
          "weighted_drop(4.0)",
          "weighted_drop(4.5)",
          "weighted_drop(5.0)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "HDggPh4fGSM2SEFYZmdsew==",
          "dtype": "i1"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "method"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "min_ranked"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = []\n",
    "for key in results.keys():\n",
    "    for n, i in enumerate(results[key]):\n",
    "        i.update({'method': key, 'phrase_id': n})\n",
    "    df += results[key]\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "\n",
    "# Count number of times each method is \"number 1\" in rank (meaning how many times each method had the lowest \"rank\")\n",
    "min_values = df.groupby(by=['phrase_id'], as_index=False)[['rank']].min()\n",
    "df = df.merge(min_values, how='left', on=['phrase_id'], suffixes=('', '_min'))\n",
    "df['min_ranked'] = df['rank'] == df['rank_min']\n",
    "evaluation = df.groupby(by=['method'])['min_ranked'].sum()\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "evaluation.plot(kind='bar', y='min_ranked')\n",
    "\n",
    "\n",
    "# df.pivot(\n",
    "#     index='phrase_id', columns='method', values='rank'\n",
    "# ).reset_index()\n",
    "\n",
    "\n",
    "# df.plot(x='phrase_id', figsize=(20, 10), logy=True, logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4008eaa",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "After evaluating 5 different methods, we decided to utilize `weighted_drop`.\n",
    "\n",
    "More specifically, `weighted_drop(2)`. The reason to choose `weighted_drop(2)` Vs the `weighted_drop(5)` is that by using $K=5$ (in $weighted\\_drop(K)$), we would pretty much not include **any** information of tokens other than the first one.\n",
    "\n",
    "For that reason, we assigned the **first** tested `weighted_drop` bigger than all the others.\n",
    "\n",
    "Final answer: `weighted_drop(2)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

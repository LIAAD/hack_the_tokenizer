{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c117f9d1",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739147f3",
   "metadata": {},
   "source": [
    "## 6.1 Motivation\n",
    "\n",
    "Gathering results for the chapter 6.1 - Motivation.\n",
    "\n",
    "These should be results showcasing that a model after being \"hacked\" can't have the \"default\" inference, because it simply alucinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70837a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Configs: \n",
      "  - model: HuggingFaceTB/SmolLM2-135M\n",
      "  - device: cuda\n",
      "  - model_kwargs: {'torch_dtype': torch.bfloat16}\n",
      "  - tokenizer_kwargs: {}\n",
      "  - Tokenizer (BPE) Dataset: `/home/yali/MEGA/Hack The Tockenizer/data/tokenizer_pt-pt.txt`\n",
      "  - Batch Size: 8\n",
      "  - Learning Rate: 1e-06 (we won't train the model here, however),\n",
      "  - # Added Tokens: 10000\n",
      "  - Embeddings Init Method: weighted_drop(1.5)\n",
      "  - Maximum New Tokens: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hack_tokenizer import hack, loader\n",
    "from hack_tokenizer.utils import constants as C\n",
    "\n",
    "C.MODEL = 'HuggingFaceTB/SmolLM2-135M'\n",
    "DATASET_TOKENIZER = C.DATA_DIR / 'tokenizer_pt-pt.txt'\n",
    "NUM_NEW_TOKENS = 10000\n",
    "MAX_NEW_TOKENS = 20\n",
    "PROMPTS = [\n",
    "    'Olá, podes contar-me um poema com as palavras: Sol, Lua, Céu e Nuvens?',\n",
    "    'Bacalhau é um dos peixes mais utilizados na culinária'\n",
    "]\n",
    "\n",
    "print(f'''Run Configs: \n",
    "  - model: {C.MODEL}\n",
    "  - device: {C.DEVICE}\n",
    "  - model_kwargs: {{'torch_dtype': torch.bfloat16}}\n",
    "  - tokenizer_kwargs: {{}}\n",
    "  - Tokenizer (BPE) Dataset: `{DATASET_TOKENIZER}`\n",
    "  - Batch Size: {C.GENERATION_BATCH_SIZE}\n",
    "  - Learning Rate: {C.LEARNING_RATE} (we won't train the model here, however),\n",
    "  - # Added Tokens: {NUM_NEW_TOKENS}\n",
    "  - Embeddings Init Method: {C.EMBED_INIT_METHOD}\n",
    "  - Maximum New Tokens: {MAX_NEW_TOKENS}\n",
    "''')\n",
    "model, tokenizer = loader.load_model_and_tokenizer(C.MODEL, C.DEVICE)\n",
    "encoding_tokenizer = loader.load_model_and_tokenizer(C.MODEL, C.DEVICE)[1]\n",
    "with open(DATASET_TOKENIZER, 'r') as f:\n",
    "    tokenizer_train_data = f.readlines()\n",
    "hacker = hack.ModelHacker(\n",
    "    tokenizer_train_data,\n",
    "    C.GENERATION_BATCH_SIZE,\n",
    "    C.LEARNING_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c818ca4",
   "metadata": {},
   "source": [
    "Default generation BEFORE hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2207824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Olá, podes contar-me um poema com as palavras: Sol, Lua, Céu e Nuvens?</PROMPT><GENERATION>\n",
      "\n",
      "Ao fazer isso, o poeta deve ter uma palavra</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Bacalhau é um dos peixes mais utilizados na culinária</PROMPT><GENERATION>.\n",
      "\n",
      "Ao entender a criatura dos peixes, o uso de</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt in PROMPTS:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'].to(model.device),\n",
    "        attention_mask=inputs['attention_mask'].to(model.device),\n",
    "        max_new_tokens = MAX_NEW_TOKENS,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        temperature=None\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:]).replace(prompt, '')    # Removing \"prompt\" from output\n",
    "    print(f'{\"\":-^100s}\\n<PROMPT>{prompt}</PROMPT><GENERATION>{generated_text}</GENERATION>\\n{\"\":-^100s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0bba6",
   "metadata": {},
   "source": [
    "(hacking model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c092f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing tokens \"contained\" within any token of original tokenizer: 100%|██████████| 15137/15137 [00:27<00:00, 543.48it/s]\n",
      "Initializing the embeddings for the new_tokens: 100%|██████████| 10000/10000 [00:01<00:00, 8771.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = hacker.hack(\n",
    "    model, tokenizer,\n",
    "    tokenizer, num_tokens=NUM_NEW_TOKENS,\n",
    "    embed_initializer_method=C.EMBED_INIT_METHOD,\n",
    "    show_progress=True,\n",
    "    train = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62c24c",
   "metadata": {},
   "source": [
    "\"Default\" inference generation AFETR hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adcfad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Olá, podes contar-me um poema com as palavras: Sol, Lua, Céu e Nuvens?</PROMPT><GENERATION>\n",
      "\n",
      "The first two words are the same, but the third is different. The first word is</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Bacalhau é um dos peixes mais utilizados na culinária</PROMPT><GENERATION>.\n",
      "\n",
      "Ao final, a criação de um novo código de program</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt in PROMPTS:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'].to(model.device),\n",
    "        attention_mask=inputs['attention_mask'].to(model.device),\n",
    "        max_new_tokens = MAX_NEW_TOKENS,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        temperature=None\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:]).replace(prompt, '')    # Removing \"prompt\" from output\n",
    "    print(f'{\"\":-^100s}\\n<PROMPT>{prompt}</PROMPT><GENERATION>{generated_text}</GENERATION>\\n{\"\":-^100s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743a371",
   "metadata": {},
   "source": [
    "Our proposed generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0be0ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Olá, podes contar-me um poema com as palavras: Sol, Lua, Céu e Nuvens?</PROMPT><GENERATION>\n",
      "\n",
      "Ao fazer isso, o poeta deve ter uma palavra</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<PROMPT>Bacalhau é um dos peixes mais utilizados na culinária</PROMPT><GENERATION>.\n",
      "\n",
      "Ao entender a criatura dos peixes, o uso de</GENERATION>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_tokens_at_a_time = 4\n",
    "for prompt in PROMPTS:\n",
    "    og_prompt = prompt \n",
    "    generated_text = ''.join(hack.ModelHacker.prompt(\n",
    "        model, tokenizer,\n",
    "        encoding_tokenizer,\n",
    "        content=prompt,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        stop_words=[],\n",
    "        print_response=False,\n",
    "        num_tokens_generated_at_once=num_tokens_at_a_time\n",
    "    ))\n",
    "    print(f'{\"\":-^100s}\\n<PROMPT>{og_prompt}</PROMPT><GENERATION>{generated_text}</GENERATION>\\n{\"\":-^100s}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
